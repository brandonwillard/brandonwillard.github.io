<!DOCTYPE html>
<html lang="en"
>
<head>
    <title>A Role for Symbolic Computation in the General Estimation of Statistical Models - Brandon T. Willard</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html">

        <meta name="author" content="Brandon T. Willard" />
        <meta name="description" content="A Role for Symbolic Computation in the General Estimation of Statistical Models code{white-space: pre;} div.sourceCode { overflow-x: auto; } table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode { margin: 0; padding: 0; vertical-align: baseline; border: none; } table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; } td.lineNumbers { text-align: right; padding-right ..." />

        <meta property="og:site_name" content="Brandon T. Willard" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="A Role for Symbolic Computation in the General Estimation of Statistical Models"/>
        <meta property="og:url" content="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html"/>
        <meta property="og:description" content="A Role for Symbolic Computation in the General Estimation of Statistical Models code{white-space: pre;} div.sourceCode { overflow-x: auto; } table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode { margin: 0; padding: 0; vertical-align: baseline; border: none; } table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; } td.lineNumbers { text-align: right; padding-right ..."/>
        <meta property="article:published_time" content="2017-01-18" />
            <meta property="article:section" content="articles" />
            <meta property="article:author" content="Brandon T. Willard" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/bootstrap.readable.min.css" type="text/css"/>
    <link href="https://brandonwillard.github.io/theme/css/font-awesome.min.css" rel="stylesheet">
    <link href="https://brandonwillard.github.io/theme/css/academicons.min.css" rel="stylesheet">

    <link href="https://brandonwillard.github.io/theme/css/pygments/vim.css" rel="stylesheet">
    <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/style.css" type="text/css"/>
        <link href="https://brandonwillard.github.io/extra/custom.css" rel="stylesheet">

        <link href="https://brandonwillard.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Brandon T. Willard ATOM Feed"/>
        <link href="https://brandonwillard.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Brandon T. Willard RSS Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://brandonwillard.github.io/" class="navbar-brand">
Brandon T. Willard            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="https://brandonwillard.github.io/pages/about.html">
                             About
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/projects.html">
                             Projects
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/publications.html">
                             Publications
                          </a></li>
                        <li class="active">
                            <a href="https://brandonwillard.github.io/category/articles.html">Articles</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="https://brandonwillard.github.io/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">

    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html"
                       rel="bookmark"
                       title="Permalink to A Role for Symbolic Computation in the General Estimation of Statistical Models">
                        A Role for Symbolic Computation in the General Estimation of Statistical Models
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-01-18T00:00:00-06:00"> Wed 18 January 2017</time>
    </span>



    
</footer><!-- /.post-info -->                    </div>
                </div>
                <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Brandon T. Willard" />
  <title>A Role for Symbolic Computation in the General Estimation of Statistical Models</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code > span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code > span.dt { color: #0057ae; } /* DataType */
code > span.dv { color: #b08000; } /* DecVal */
code > span.bn { color: #b08000; } /* BaseN */
code > span.fl { color: #b08000; } /* Float */
code > span.cn { color: #aa5500; } /* Constant */
code > span.ch { color: #924c9d; } /* Char */
code > span.sc { color: #3daee9; } /* SpecialChar */
code > span.st { color: #bf0303; } /* String */
code > span.vs { color: #bf0303; } /* VerbatimString */
code > span.ss { color: #ff5500; } /* SpecialString */
code > span.im { color: #ff5500; } /* Import */
code > span.co { color: #898887; } /* Comment */
code > span.do { color: #607880; } /* Documentation */
code > span.an { color: #ca60ca; } /* Annotation */
code > span.cv { color: #0095ff; } /* CommentVar */
code > span.ot { color: #006e28; } /* Other */
code > span.fu { color: #644a9b; } /* Function */
code > span.va { color: #0057ae; } /* Variable */
code > span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code > span.op { color: #1f1c1b; } /* Operator */
code > span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code > span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code > span.pp { color: #006e28; } /* Preprocessor */
code > span.at { color: #0057ae; } /* Attribute */
code > span.re { color: #0057ae; } /* RegionMarker */
code > span.in { color: #b08000; } /* Information */
code > span.wa { color: #bf0303; } /* Warning */
code > span.al { color: #bf0303; font-weight: bold; } /* Alert */
code > span.er { color: #bf0303; text-decoration: underline; } /* Error */
code > span. { color: #1f1c1b; } /* Normal */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<!--  -->
<!-- <div id="header"> -->
<!-- <h1 class="title">A Role for Symbolic Computation in the General Estimation of Statistical Models</h1> -->
<!--  -->
<!--  -->
<!-- <h2 class="author">Brandon T. Willard</h2> -->
<!--  -->
<!--  -->
<!-- <h3 class="date">2017–01–18</h3> -->
<!--  -->
<!-- </div> -->
<!--  -->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In this document we describe how symbolic computation can be used to provide generalizable statistical estimation through a combination of existing open source frameworks. Specifically, we will show how symbolic tools can be used to address the estimation of non-smooth functions that appear in models with parameter regularization, shrinkage and sparsity. We employ a mathematical framework that makes extensive use of <em>proximal operators</em> <span class="citation" data-cites="parikh_proximal_2014 combettes_proximal_2011">(Parikh and Boyd 2014; Combettes and Pesquet 2011)</span> and their properties for maximum a posteriori (MAP) estimation: i.e. the <em>proximal framework</em>. This framework produces what we’ll call <em>proximal methods</em> and their implementations as <em>proximal algorithms</em>.</p>
<p>In <span class="citation" data-cites="polson_proximal_2015">Polson, Scott, and Willard (2015)</span> we outlined a set of seemingly disparate optimization techniques within the fields of statistics, computer vision, and machine learning (e.g. gradient descent, ADMM, EM, Douglas-Rachford) that are unified by their various applications of proximal methods. These methods–and the concepts behind them–have found much success in recent times and admit quite a few interesting paths for research. In other words, there are many reasons to alone discuss the implementation of proximal methods.</p>
<p>Proximal operators also enjoy a breadth of closed-form solutions and useful properties that are amenable to symbolic computation. In more than a few cases, the work required to produce a proximal algorithm overlaps with well-established features of computer algebra systems and symbolic mathematics, such as symbolic differentiation and algebraic equation solving.</p>
<p>Symbolic integration provides an excellent example of how proximal operators could be implemented in a symbolic system. In these systems, mappings between functions (as canonicalized graphs) and their generalized hypergeometric equivalents are used to exploit the latter’s relevant convolution identities. In the same vein, it is possible to use tables of closed-form proximal operators and their properties to produce a wide array of estimation algorithms for many non-smooth functions. We outline how this might be done in the following sections.</p>
<p>Otherwise, the ideas discussed here are part of a never-ending attempt to answer a question that arises naturally in both mathematics and programming–at all levels: <em>How does one provide a means of generating robust solutions to as many problems as possible?</em> Instead of the common efforts to independently implement each model, method and/or combination of the two–followed by their placement in an API or library of functions–implementations can be encoded in and organized by the very mathematics from which they were derived. This close coupling between mathematical principles and their implementations might be the only reasonable way to remove barriers between theory, research and practice.</p>
<section id="a-context" class="level2">
<h2>A Context</h2>
<p>Much recent work in statistical modeling and estimation has had the goal of producing sparse results and/or efficient, near automatic model selection. This objective is shared with other related practices–such as Deep Learning and Compressed Sensing. In the former case, we can point to Dropout <span class="citation" data-cites="srivastava_dropout_2014">(Srivastava et al. 2014)</span> and–in the latter–<span class="math inline">\(\ell_p\)</span> regularization <span class="citation" data-cites="donoho_compressed_2006">(Donoho 2006)</span> as basic examples.</p>
<p>Here we’ll simply assume that a practitioner intends to produce sparse estimates using the well-known LASSO–or <span class="math inline">\(\ell_1\)</span> penalty.</p>
<p>In PyMC3 <span class="citation" data-cites="salvatier_probabilistic_2016">(Salvatier, Wiecki, and Fonnesbeck 2016)</span>, the Bayes version of LASSO <span class="citation" data-cites="park_bayesian_2008">(Park and Casella 2008)</span> is easily specified.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> scipy <span class="im">as</span> sc

<span class="im">import</span> pymc3 <span class="im">as</span> pm
<span class="im">import</span> theano
<span class="im">import</span> theano.tensor <span class="im">as</span> tt
<span class="im">from</span> theano <span class="im">import</span> shared <span class="im">as</span> tt_shared

theano.config.mode <span class="op">=</span> <span class="st">&#39;FAST_COMPILE&#39;</span>

mu_true <span class="op">=</span> np.zeros(<span class="dv">100</span>)
mu_true[:<span class="dv">20</span>] <span class="op">=</span> np.exp(<span class="op">-</span>np.arange(<span class="dv">20</span>)) <span class="op">*</span> <span class="dv">100</span>

X <span class="op">=</span> np.random.randn(<span class="bu">int</span>(np.alen(mu_true) <span class="op">*</span> <span class="fl">0.7</span>), np.alen(mu_true))
y <span class="op">=</span> sc.stats.norm.rvs(loc<span class="op">=</span>X.dot(mu_true), scale<span class="op">=</span><span class="dv">10</span>)

X_tt <span class="op">=</span> tt_shared(X, name<span class="op">=</span><span class="st">&#39;X&#39;</span>, borrow<span class="op">=</span><span class="va">True</span>)
y_tt <span class="op">=</span> tt_shared(y, name<span class="op">=</span><span class="st">&#39;y&#39;</span>, borrow<span class="op">=</span><span class="va">True</span>)

<span class="cf">with</span> pm.Model() <span class="im">as</span> lasso_model:
    <span class="co"># Would be nice if we could pass the symbolic y_tt.shape, so</span>
    <span class="co"># that our model would automatically conform to changes in</span>
    <span class="co"># the shared variables X_tt.</span>
    <span class="co"># See https://github.com/pymc-devs/pymc3/pull/1125</span>
    beta_rv <span class="op">=</span> pm.Laplace(<span class="st">&#39;beta&#39;</span>, mu<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">1</span>, shape<span class="op">=</span>X.shape[<span class="dv">1</span>])
    y_rv <span class="op">=</span> pm.Normal(<span class="st">&#39;y&#39;</span>, mu<span class="op">=</span>X_tt.dot(beta_rv), sd<span class="op">=</span><span class="dv">1</span>,
                     shape<span class="op">=</span>y.shape[<span class="dv">0</span>], observed<span class="op">=</span>y_tt)</code></pre></div>
<p>Again, the negative total log likelihood in our example has a non-smooth <span class="math inline">\(\ell_1\)</span> term. Keeping this in mind, let’s say we wanted to produce a MAP estimate using PyMC3. A function is already provided for this task: <code>find_MAP</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">with</span> lasso_model:
    params_0 <span class="op">=</span> pm.find_MAP(<span class="bu">vars</span><span class="op">=</span>[beta_rv])</code></pre></div>
<p>In our run of the above, an exception was thrown due to <code>nan</code> values within the gradient evaluation. We can inspect the gradient at <span class="math inline">\(\beta = 0, 1\)</span> and reproduce the result.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">start <span class="op">=</span> pm.Point({<span class="st">&#39;beta&#39;</span>: np.zeros(X.shape[<span class="dv">1</span>])}, model<span class="op">=</span>lasso_model)
bij <span class="op">=</span> pm.DictToArrayBijection(pm.ArrayOrdering(lasso_model.<span class="bu">vars</span>),
start)
logp <span class="op">=</span> bij.mapf(lasso_model.fastlogp)
dlogp <span class="op">=</span> bij.mapf(lasso_model.fastdlogp(lasso_model.<span class="bu">vars</span>))

<span class="co"># Could also inspect the log likelihood of the prior:</span>
<span class="co"># beta_rv.dlogp().f(np.zeros_like(start[&#39;beta&#39;]))</span>

grad_at_0 <span class="op">=</span> dlogp(np.zeros_like(start[<span class="st">&#39;beta&#39;</span>]))
grad_at_1 <span class="op">=</span> dlogp(np.ones_like(start[<span class="st">&#39;beta&#39;</span>]))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(np.<span class="bu">sum</span>(np.isnan(grad_at_0)))
<span class="dv">100</span>
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(np.<span class="bu">sum</span>(np.isnan(grad_at_1)))
<span class="dv">0</span></code></pre></div>
<p>The s are not due to any short-coming of PyMC3; they only demonstrate a suitable place for our ideas and improvements. Additionally, by working within PyMC3, we can readily apply certain mathematical results. For instance, theorems that apply only to distributions. This idea is more relevant to the graph optimizations we consider later, but is still very important.</p>
</section>
</section>
<section id="the-proximal-context" class="level1">
<h1>The Proximal Context</h1>
<p>We start with the essential ingredient: the proximal operator.</p>
<div class="Def" markdown="" env-number="1" title-name="[Proximal Operator]">
<p><span class="math display">\[\begin{equation*}
\operatorname*{prox}_{\phi}(x) =
    \operatorname*{argmin}_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
\end{equation*}\]</span></p>
</div>
<p>As we mentioned earlier, the proximal operator is the main tool of proximal algorithms. Exact solutions to proximal operators exist for many <span class="math inline">\(\phi\)</span>, and, since they’re often quite simple in form, their computation is relatively cheap: a property that the proximal methods themselves can inherit.</p>
<p>Consider the MAP estimation of a penalized likelihood, i.e. <span class="math display">\[\begin{equation}
\beta^* = \operatorname*{argmin}_\beta \left\{ l(\beta) + \gamma \phi(\beta) \right\}
  \;,
  \label{eq:prox_problem}
\end{equation}\]</span> where functions <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span> are commonly referred to as likelihood and prior terms (or loss and penalty), respectively. The proximal framework usually assumes <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span> are at least lower semi-continuous and convex–although quite a few useful results still hold for non-convex functions.</p>
<p>Notice that Equation <span class="math inline">\(\eqref{eq:prox_problem}\)</span> takes the form of a proximal operator when <span class="math inline">\(l(\beta) = \frac{1}{2} (y - \beta)^2\)</span>. Otherwise, in regression problems, we have <span class="math inline">\(l(\beta) = \frac{1}{2} \|y - X \beta\|^2\)</span>. In this case, properties of the proximal operator can be used to produce independent proximal operators in each dimension of <span class="math inline">\(\beta\)</span>. Since more than one property of the proximal operator can accomplish this–and result in distinct approaches–one might begin to see here a reason for the breadth of proximal methods.</p>
<p>The proximal operator relevant to our example, <span class="math inline">\(\operatorname*{prox}_{|\cdot|}\)</span>, is equivalent to the soft-thresholding operator. Its implementation in Theano is somewhat trivial, but–for the sake of exposition–we provide an example.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">beta_tt <span class="op">=</span> tt.vector(<span class="st">&#39;beta&#39;</span>, dtype<span class="op">=</span>tt.config.floatX)
beta_tt.tag.test_value <span class="op">=</span> np.r_[<span class="op">-</span><span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="dv">0</span>, <span class="fl">0.2</span>, <span class="dv">1</span>,
<span class="dv">10</span>].astype(tt.config.floatX)

lambda_tt <span class="op">=</span> tt.scalar(<span class="st">&#39;lambda&#39;</span>, dtype<span class="op">=</span>tt.config.floatX)
lambda_tt.tag.test_value <span class="op">=</span> np.array(<span class="fl">0.5</span>).astype(tt.config.floatX)

<span class="kw">def</span> soft_threshold(beta_, lambda_):
    <span class="cf">return</span> tt.sgn(beta_) <span class="op">*</span> tt.maximum(tt.abs_(beta_) <span class="op">-</span> lambda_, <span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(soft_threshold(beta_tt, lambda_tt).tag.test_value)
[<span class="op">-</span><span class="fl">9.5</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">-</span><span class="dv">0</span>.   <span class="dv">0</span>.   <span class="dv">0</span>.   <span class="fl">0.5</span>  <span class="fl">9.5</span>]</code></pre></div>
<p>Proximal operators can be composed with a gradient step to produce the <em>proximal gradient</em> algorithm: <span class="math display">\[\begin{equation}
\beta = \operatorname*{prox}_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}\]</span></p>
<p>Besides the proximal operator for <span class="math inline">\(\phi\)</span>, steps in the proximal gradient algorithm are very straightforward and require only the gradient of <span class="math inline">\(l(\beta)\)</span>. This is where a tangible benefit of symbolic computation becomes apparent: <span class="math inline">\(\nabla l(\beta)\)</span> can be computed automatically and efficiently. With [backtracking] line search to handle unknown step sizes, <span class="math inline">\(\alpha\)</span>, the proximal gradient algorithm provides a surprisingly general means of sparse estimation.</p>
</section>
<section id="the-symbolic-operations" class="level1">
<h1>The Symbolic Operations</h1>
<p>In order to identify a relevant, non-smooth problem, check that a given proximal method’s conditions are satisfied (e.g. convexity), and potentially solve the resulting proximal operators in closed-form, we need to obtain expressions for <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span>.</p>
<p>In some cases, we’re able to tease apart <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span> using only the interface provided by PyMC3. Specifically, the <em>observed</em> and <em>unobserved</em> random variable fields in PyMC3 models.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> theano <span class="im">import</span> clone <span class="im">as</span> tt_clone

logl <span class="op">=</span> tt_clone(lasso_model.observed_RVs[<span class="dv">0</span>].logpt,
                {beta_rv: beta_tt})
logl.name <span class="op">=</span> <span class="st">&quot;logl&quot;</span></code></pre></div>
<p>Instead, let’s assume we’re extending <code>find_MAP</code> with even more generality, so that we can’t determine <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span> in this way. This situation can occur when a user specifies custom distributions or potential functions. Regardless, we need to operate at a more symbolic level.</p>
<div class="remark" markdown="" env-number="1" title-name="">
<p>At this point, it is extremely worthwhile to browse the <a href="http://deeplearning.net/software/theano/extending/graphstructures.html">Theano documentation</a> regarding graphs and their constituent objects.</p>
</div>
<p>The total log-likelihood is a good place to start. Let’s look at the symbolic graph for the log-likelihood of our model.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> theano <span class="im">import</span> pp <span class="im">as</span> tt_pp
<span class="im">from</span> theano <span class="im">import</span> pprint <span class="im">as</span> tt_pprint</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(tt_pp(lasso_model.logpt))
(Sum{acc_dtype<span class="op">=</span>float64}(Sum{acc_dtype<span class="op">=</span>float64}(((<span class="op">-</span>log(TensorConstant{<span class="dv">2</span>}))
<span class="op">-</span> (<span class="op">|</span>(<span class="op">\</span>beta <span class="op">-</span> TensorConstant{<span class="dv">0</span>})<span class="op">|</span> <span class="op">/</span> TensorConstant{<span class="dv">1</span>})))) <span class="op">+</span>
Sum{acc_dtype<span class="op">=</span>float64}(Sum{acc_dtype<span class="op">=</span>float64}(switch(TensorConstant{<span class="dv">1</span>},
(((TensorConstant{<span class="op">-</span><span class="fl">1.0</span>} <span class="op">*</span> ((y <span class="op">-</span> (X <span class="op">\</span>dot <span class="op">\</span>beta)) <span class="op">**</span> TensorConstant{<span class="dv">2</span>}))
<span class="op">+</span> log(TensorConstant{<span class="fl">0.159154943092</span>})) <span class="op">/</span> TensorConstant{<span class="fl">2.0</span>}),
TensorConstant{<span class="op">-</span>inf}))))</code></pre></div>
<p>The <a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#pretty-printing">pretty printed</a> Theano graph tells us–among other things–that we indeed have a sum of <span class="math inline">\(\ell_2\)</span> and <span class="math inline">\(\ell_1\)</span> terms, although they are found among other confusing results (such as a <code>switch</code> statement).</p>
<p>As with most graphs produced by symbolic algebra systems, we need to understand how operations and objects are expressed in a graph and exactly which ones are relevant to us. After doing so, we can develop a means of finding what we want. The <a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#debug-print">debug printout</a> is often a better visual summary of graphs, since it expresses branches clearly.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(lasso_model.logpt)
Elemwise{add,no_inplace} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> B] <span class="st">&#39;&#39;</span>
 <span class="op">|</span> <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> C] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>   <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>     <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> E] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>     <span class="op">|</span> <span class="op">|</span>Elemwise{neg,no_inplace} [<span class="bu">id</span> F] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>     <span class="op">|</span>   <span class="op">|</span>Elemwise{log,no_inplace} [<span class="bu">id</span> G] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>     <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> H]
 <span class="op">|</span>     <span class="op">|</span>Elemwise{true_div,no_inplace} [<span class="bu">id</span> I] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>       <span class="op">|</span>Elemwise{abs_,no_inplace} [<span class="bu">id</span> J] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>       <span class="op">|</span> <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> K] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>       <span class="op">|</span>   <span class="op">|</span>beta [<span class="bu">id</span> L]
 <span class="op">|</span>       <span class="op">|</span>   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> M] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>       <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">0</span>} [<span class="bu">id</span> N]
 <span class="op">|</span>       <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> O] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>         <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [<span class="bu">id</span> P]
 <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> Q] <span class="st">&#39;&#39;</span>
   <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> R] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>Elemwise{switch,no_inplace} [<span class="bu">id</span> S] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> T] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [<span class="bu">id</span> P]
       <span class="op">|</span>Elemwise{true_div,no_inplace} [<span class="bu">id</span> U] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span>Elemwise{add,no_inplace} [<span class="bu">id</span> V] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> W] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> X] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="op">-</span><span class="fl">1.0</span>} [<span class="bu">id</span> Y]
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>Elemwise{<span class="bu">pow</span>,no_inplace} [<span class="bu">id</span> Z] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> BA] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span> <span class="op">|</span>y [<span class="bu">id</span> BB]
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span> <span class="op">|</span>dot [<span class="bu">id</span> BC] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>   <span class="op">|</span>X [<span class="bu">id</span> BD]
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>   <span class="op">|</span>beta [<span class="bu">id</span> L]
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> BE] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> H]
       <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> BF] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>Elemwise{log,no_inplace} [<span class="bu">id</span> BG] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="fl">0.159154943092</span>} [<span class="bu">id</span> BH]
       <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> BI] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>   <span class="op">|</span>TensorConstant{<span class="fl">2.0</span>} [<span class="bu">id</span> BJ]
       <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> BK] <span class="st">&#39;&#39;</span>
         <span class="op">|</span>TensorConstant{<span class="op">-</span>inf} [<span class="bu">id</span> BL]</code></pre></div>
<p>We see that the top-most operator is an <code>Elemwise</code> that applies the scalar <code>add</code> operation. This is the “<span class="math inline">\(+\)</span>” in <span class="math inline">\(l + \phi\)</span>. If we were to consider the inputs of this operator as candidates for <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span>, then we could do the following:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(lasso_model.logpt.owner.inputs)
[Sum{acc_dtype<span class="op">=</span>float64}.<span class="dv">0</span>, Sum{acc_dtype<span class="op">=</span>float64}.<span class="dv">0</span>]</code></pre></div>
<p>Starting from the sub-graphs of each term, we could then search for any non-smooth functions that have known closed-form proximal operators. In our case, we only consider the absolute value function.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> get_abs_between(input_node):
    <span class="co">&quot;&quot;&quot; Search for `abs` in the operations between our input and the</span>
<span class="co">    log-likelihood output node.</span>
<span class="co">    &quot;&quot;&quot;</span>
    term_ops <span class="op">=</span> <span class="bu">list</span>(tt.gof.graph.ops([input_node],
[lasso_model.logpt]))

    <span class="co"># Is there an absolute value in there?</span>
    <span class="cf">return</span> <span class="bu">filter</span>(<span class="kw">lambda</span> x: x.op <span class="kw">is</span> tt.abs_, term_ops)

abs_res <span class="op">=</span> [(get_abs_between(in_), in_)
           <span class="cf">for</span> in_ <span class="kw">in</span> lasso_model.logpt.owner.inputs]

<span class="cf">for</span> r_ <span class="kw">in</span> abs_res:
    <span class="cf">if</span> <span class="bu">len</span>(r_[<span class="dv">0</span>]) <span class="op">==</span> <span class="dv">0</span>:
        phi <span class="op">=</span> r_[<span class="dv">1</span>]
    <span class="cf">else</span>:
        logp <span class="op">=</span> r_[<span class="dv">1</span>]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(logp)
Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> B] <span class="st">&#39;&#39;</span>
   <span class="op">|</span>Elemwise{switch,no_inplace} [<span class="bu">id</span> C] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [<span class="bu">id</span> E]
     <span class="op">|</span>Elemwise{true_div,no_inplace} [<span class="bu">id</span> F] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span>Elemwise{add,no_inplace} [<span class="bu">id</span> G] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> H] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> I] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="op">-</span><span class="fl">1.0</span>} [<span class="bu">id</span> J]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>Elemwise{<span class="bu">pow</span>,no_inplace} [<span class="bu">id</span> K] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> L] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span> <span class="op">|</span>y [<span class="bu">id</span> M]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span> <span class="op">|</span>dot [<span class="bu">id</span> N] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>   <span class="op">|</span>X [<span class="bu">id</span> O]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>   <span class="op">|</span>beta [<span class="bu">id</span> P]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> Q] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> R]
     <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> S] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span>   <span class="op">|</span>Elemwise{log,no_inplace} [<span class="bu">id</span> T] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="fl">0.159154943092</span>} [<span class="bu">id</span> U]
     <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> V] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>   <span class="op">|</span>TensorConstant{<span class="fl">2.0</span>} [<span class="bu">id</span> W]
     <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> X] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>TensorConstant{<span class="op">-</span>inf} [<span class="bu">id</span> Y]
<span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(phi)
Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>
 <span class="op">|</span>Sum{acc_dtype<span class="op">=</span>float64} [<span class="bu">id</span> B] <span class="st">&#39;&#39;</span>
   <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> C] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>
     <span class="op">|</span> <span class="op">|</span>Elemwise{neg,no_inplace} [<span class="bu">id</span> E] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>   <span class="op">|</span>Elemwise{log,no_inplace} [<span class="bu">id</span> F] <span class="st">&#39;&#39;</span>
     <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> G]
     <span class="op">|</span>Elemwise{true_div,no_inplace} [<span class="bu">id</span> H] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>Elemwise{abs_,no_inplace} [<span class="bu">id</span> I] <span class="st">&#39;&#39;</span>
       <span class="op">|</span> <span class="op">|</span>Elemwise{sub,no_inplace} [<span class="bu">id</span> J] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>   <span class="op">|</span>beta [<span class="bu">id</span> K]
       <span class="op">|</span>   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> L] <span class="st">&#39;&#39;</span>
       <span class="op">|</span>     <span class="op">|</span>TensorConstant{<span class="dv">0</span>} [<span class="bu">id</span> M]
       <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> N] <span class="st">&#39;&#39;</span>
         <span class="op">|</span>TensorConstant{<span class="dv">1</span>} [<span class="bu">id</span> O]</code></pre></div>
<p>The above approach is still too limiting; we need something more robust. For instance, our logic could fail on graphs that are expressed as <span class="math inline">\(\eta (l + \phi) + 1\)</span>–although a graph for the equivalent expression <span class="math inline">\(\eta l + \eta \phi + \eta\)</span> might succeed. These are types of weaknesses inherent to naive approaches like ours. Furthermore, sufficient logic that uses a similar approach is likely to result in complicated and less approachable code.</p>
<p>The appropriate computational tools are found in the subjects of graph unification and term rewriting, as well as the areas of functional and logic programming. Luckily, Theano provides some basic unification capabilities through its <code>PatternSub</code> class.</p>
<p><code>PatternSub</code> works within the context of Theano <a href="http://deeplearning.net/software/theano/optimizations.html">graph optimization</a>. Graph optimizations perform the common symbolic operations of reduction/simplification and rewriting. Consider the <code>phi</code> variable; the print-outs show an unnecessary subtraction with <span class="math inline">\(0\)</span>. Clearly this step is unnecessary, so–in a basic way–we can see that the graph hasn’t been simplified, yet.</p>
<p>Many standard algebraic simplifications are already present in Theano, and, by creating our own graph optimizations, we can provide the advanced functionality we’ve been alluding to.</p>
<div class="example" markdown="" env-number="1" title-name="[Algebraic Graph Optimization]">
<p>As a quick demonstration, we’ll make replacement patterns for multiplicative distribution across two forms of addition: <code>sum</code> and <code>add</code>.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">test_a_tt <span class="op">=</span> tt.as_tensor_variable(<span class="dv">5</span>, name<span class="op">=</span><span class="st">&#39;a&#39;</span>)
test_b_tt <span class="op">=</span> tt.as_tensor_variable(<span class="dv">2</span>, name<span class="op">=</span><span class="st">&#39;b&#39;</span>)
test_c_tt <span class="op">=</span> tt.as_tensor_variable(np.r_[<span class="dv">1</span>, <span class="dv">2</span>], name<span class="op">=</span><span class="st">&#39;c&#39;</span>)

test_exprs_tt <span class="op">=</span> (test_a_tt <span class="op">*</span> test_b_tt,)
test_exprs_tt <span class="op">+=</span> (test_a_tt <span class="op">*</span> (test_b_tt <span class="op">+</span> test_a_tt),)
test_exprs_tt <span class="op">+=</span> (test_a_tt <span class="op">*</span> (test_c_tt <span class="op">+</span> test_a_tt),)
test_exprs_tt <span class="op">+=</span> (test_a_tt <span class="op">*</span> (test_c_tt <span class="op">+</span> test_c_tt),)

mul_dist_pat_tt <span class="op">=</span> (tt.gof.opt.PatternSub(
    (tt.mul, <span class="st">&#39;x&#39;</span>, (tt.<span class="bu">sum</span>, <span class="st">&#39;y&#39;</span>, <span class="st">&#39;z&#39;</span>)),
    (tt.<span class="bu">sum</span>, (tt.mul, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>), (tt.mul, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;z&#39;</span>))
),)
mul_dist_pat_tt <span class="op">+=</span> (tt.gof.opt.PatternSub(
    (tt.mul, <span class="st">&#39;x&#39;</span>, (tt.add, <span class="st">&#39;y&#39;</span>, <span class="st">&#39;z&#39;</span>)),
    (tt.add, (tt.mul, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>), (tt.mul, <span class="st">&#39;x&#39;</span>, <span class="st">&#39;z&#39;</span>))
),)</code></pre></div>
<p>Substitutions can be applied to an objective function until it is in a fully-reduced form: <code>EquilibriumOptimizer</code> provides this functionality.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">test_sub_eqz_opt_tt <span class="op">=</span> tt.gof.opt.EquilibriumOptimizer(
    mul_dist_pat_tt, max_use_ratio<span class="op">=</span><span class="dv">10</span>)

test_fgraph_tt <span class="op">=</span> tt.gof.fg.FunctionGraph(
    tt.gof.graph.inputs(test_exprs_tt), test_exprs_tt)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(test_fgraph_tt)
Elemwise{mul,no_inplace} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>   <span class="dv">5</span>
 <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> C]
Elemwise{mul,no_inplace} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>   <span class="dv">8</span>
 <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span>Elemwise{add,no_inplace} [<span class="bu">id</span> E] <span class="st">&#39;&#39;</span>   <span class="dv">4</span>
   <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> C]
   <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
Elemwise{mul,no_inplace} [<span class="bu">id</span> F] <span class="st">&#39;&#39;</span>   <span class="dv">9</span>
 <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> G] <span class="st">&#39;&#39;</span>   <span class="dv">3</span>
 <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span>Elemwise{add,no_inplace} [<span class="bu">id</span> H] <span class="st">&#39;&#39;</span>   <span class="dv">7</span>
   <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> I]
   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> J] <span class="st">&#39;&#39;</span>   <span class="dv">2</span>
     <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
Elemwise{mul,no_inplace} [<span class="bu">id</span> K] <span class="st">&#39;&#39;</span>   <span class="dv">6</span>
 <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> L] <span class="st">&#39;&#39;</span>   <span class="dv">1</span>
 <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span>Elemwise{add,no_inplace} [<span class="bu">id</span> M] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
   <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> I]
   <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> I]</code></pre></div>
<p>Now, when we apply the optimization, the <code>FunctionGraph</code> should contain the replacements.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">test_fgraph_opt <span class="op">=</span> test_sub_eqz_opt_tt.optimize(test_fgraph_tt)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(test_fgraph_tt)
Elemwise{mul,no_inplace} [<span class="bu">id</span> A] <span class="st">&#39;&#39;</span>   <span class="dv">5</span>
 <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> C]
Elemwise{add,no_inplace} [<span class="bu">id</span> D] <span class="st">&#39;&#39;</span>   <span class="dv">10</span>
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> E] <span class="st">&#39;&#39;</span>   <span class="dv">4</span>
 <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">2</span>} [<span class="bu">id</span> C]
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> F] <span class="st">&#39;&#39;</span>   <span class="dv">3</span>
   <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
   <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
Elemwise{add,no_inplace} [<span class="bu">id</span> G] <span class="st">&#39;&#39;</span>   <span class="dv">12</span>
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> H] <span class="st">&#39;&#39;</span>   <span class="dv">9</span>
 <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> I] <span class="st">&#39;&#39;</span>   <span class="dv">2</span>
 <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span> <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> J]
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> K] <span class="st">&#39;&#39;</span>   <span class="dv">8</span>
   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> I] <span class="st">&#39;&#39;</span>   <span class="dv">2</span>
   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> L] <span class="st">&#39;&#39;</span>   <span class="dv">1</span>
     <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
Elemwise{add,no_inplace} [<span class="bu">id</span> M] <span class="st">&#39;&#39;</span>   <span class="dv">11</span>
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> N] <span class="st">&#39;&#39;</span>   <span class="dv">7</span>
 <span class="op">|</span> <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> O] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
 <span class="op">|</span> <span class="op">|</span> <span class="op">|</span>TensorConstant{<span class="dv">5</span>} [<span class="bu">id</span> B]
 <span class="op">|</span> <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> J]
 <span class="op">|</span>Elemwise{mul,no_inplace} [<span class="bu">id</span> P] <span class="st">&#39;&#39;</span>   <span class="dv">6</span>
   <span class="op">|</span>DimShuffle{x} [<span class="bu">id</span> O] <span class="st">&#39;&#39;</span>   <span class="dv">0</span>
   <span class="op">|</span>TensorConstant{[<span class="dv">1</span> <span class="dv">2</span>]} [<span class="bu">id</span> J]</code></pre></div>
</div>
<p>Even more symbolic capabilities might be needed to [efficiently] achieve the functionality we desire. Standalone libraries like SymPy and <a href="https://github.com/logpy/logpy/">LogPy</a> can be adapted to Theano graphs and provide these capabilities–although direct implementation in Theano may be better.</p>
<p>Finally, let’s briefly imagine how convexity could be determined symbolically. For differentiable terms, we could start with a simple second derivative test. Within Theano, a “second derivative” can be obtained using the <code>hessian</code> function, and within <code>theano.sandbox.linalg</code> are <code>Optimizer</code> hints for matrix positivity and other properties relevant to determining convexity.</p>
<div class="remark" markdown="" env-number="2" title-name="">
<p>Other great examples of linear algebra themed optimizations are in <code>theano.sandbox.linalg</code>: for instance, <code>no_transpose_symmetric</code>. Some of these demonstrate exactly how straight-forward adding algebraic features can be.</p>
</div>
<p>Although our convexity testing idea is far too simple for some functions, the point is that the basic tools necessary for work in this direction are already in place. With the logic programming and symbolic libraries mentioned earlier, a robust implementation of the convex function calculus could be very much in reach.</p>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>We’ve sketched out some ideas and tools with which one could develop a robust estimation platform guided by the more abstract mathematical frameworks from which new and efficient methods are produced.</p>
<p>Some key steps may require the integration of a fully featured symbolic algebra system. Along these lines, connections between Theano, SymPy and LogPy have been explored in <span class="citation" data-cites="rocklin_mathematically_2013">Rocklin (2013)</span>–as well as many other important aspects of the topics discussed here.</p>
<p>Besides the automation of proximal algorithms themselves, there are areas of application involving very large and complex models–perhaps the ones arising in Deep Learning. How might we consider the operator splitting of ADMM within deeply layered or hierarchical models <span class="citation" data-cites="polson_statistical_2015">(Polson, Willard, and Heidari 2015)</span>? At which levels and on which terms should the splitting be performed? Beyond trying to solve the potentially unwieldy mathematics arising from such questions, by imbuing these symbolic tools with more mathematical awareness, we can at least experiment in these directions and quickly offer numerical solutions. This is–in part–the edge from which statistics hasn’t been benefiting and modern machine learning has.</p>
<p>Before closing, a very related–and interesting–set of ideas is worth mentioning: the possibility of encoding more symbolic knowledge into probabilistic programming platforms like PyMC3. Using the same optimization mechanisms as the examples here, simple distributional relationships can be encoded. For instance, the convolution of normally distributed random variables:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">mu_X <span class="op">=</span> tt.vector(<span class="st">&#39;mu_X&#39;</span>)
mu_X.tag.test_value <span class="op">=</span> np.array([<span class="dv">1</span>.], dtype<span class="op">=</span>tt.config.floatX)
sd_X <span class="op">=</span> tt.vector(<span class="st">&#39;sd_X&#39;</span>)
sd_X.tag.test_value <span class="op">=</span> np.array([<span class="dv">2</span>.], dtype<span class="op">=</span>tt.config.floatX)

mu_Y <span class="op">=</span> tt.vector(<span class="st">&#39;mu_Y&#39;</span>)
mu_Y.tag.test_value <span class="op">=</span> np.array([<span class="dv">1</span>.], dtype<span class="op">=</span>tt.config.floatX)
sd_Y <span class="op">=</span> tt.vector(<span class="st">&#39;sd_Y&#39;</span>)
sd_Y.tag.test_value <span class="op">=</span> np.array([<span class="fl">0.5</span>], dtype<span class="op">=</span>tt.config.floatX)

<span class="cf">with</span> pm.Model() <span class="im">as</span> conv_model:
    X_rv <span class="op">=</span> pm.Normal(<span class="st">&#39;X&#39;</span>, mu_X, sd<span class="op">=</span>sd_X, shape<span class="op">=</span>(<span class="dv">1</span>,))
    Y_rv <span class="op">=</span> pm.Normal(<span class="st">&#39;Y&#39;</span>, mu_Y, sd<span class="op">=</span>sd_Y, shape<span class="op">=</span>(<span class="dv">1</span>,))
    Z_rv <span class="op">=</span> X_rv <span class="op">+</span> Y_rv</code></pre></div>
<p>We create a Theano <code>Op</code> to handle the convolution.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">class</span> NormConvOp(tt.Op):
    __props__ <span class="op">=</span> ()

    <span class="kw">def</span> make_node(<span class="va">self</span>, <span class="op">*</span>inputs):
        name_new <span class="op">=</span> <span class="bu">str</span>.join(<span class="st">&#39;+&#39;</span>, [<span class="bu">getattr</span>(in_, <span class="st">&#39;name&#39;</span>, <span class="st">&#39;&#39;</span>) <span class="cf">for</span> in_ <span class="kw">in</span>
inputs])
        mu_new <span class="op">=</span> tt.add(<span class="op">*</span>[in_.distribution.mu <span class="cf">for</span> in_ <span class="kw">in</span> inputs])
        sd_new <span class="op">=</span> tt.sqrt(tt.add(<span class="op">*</span>[in_.distribution.sd<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> in_ <span class="kw">in</span>
inputs]))
        conv_rv <span class="op">=</span> pm.Normal(name_new, mu<span class="op">=</span>mu_new, sd<span class="op">=</span>sd_new,
                            <span class="co"># Is this another place where</span>
automatically<span class="op">/</span>Theano managed
                            <span class="co"># shapes are really needed.  For now, we</span>
hack it.
                            shape<span class="op">=</span>(<span class="dv">1</span>,))

        <span class="cf">return</span> tt.Apply(<span class="va">self</span>, inputs, [conv_rv])

    <span class="kw">def</span> perform(<span class="va">self</span>, node, inputs, output_storage):
        z <span class="op">=</span> output_storage[<span class="dv">0</span>]
        z[<span class="dv">0</span>] <span class="op">=</span> np.add(<span class="op">*</span>inputs)</code></pre></div>
<p>Now, all that’s needed is a <code>PatternSub</code> like before.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> is_normal_dist(x):
    <span class="cf">return</span> <span class="bu">hasattr</span>(x, <span class="st">&#39;distribution&#39;</span>) <span class="kw">and</span> <span class="bu">isinstance</span>(x.distribution,
pm.Normal)

norm_conv_pat_tt <span class="op">=</span> (tt.gof.opt.PatternSub(
    (tt.add,
     {<span class="st">&#39;pattern&#39;</span>: <span class="st">&#39;x&#39;</span>,
      <span class="st">&#39;constraint&#39;</span>: <span class="kw">lambda</span> x: is_normal_dist(x)},
     {<span class="st">&#39;pattern&#39;</span>: <span class="st">&#39;y&#39;</span>,
      <span class="st">&#39;constraint&#39;</span>: <span class="kw">lambda</span> x: is_normal_dist(x)}
     ),
    (NormConvOp(), <span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>)),)

norm_conv_opt_tt <span class="op">=</span> tt.gof.opt.EquilibriumOptimizer(norm_conv_pat_tt,
                                                   max_use_ratio<span class="op">=</span><span class="dv">10</span>)

Z_fgraph_tt <span class="op">=</span> tt.gof.fg.FunctionGraph([X_rv, Y_rv], [Z_rv])

<span class="co"># We lose the `FreeRV.distribution` attribute when cloning the graph</span>
<span class="co"># with `theano.gof.graph.clone_get_equiv` in `FunctionGraph`, so this</span>
<span class="co"># hackishly reattaches that information:</span>
_ <span class="op">=</span> [<span class="bu">setattr</span>(g_in, <span class="st">&#39;distribution&#39;</span>, s_in.distribution)
     <span class="cf">for</span> s_in, g_in <span class="kw">in</span> <span class="bu">zip</span>([X_rv, Y_rv], Z_fgraph_tt.inputs)]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="cf">with</span> conv_model:
    _ <span class="op">=</span> norm_conv_opt_tt.optimize(Z_fgraph_tt)

norm_conv_var_dist <span class="op">=</span> Z_fgraph_tt.outputs[<span class="dv">0</span>].distribution</code></pre></div>
<p>The resulting graph:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> tt.printing.debugprint(Z_fgraph_tt)
NormConvOp [<span class="bu">id</span> A] <span class="st">&#39;X+Y&#39;</span>   <span class="dv">0</span>
 <span class="op">|</span>X [<span class="bu">id</span> B]
 <span class="op">|</span>Y [<span class="bu">id</span> C]</code></pre></div>
<p>and the convolution’s parameters (for the test values):</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(norm_conv_var_dist.mu.tag.test_value)
[ <span class="dv">2</span>.]
<span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(norm_conv_var_dist.sd.tag.test_value)
[ <span class="fl">2.06155281</span>]</code></pre></div>
<p>More sophisticated routines–like the example above–could implement parameter expansions, efficient re-parameterizations and equivalent scale mixture forms in an effort to optimize a graph for sampling or point evaluation. Objectives for these optimizations could be straightforward and computationally based (e.g. reducing the number of operations in computations of the log likelihood and other quantities) or more statistically focused (e.g. highly efficient sampling, improve mixing). These ideas are most definitely not new–one example is given by <span class="citation" data-cites="mohasel_afshar_probabilistic_2016">Mohasel Afshar (2016)</span> for symbolic Gibbs sampling, but we hope the examples given here make the point that the tools are readily available and quite accessible.</p>
<p>We’ll end on a much more spacey consideration. Namely, that this is a context in which we can start experimenting rapidly with objectives over the space of estimation routines. This space is generated by–but not limited to–the variety of symbolic representations, re-parameterizations, etc., mentioned above. It does not necessarily require the complete estimation of a model at each step, nor even the numeric value of quantities like the gradient or Hessian. It may involve them, but not their evaluation; perhaps, instead, symbolic comparisons of competing gradients and Hessians arising from different representations. What we’re describing lies somewhere between the completely numeric assessments common today, and the entirely symbolic work found within the theorems and manipulations of the mathematics we use to derive methods.</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-combettes_proximal_2011">
<p>Combettes, Patrick L, and Jean-Christophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” <em>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</em>, 185–212.</p>
</div>
<div id="ref-donoho_compressed_2006">
<p>Donoho, David L. 2006. “Compressed Sensing.” <em>IEEE Transactions on Information Theory</em> 52 (4): 1289–1306. <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066" class="uri">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066</a>.</p>
</div>
<div id="ref-mohasel_afshar_probabilistic_2016">
<p>Mohasel Afshar, Hadi. 2016. “Probabilistic Inference in Piecewise Graphical Models.” <a href="https://digitalcollections.anu.edu.au/handle/1885/107386" class="uri">https://digitalcollections.anu.edu.au/handle/1885/107386</a>.</p>
</div>
<div id="ref-parikh_proximal_2014">
<p>Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” <em>Foundations and Trends in Optimization</em> 1 (3): 123–231. doi:<a href="https://doi.org/10.1561/2400000003">10.1561/2400000003</a>.</p>
</div>
<div id="ref-park_bayesian_2008">
<p>Park, Trevor, and George Casella. 2008. “The Bayesian Lasso.” <em>Journal of the American Statistical Association</em> 103 (482): 681–86. <a href="http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337" class="uri">http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337</a>.</p>
</div>
<div id="ref-polson_proximal_2015">
<p>Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” <em>Statistical Science</em> 30 (4): 559–81. <a href="http://projecteuclid.org/euclid.ss/1449670858" class="uri">http://projecteuclid.org/euclid.ss/1449670858</a>.</p>
</div>
<div id="ref-polson_statistical_2015">
<p>Polson, Nicholas G., Brandon T. Willard, and Massoud Heidari. 2015. “A Statistical Theory of Deep Learning via Proximal Splitting.” <em>arXiv Preprint arXiv:1509.06061</em>. <a href="http://arxiv.org/abs/1509.06061" class="uri">http://arxiv.org/abs/1509.06061</a>.</p>
</div>
<div id="ref-rocklin_mathematically_2013">
<p>Rocklin, Matthew. 2013. “Mathematically Informed Linear Algebra Codes Through Term Rewriting.” PhD thesis, PhD Thesis, August. <a href="http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf" class="uri">http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf</a>.</p>
</div>
<div id="ref-salvatier_probabilistic_2016">
<p>Salvatier, John, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using PyMC3.” <em>PeerJ Computer Science</em> 2 (April): e55. <a href="https://peerj.com/articles/cs-55" class="uri">https://peerj.com/articles/cs-55</a>.</p>
</div>
<div id="ref-srivastava_dropout_2014">
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>The Journal of Machine Learning Research</em> 15 (1): 1929–58. <a href="http://dl.acm.org/citation.cfm?id=2670313" class="uri">http://dl.acm.org/citation.cfm?id=2670313</a>.</p>
</div>
</div>
</section>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
</body>
</html>

            </div>
            <!-- /.entry-content -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

                    var disqus_identifier = 'a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models';
                var disqus_url = 'https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="/images/profile-pic.png"/>
        </p>
    <p>
        <strong>About Brandon T. Willard</strong><br/>
        applied math/stats person
    </p>
</div>
<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="http://linkedin.com/pub/brandon-willard/10/bb4/468/"><i class="fa fa-linkedin-square fa-lg"></i> linkedin</a></li>
                <li class="list-group-item"><a href="https://scholar.google.com/citations?user=g0oUxG4AAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-lg"></i> google scholar</a></li>
                <li class="list-group-item"><a href="https://plus.google.com/+brandonwillard"><i class="fa fa-google-plus-square fa-lg"></i> google+</a></li>
                <li class="list-group-item"><a href="https://bitbucket.org/brandonwillard"><i class="fa fa-bitbucket-square fa-lg"></i> bitbucket</a></li>
                <li class="list-group-item"><a href="https://github.com/brandonwillard"><i class="fa fa-github-square fa-lg"></i> github</a></li>
              </ul>
            </li>



            <li class="list-group-item"><a href="https://brandonwillard.github.io/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
                <ul class="list-group " id="tags">
                </ul>
            </li>
    </ul>
</section>
            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2017 Brandon T. Willard
            &middot; Powered by <a href="https://github.com/DandyDev/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>              <p><small>  <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://brandonwillard.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://brandonwillard.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://brandonwillard.github.io/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-91585967-1', '');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->

</body>
</html>