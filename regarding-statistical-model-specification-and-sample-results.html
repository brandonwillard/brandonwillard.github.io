<!DOCTYPE html>
<html lang="en"
>
<head>
    <title>Regarding Statistical Model Specification and Sample Results - Brandon T. Willard</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://brandonwillard.github.io/regarding-statistical-model-specification-and-sample-results.html">

        <meta name="author" content="Brandon Willard" />
        <meta name="description" content="Introduction In this post I want to address some concepts regarding statistical model specification within the Bayesian paradigm, motivation for its use, and the utility of sample results (e.g. empirical posterior distributions). This write-up isn’t intended to be thorough or self-contained, especially since numerous quality introductions already exist ..." />

        <meta property="og:site_name" content="Brandon T. Willard" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Regarding Statistical Model Specification and Sample Results"/>
        <meta property="og:url" content="https://brandonwillard.github.io/regarding-statistical-model-specification-and-sample-results.html"/>
        <meta property="og:description" content="Introduction In this post I want to address some concepts regarding statistical model specification within the Bayesian paradigm, motivation for its use, and the utility of sample results (e.g. empirical posterior distributions). This write-up isn’t intended to be thorough or self-contained, especially since numerous quality introductions already exist ..."/>
        <meta property="article:published_time" content="2016-11-01" />
            <meta property="article:section" content="articles" />
            <meta property="article:author" content="Brandon Willard" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://brandonwillard.github.io/theme/css/font-awesome.min.css" rel="stylesheet">
    <link href="https://brandonwillard.github.io/theme/css/academicons.min.css" rel="stylesheet">

    <link href="https://brandonwillard.github.io/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/style.css" type="text/css"/>
        <link href="https://brandonwillard.github.io/extra/custom.css" rel="stylesheet">

        <link href="https://brandonwillard.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Brandon T. Willard ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://brandonwillard.github.io/" class="navbar-brand">
Brandon T. Willard            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="https://brandonwillard.github.io/pages/about.html">
                             About
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/projects.html">
                             Projects
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/publications.html">
                             Publications
                          </a></li>
                        <li class="active">
                            <a href="https://brandonwillard.github.io/category/articles.html">Articles</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><a href="https://brandonwillard.github.io/archives.html"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">

    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://brandonwillard.github.io/regarding-statistical-model-specification-and-sample-results.html"
                       rel="bookmark"
                       title="Permalink to Regarding Statistical Model Specification and Sample Results">
                        Regarding Statistical Model Specification and Sample Results
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2016-11-01T00:00:00-05:00"> Tue 01 November 2016</time>
    </span>



    
</footer><!-- /.post-info -->                    </div>
                </div>
                <h1>Introduction</h1>
<p>In this post I want to address some concepts regarding statistical model specification within the Bayesian paradigm, motivation for its use, and the utility of sample results (e.g. empirical posterior distributions). This write-up isn’t intended to be thorough or self-contained, especially since numerous quality introductions already exist for Bayesian modeling and MCMC (e.g. <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a>). Also, what’s advocated here is in large part just <em>statistical</em> modeling and not exclusively <em>Bayesian</em>.</p>
<p>The generality, applicability and relative simplicity of the core concepts within Bayesian modeling are sadly overlooked in practice. Bayes is too often conflated with MCMC and its associated computational costs, or be seen as needlessly “mathy” and technical. I argue that there is an oft unacknowledged trade-off in the efforts of mathematical modeling, and that Bayesian modeling helps navigate that complexity. In doing so, one can save on expended efforts in the long run.</p>
<p>When a model is [fully] specified in a statistical or Bayesian way, the modeler has at their disposal distributions for the unknown quantities of interest; these distributions are often the primary interest. The desired estimates–and ones derived from them–are found “within” those distributions. For instance, as a distribution’s moments (e.g. mean, mode, etc.), which can correspond to estimation errors for parameters or functions thereof (e.g. rolling sums and averages).</p>
<p>Normally, modeling objectives are specified in terms of <em>point-estimates</em> instead of distributions: like the “best” parameters and their associated errors. This situation is also covered by the Bayesian paradigm, especially when the corresponding distributions have a closed-form and are fully specified by a finite number of parameters. However, when this isn’t the case, point-estimates provide only part of the picture. It’s usually these missing parts that make model assessment and prediction mostly separate and difficult endeavours.</p>
<p>Even so, modeling and estimation often proceeds without much statistical consideration or context, making these distributions–and the results they can provide–more and more inaccessible. In a situation where modeling started with common machine learning/statistical software and resulted in non-statistical extensions thereto, the details needed for things like uncertainty quantification broadly equate to specifying the missing statistical context. Actually, “retro-fitting” is a better description in many cases. Considerations like this might be reason enough to–at least minimally–maintain clear statistical assumptions. The Bayesian approach fulfills this requirement and much more.</p>
<p>As a starting point, one can find quite a few non-Bayes models with Bayesian interpretations. Otherwise, finding a Bayesian interpretation itself can advance an understanding of the statistical assumptions and properties of an existing non-Bayes model. Multiple examples arise from models defined by objective or loss functions with forms equivalent to the total log-likelihoods of Bayesian models. This, for instance, is one way that general point-wise estimates can be related to maximum a posteriori (MAP) estimates in the Bayesian context.</p>
<p>Anyway, there’s too much to consider here, so I’m going to continue with some illustrations and details for just a few of these considerations.</p>
<h3>Notation</h3>
<p>Before getting into the details, let’s cover some preliminaries regarding notation.</p>
<p>The symbol <span class="math">\(\sim\)</span> is overloaded to mean a couple things. First, a statement like <span class="math">\(X \sim \operatorname{P}\)</span> means “<span class="math">\(X\)</span> is distributed according to <span class="math">\(\operatorname{P}\)</span>”, when <span class="math">\(X\)</span> is understood to be a random variable (generally denoted by capital letter variables). Second, for a non-random variable <span class="math">\(x\)</span>, <span class="math">\(x \sim \operatorname{P}\)</span> and <span class="math">\(x \sim X\)</span> means “<span class="math">\(x\)</span> is a sample from distribution <span class="math">\(\operatorname{P}\)</span>”. When <span class="math">\(\operatorname{P}\)</span> is not meant to signify a distribution, but instead a generic function–like a probability density function <span class="math">\(p(X=x) \equiv p(x)\)</span>, then the distribution in question is [the] one arising from the function (interpreted as a probability density and/or measure)–when possible. See <a href="https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics">here</a> for a similar notation. Also, whenever indices are dropped, the resulting symbol is assumed to be a stacked matrix containing each entry, e.g. </p>
<div class="math">$$\begin{equation*}
X^\top = \begin{pmatrix} X_1 &amp; \dots &amp; X_N \end{pmatrix} \;.
\end{equation*}$$</div>
<p> When the indexed symbol is a vector, then it is customary to denote the row stacked matrix of each vector with the symbol’s capital letter. E.g., for [column] vectors <span class="math">\(z_i\)</span> over <span class="math">\(i \in \{1, \dots, N\}\)</span>, </p>
<div class="math">$$\begin{equation*}
Z = \begin{pmatrix} z_1 \\ \vdots \\ z_N \end{pmatrix} \;.
\end{equation*}$$</div>
<h1>A Simple Model</h1>
<p>First, a simple normal-normal model </p>
<div class="math">$$\begin{equation}
Y_t \sim \operatorname{N}(x^\top_t \theta, \sigma^2), \quad
    \theta \sim \operatorname{N}(\mu, I \tau^2)
    \label{eq:normal-normal}
\end{equation}$$</div>
<p> for an identity matrix <span class="math">\(I\)</span>, observed random variable <span class="math">\(Y_t\)</span> at time <span class="math">\(t \in \{1, \dots, T\}\)</span>, and known constant values (of matching dimensions) <span class="math">\(x_t\)</span>, <span class="math">\(\sigma\)</span>, <span class="math">\(\mu\)</span> and <span class="math">\(\tau\)</span>. The <span class="math">\(x_t\)</span> play the role of predictors, or features, and we’ll assume that the time dependencies arise primarily through them.</p>
<p>In Bayes parlance, the model in \eqref{eq:normal-normal} gives <span class="math">\(\theta\)</span> a normal prior distribution, and the primary goal involves estimating the “posterior” distribution <span class="math">\(p(\theta \mid y)\)</span>–for a vector of observations <span class="math">\(y\)</span> under the assumption <span class="math">\(y \sim Y\)</span>.</p>
<p>This simple example has the well known closed-form posterior solution for <span class="math">\(\theta\)</span>, </p>
<div class="math">$$\begin{equation}
\left(\theta \mid y_t\right) \sim \operatorname{N}(m, C)
    \;.
    \label{eq:theta-posterior}
\end{equation}$$</div>
<p> for </p>
<div class="math">$$\begin{equation*}
\begin{gathered}
  m = C \left(\mu \tau^{-2} + X^\top y\, \sigma^{-2}\right), \quad
  C = \left(\tau^{-2} + \operatorname{diag}(X^\top X) \sigma^{-2}\right)^{-1}
  \;.\end{gathered}
\end{equation*}$$</div>
<p>Results like this are easily obtained for the classical pairings of “conjugate” distributions. Detailed <a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">tables</a> and <a href="https://goo.gl/UCL3pc">tutorials</a> for conjugate distributions can be found online or in any standard text.</p>
<h1>Estimation (via MCMC)</h1>
<p>From here on let’s assume we do not have the closed-form result in \eqref{eq:theta-posterior}. Instead, we’ll estimate the posterior numerically with <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a>. Again, MCMC is covered to varying degrees of detail all over the place (e.g. <a href="https://goo.gl/JNwfuo">here</a>), so we’ll skip most of those details. Let’s say we’ve decided to use <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings</a>.</p>
<p>For demonstration purposes, we produce a simulation of some data we might observe and for which we would consider applying the model in \eqref{eq:normal-normal}.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">scs</span>

<span class="c1"># Unknown parameter</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="c1"># [Assumed] known parameter</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="c1"># Prior parameters</span>
<span class="n">tau2</span> <span class="o">=</span> <span class="mf">1e2</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">start_datetime</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">tslib</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">())</span>
<span class="n">sim_index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">&#39;2016-01-01 12:00:00&#39;</span><span class="p">,</span>
                          <span class="n">end</span><span class="o">=</span><span class="s1">&#39;2016-01-08 12:00:00&#39;</span><span class="p">,</span>
                          <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;H&#39;</span><span class="p">)</span>

<span class="c1"># Simulated observations</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">alen</span><span class="p">(</span><span class="n">sim_index</span><span class="p">)))</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">X</span> <span class="o">*</span> <span class="n">mu_true</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma2</span><span class="p">)</span>
</pre></div>


<p>A Metropolis-Hastings sampler would perform a simple loop that accepts or rejects samples from a proposal distribution, <span class="math">\(\theta_i \sim p(\theta_i \mid \theta_{i-1})\)</span>, according to the probability </p>
<div class="math">$$\begin{equation*}
\min\left\{1,
  \frac{p(Y = y \mid X, \theta_i)}{p(Y = y \mid X, \theta_{i-1})}
  \frac{p(\theta_i \mid \theta_{i-1})}{p(\theta_{i-1} \mid \theta_i)}
  \right\}
  \;.
\end{equation*}$$</div>
<p> Let’s say our proposal is a normal distribution with a mean equal to the previous sample and a variance given by <span class="math">\(\lambda^2\)</span>. The resulting sampling scheme is a random walk Metropolis-Hastings sampler, and since the proposal is a symmetric distribution, <span class="math">\(\frac{p(\theta_i \mid \theta_{i-1})}{p(\theta_{i-1} \mid \theta_i)} = 1\)</span>.</p>
<p>In code, this could look like</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="k">def</span> <span class="nf">model_logpdf</span><span class="p">(</span><span class="n">theta_</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta_</span><span class="p">,</span>
                                 <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">)))</span>
    <span class="n">res</span> <span class="o">+=</span> <span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">theta_</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
                           <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tau2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">N_samples</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">current_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
<span class="n">proposal_logpdf</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">N_samples</span><span class="p">):</span>
    <span class="n">proposal_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">current_sample</span><span class="p">,</span>
                                       <span class="n">scale</span><span class="o">=</span><span class="n">lam</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">l_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model_logpdf</span><span class="p">(</span><span class="n">proposal_sample</span><span class="p">))</span>
    <span class="n">l_ratio</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">model_logpdf</span><span class="p">(</span><span class="n">current_sample</span><span class="p">))</span>

    <span class="n">p_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">proposal_logpdf</span><span class="p">(</span><span class="n">current_sample</span><span class="p">,</span>
                                     <span class="n">loc</span><span class="o">=</span><span class="n">proposal_sample</span><span class="p">))</span>
    <span class="n">p_ratio</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">proposal_logpdf</span><span class="p">(</span><span class="n">proposal_sample</span><span class="p">,</span>
                                      <span class="n">loc</span><span class="o">=</span><span class="n">current_sample</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">l_ratio</span> <span class="o">+</span> <span class="n">p_ratio</span><span class="p">):</span>
        <span class="n">current_sample</span> <span class="o">=</span> <span class="n">proposal_sample</span>

    <span class="n">theta_samples</span> <span class="o">+=</span> <span class="p">[</span><span class="n">current_sample</span><span class="p">]</span>
</pre></div>


<p>The Metropolis-Hastings sampler does not rely on any prior information or Bayesian formulations. Although the prior is implicitly involved, via the total probability, the concepts behind the sampler itself are still valid without it. Basically, Metropolis-Hastings–like many other MCMC sampling routines–is not specifically Bayesian. It’s better to simply consider MCMC as just another estimation approach (or perhaps a type of stochastic optimization).</p>
<p><a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a> is arguably the other most ubiquitous MCMC technique. Since a model specified in a Bayesian way usually provides a clear joint distribution (or at least something proportional to it) and conditional probabilities, Gibbs sampling is well facilitated.</p>
<p>The context of Bayesian modeling is, however, a good source of direction and motivation for improvements to a sampling procedure (and estimation in general). Under Bayesian assumptions, decompositions and reformulations for broad classes of distributions are often immediately available. Guiding theorems, like the <a href="https://en.wikipedia.org/wiki/Rao%E2%80%93Blackwell_theorem">Rao-Blackwell</a> theorem, are also applicable, and–more generally–the same principles, tools and results that guide the model creation and assessment process can also feed into the estimation process. Making these two processes less disjoint can arguably be an advantage.</p>
<h2>The Situation on Implementation</h2>
<p>MCMC sampling schemes like the above are fairly general and easily abstracted, giving rise to some generic frameworks that put more focus on model specification and attempt to automate the choice of estimation (or implement one robust technique). Some of the more common frameworks are Bayesian in nature: <a href="http://www.openbugs.net/w/FrontPage">OpenBUGS</a>, <a href="http://mcmc-jags.sourceforge.net/">JAGS</a>, <a href="http://mc-stan.org/">Stan</a>, and <a href="https://pymc-devs.github.io/pymc/">PyMC2</a> / <a href="https://pymc-devs.github.io/pymc3/">PyMC3</a>. These libraries provide a sort of meta-language that facilitates the specification of a Bayesian model and mirrors the mathematical language of probability. They also implicitly implement the <a href="https://en.wikipedia.org/wiki/Algebra_of_random_variables">algebra of random variables</a> and automatically handle the mechanics of variable transforms.</p>
<p>Our model, estimated with a Metropolis-Hastings sampler, can be expressed in PyMC3 with the following code:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s1">&#39;FAST_COMPILE&#39;</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Model definition</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">tau2</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">sigma2</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>

    <span class="c1"># Posterior sampling</span>
    <span class="n">sample_steps</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">()</span>
    <span class="n">sample_traces</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">sample_steps</span><span class="p">)</span>
</pre></div>


<p>As per the basic examples in the <a href="https://goo.gl/WW3TO8">PyMC3 notebooks</a>, the posterior samples are plotted below using the following code:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">usetex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">tp_axes</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">sample_traces</span><span class="p">)</span>
</pre></div>


<p>We can also superimpose the true posterior density given by \eqref{eq:theta-posterior} with the following:</p>
<div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="n">a_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">r&#39;Posterior $(\theta \mid y)$ Samples&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">a_</span> <span class="ow">in</span>
<span class="n">tp_axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>

<span class="n">freq_axis</span> <span class="o">=</span> <span class="n">tp_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">freq_axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">r&#39;$\theta$&#39;</span><span class="p">)</span>

<span class="n">sample_axis</span> <span class="o">=</span> <span class="n">tp_axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sample_axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">r&#39;$i$&#39;</span><span class="p">)</span>

<span class="n">rhs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">tau2</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">y_obs</span><span class="p">)</span>
<span class="n">tau_post</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">tau2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="n">post_mean</span> <span class="o">=</span> <span class="n">rhs</span><span class="o">/</span><span class="n">tau_post</span>
<span class="n">post_var_inv</span> <span class="o">=</span> <span class="n">tau_post</span>

<span class="n">post_pdf</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">scs</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">,</span>
                   <span class="n">loc</span><span class="o">=</span><span class="n">post_mean</span><span class="p">,</span>
                   <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">post_var_inv</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">add_function_plot</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mf">1e2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">post_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span>
                             <span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">post_data</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">post_pdf</span><span class="p">,</span> <span class="n">post_range</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">post_range</span><span class="p">,</span> <span class="n">post_data</span><span class="p">,</span>
                   <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="c1"># Add true posterior pdf to the plot</span>
<span class="n">add_function_plot</span><span class="p">(</span><span class="n">post_pdf</span><span class="p">,</span> <span class="n">freq_axis</span><span class="p">,</span>
                  <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;Exact&#39;</span><span class="p">)</span>

<span class="c1"># Add manually produced MH samples to the plot</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">freq_axis</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;Manual MH&#39;</span><span class="p">)</span>

<span class="n">sample_axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;Manual MH&#39;</span><span class="p">)</span>


<span class="n">freq_axis</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sample_axis</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_theta_post_plot_1.png" alt="Posterior samples" /></p>
<h2>The Costs</h2>
<p>MCMC, and specifically the Metropolis-Hastings approach used above, can look very simple and universally applicable, but–of course–there’s a trade-off occurring somewhere. The trade-offs most often appear in relation to the complexity and cost of [intermediate] sampling steps and convergence rates. To over simplify, the standard <span class="math">\(O(N^{-1/2})\)</span> error rate–from the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>–is the MCMC baseline, which isn’t all that competitive with some of the standard deterministic optimization methods.</p>
<p>Even for conceptually simple models, the proposal distribution (and its parameters) are not always easy to choose or cheap to tune. The upfront computational costs can be quite high for the more generic MCMC approaches, but there are almost always paths toward efficient samplers–in the context of a specific problem, at least.</p>
<p>In practice, the generality and relative simplicity of the Bayes approach, combined with MCMC, can be somewhat misleading to newcomers. After some immediate success with simpler and/or scaled down problems, one is soon lead to believe that the cost of direct computations and the effort and skill required to derive efficient methods is not worth the potential parsimony and extra information provided by sample results.</p>
<p>The unfortunate outcome of this situation is sometimes an effective rejection of Bayes and MCMC altogether. Although the point hasn’t been illustrated here, MCMC isn’t the only option. Bayesian models are just as amenable to deterministic estimation as non-Bayesian ones, and a wide array of efficient deterministic estimation techniques are available–albeit not so common in standard practice (e.g. <a href="http://projecteuclid.org/euclid.ss/1449670858">proximal algorithms</a>).</p>
<h1>Predictions</h1>
<p>The sampling situation offered by MCMC (and Bayes) puts one in a nice situation to make extensive use of predictions <em>and</em> obtain uncertainty measures (e.g. variances, credible intervals, etc.).</p>
<p>In general, posterior predictive samples are fairly easy to obtain. Once you have posterior samples of <span class="math">\(\theta\)</span>, say <span class="math">\(\{\theta_i\}_{i=0}^M\)</span>, simply plug those into the sampling/observation distribution and sample <span class="math">\(Y\)</span> values. Specifically, </p>
<div class="math">$$\begin{equation}
\{y_i \sim p(Y \mid X, \theta_i) : \theta_i \sim p(\theta_i \mid y)\}_{i=0}^M
  \label{eq:post_predict_samples}
\end{equation}$$</div>
<p> is a posterior predictive sample from <span class="math">\(p(Y \mid X, y)\)</span>.</p>
<p>The procedural interpretation of \eqref{eq:post_predict_samples} is:</p>
<ol>
<li>
<p>Sample <span class="math">\(\theta_i \sim p(\theta_i \mid y)\)</span></p>
</li>
<li>
<p>Sample <span class="math">\(y_i \sim p(Y \mid X, \theta_i)\)</span></p>
</li>
</ol>
<p>Assuming we’ve already produced a posterior sample, this is as simple as plugging those <span class="math">\(\theta_i\)</span> into the observation distribution \eqref{eq:normal-normal} and sampling. The cumulative effect of this process is equivalent to producing an estimate of the marginal </p>
<div class="math">$$\begin{equation*}
\int p(Y_t \mid x_t, \theta) p(\theta \mid y) d\theta = p(Y_t \mid x_t, y)
  \;.
\end{equation*}$$</div>
<p>The posterior predictive sample in \eqref{eq:post_predict_samples} contains much of the information a modeler desires. Take the variance of this sample and one has a common measure of prediction error; produce quantiles of the sample and one has <a href="https://en.wikipedia.org/wiki/Credible_interval">“credible”</a> prediction intervals. The sample produced by mapping an arbitrary function to each posterior predictive sample is itself amenable to the aforementioned summaries, allowing one to easily produce errors for complicated uses of predicted quantities. We illustrate these use cases below.</p>
<p>Using our previous simulation and PyMC3, the posterior predictive samples are obtained with</p>
<div class="highlight"><pre><span></span><span class="n">ppc_samples</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">sample_traces</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>


<p>and plotted with</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">ppc_hpd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_samples</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">alen</span><span class="p">(</span><span class="n">y_obs</span><span class="p">)),</span>
                 <span class="n">ppc_hpd</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                 <span class="n">ppc_hpd</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;$(Y \mid X, y)$ 95\</span><span class="si">% i</span><span class="s1">nterval&#39;</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$y$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ppc_samples</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;$E[Y \mid X, y]$&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_hourly_ppc_plot_1.png" alt="Posterior predictive samples" /></p>
<div class="example" env-number="1" title-name="">
<p>Let’s say we’re interested in daily, monthly, or yearly averages for <span class="math">\(Y_t\)</span> at a lower frequency–like minutes or hours. Similarly, we might want to consider functions of differences between the outputs of different models, <span class="math">\(f(Y^{(j)} - Y^{(k)})\)</span> for <span class="math">\(j, k \in \{1, 2\}\)</span>, or more generally <span class="math">\(f(Y^{(j)}, Y^{(k)})\)</span>. These quantities derived from simple manipulations of <code>ppc_hpd</code>.</p>
</div>
<p>Next, we produce predictions for daily averages–along with [credible] intervals.</p>
<div class="highlight"><pre><span></span><span class="n">y_obs_h</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sim_index</span><span class="p">)</span>

<span class="n">ppc_samples_h</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ppc_samples</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sim_index</span><span class="p">)</span>
<span class="n">ppc_samples_h</span> <span class="o">=</span> <span class="n">ppc_samples_h</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="n">ppc_samples_h</span> <span class="o">=</span> <span class="n">ppc_samples_h</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ppc_quantiles_d</span> <span class="o">=</span> <span class="n">ppc_samples_h</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;D&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]))</span>

<span class="n">ppc_quantiles_d</span> <span class="o">=</span> <span class="n">ppc_quantiles_d</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>

<span class="n">y_obs_d</span> <span class="o">=</span> <span class="n">y_obs_h</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;D&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">y_obs_d</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;$f(y)$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">ppc_quantiles_d</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                 <span class="n">ppc_quantiles_d</span><span class="p">[</span><span class="mf">0.05</span><span class="p">],</span>
                 <span class="n">ppc_quantiles_d</span><span class="p">[</span><span class="mf">0.95</span><span class="p">],</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;$(f(Y) \mid X, y)$ 95\</span><span class="si">% i</span><span class="s1">nterval&#39;</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ppc_quantiles_d</span><span class="p">[</span><span class="mf">0.5</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">r&#39;$E[f(Y) \mid X, y]$&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_daily_ppc_plot_1.png" alt="Daily posterior predictive results from the hourly posterior." /></p>
<h1>Hierarchical Extensions</h1>
<p>Even though we only considered “in-sample” predictions in the previous section, out-of-sample and missing values are covered by exactly the same process (neatly simplified by PyMC3’s <code>sample_ppc</code>). In our example we needed an exogenous variable <span class="math">\(x_t\)</span> in order to sample a point from the observation model <span class="math">\((Y_t \mid x_t)\)</span>. When the values in <span class="math">\(X\)</span> cannot be obtained–e.g. future values of a non-deterministic quantity–clever, context specific imputations are usually proposed.</p>
<p>Nearly every instance of such imputations gives rise to an implicit model. Going back to our preference for transparent statistical specification, it behooves us to formally specify the model. If we do so in a well-defined Bayes way, then we’re immediately provided the exact same conveniences as above.</p>
<div class="example" env-number="2" id="ex:X_temp" title-name="">
<div id="ex:X_temp_math" style="display:none;visibility:hidden">

$$\begin{equation}\tag{2}\label{ex:X_temp}\end{equation}$$

</div>
<p>If the <span class="math">\(X\)</span> values in our sample now correspond to, say, temperature, and today is the last day in our time-indexed observations <code>y_obs</code>, then predicting forward in time will require temperatures for the future.</p>
</div>

<p>One answer to this situation is a model for <span class="math">\(x_t\)</span>. If we specify some <span class="math">\(X_t \sim P\)</span>, then we can apply the same principles above via the posterior predictive <span class="math">\(p(X_t)\)</span>. This posterior predictive will have no exogenous dependencies (unless we want it to), and its posterior can be estimated with our given <span class="math">\(X\)</span> observations. All this occurs in exactly the same fashion as our model for <span class="math">\(Y_t\)</span>.</p>
<p>In practice, one often sees the use of summary statistics from previous <span class="math">\(x_t\)</span> observations in intervals representative of the desired prediction period. For instance, in the context of Example \ref{ex:X_temp}, the average temperatures in previous years over the months corresponding to the prediction interval (e.g. January-February averages through 2010 to 2016 as imputations for January-February 2017).</p>
<p>This isn’t a bad idea, per se, but it is a needlessly indirect–and often insufficient–approach to defining a statistical model for <span class="math">\(X\)</span>. It leaves out critical distributional details, the same details needed to determine how anything using our new <span class="math">\(x_t\)</span> estimates might be affected (through <a href="https://en.wikipedia.org/wiki/Propagation_of_uncertainty">propagation of uncertainty</a>). Eventually one comes around to specifying these details, but, in situations of sufficient complexity, this practice doesn’t produce a very clean, manageable or easily extensible model.</p>
<p>The kinds of complicated models arising in these situations are both conceptually and technically difficult to use, and–as a result–it can be very hard to produce anything other than naive asymptotic approximations for errors and intervals. Sadly, these approximations are generally insufficient for all but the simplest scenarios.</p>
<p>In contrast, we can model the <span class="math">\(x_t\)</span> values directly and have a very clear cut path toward out-of-sample predictions and their distributional properties. Even if we hold to the belief that the previous average values are a reasonable imputation, then a number of simple models can account for that assumption.</p>
<div class="example" env-number="3" id="ex:prior_extension" title-name="">
<div id="ex:prior_extension_math" style="display:none;visibility:hidden">

$$\begin{equation}\tag{3}\label{ex:prior_extension}\end{equation}$$

</div>
<p>Let’s consider a normal regression model for <span class="math">\(x_t\)</span> with seasonal factors, i.e. </p>
<div class="math">$$X_t \sim \operatorname{N}(d(t)^\top \beta, I \sigma_x^2)$$</div>
<p> where <span class="math">\(d(t)\)</span> is an indicator vector containing the seasonal factors and <span class="math">\(I\)</span> is an identity matrix.</p>
<p>Keep in mind that we’ve stretched the notation a bit by letting <span class="math">\(X_t\)</span> be a random vector at time <span class="math">\(t\)</span>, while <span class="math">\(X\)</span> is still the stacked matrix of observed <span class="math">\(x_t\)</span> values. Now, we’re simply adding the assumption <span class="math">\(x_t \sim X_t\)</span>.</p>
<p>Let’s say that our new <span class="math">\(\beta\)</span> vector has terms for each day of the week; this means the matrix of stacked <span class="math">\(d(t)\)</span> values, <span class="math">\(D\)</span>, is some classical factor design matrix with levels for each day. The product <span class="math">\(d(t)^\top \beta\)</span> is then some scalar mean for the day corresponding to <span class="math">\(t\)</span>.</p>
<p>A simple substitution of this model for our previously constant <span class="math">\(X\)</span> matrix, results in a sort of hierarchical model, which we can now coherently marginalize and obtain the desired posterior predictive, <span class="math">\(p(Y \mid y)\)</span>. This time, the posterior predictive is independent of <span class="math">\(X_t\)</span>, so we can produce results for any <span class="math">\(t\)</span>.</p>
<p>The change in our complete model is relatively minimal. The model above for <span class="math">\(X\)</span> results in the following marginal observation model: </p>
<div class="math">$$\begin{aligned}
    \left(Y_t \mid \beta, \theta \right) &amp;\propto
    \int p(Y_t \mid X_t, \theta) p(X_t \mid \beta) dX
    \\
    &amp;\sim \operatorname{N}\left(
    d(t)^\top \beta \cdot \theta,
    \sigma^2 + \sigma_x^2 \cdot d(t)^\top \beta \beta^\top d(t) \right)
    \;.
  \end{aligned}$$</div>
</div>

<p>The reduction in Example \ref{ex:prior_extension} is quite reasonable and could be considered an entire re-definition of our initial observation model in \eqref{eq:normal-normal}. A change like this is a natural part of the standard model development cycle. However, this is not the only way to look at it. In the Bayesian setting we can keep the observation model fixed and iterate on the prior’s specification. The resulting marginal distribution could effectively be the same under both approaches (if desired), but the latter has the advantage of at least maintaining–conditionally–our earlier work.</p>
<div class="example" env-number="4" title-name="">
<p>We haven’t given a prior to <span class="math">\(\beta\)</span>, but if we did, in the absence of conflicting assumptions, we might want the product <span class="math">\(\beta \cdot \theta\)</span> to simplified to a single unknown variables of its own, so that we’re not estimating two “entangled” variables. This idea might be inspired by an understanding of the classical <a href="https://en.wikipedia.org/wiki/Parameter_identification_problem">identification</a> issue arising from such products.</p>
<p>With <span class="math">\(\beta\)</span> constant, the form of our marginal observation model is basically unchanged from our initial under <span class="math">\(x_t \to d(t)^\top \beta\)</span> and <span class="math">\(\sigma^2 \to \sigma^2 + \sigma_x^2 \cdot d(t)^\top \beta \beta^\top d(t)\)</span>.</p>
</div>
<p>Adherence to established models or industry standards is not uncommon. Outside of hierarchical model development, it can be very difficult to make these connections and coherently propagate statistical assumptions.</p>
<p>This model development process expands in complexity and applicability through natural and compartmental extensions of existing terms. Simpler, “base” models are found as marginalizations of the new terms, and all the same estimation techniques apply.</p>
<p>We’ll close with an illustration of the piecewise exogenous variable model described in Example \ref{ex:prior_extension}. A few days are added to demonstrate out-of-sample predictions and the design matrix, <span class="math">\(D\)</span>, for \eqref{eq:exogenous_model} is produced using <a href="https://patsy.readthedocs.io/en/latest/">Patsy</a>.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">patsy</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="n">ext_sim_index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">&#39;2016-01-01 12:00:00&#39;</span><span class="p">,</span>
                              <span class="n">end</span><span class="o">=</span><span class="s1">&#39;2016-01-16 12:00:00&#39;</span><span class="p">,</span>
                              <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;H&#39;</span><span class="p">)</span>

<span class="n">y_obs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_obs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sim_index</span><span class="p">,</span>
                        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">r&#39;y&#39;</span><span class="p">])</span>

<span class="c1"># The extra out-of-sample days are set to NaN</span>
<span class="n">y_obs_df</span> <span class="o">=</span> <span class="n">y_obs_df</span><span class="o">.</span><span class="n">reindex</span><span class="p">(</span><span class="n">ext_sim_index</span><span class="p">)</span>

<span class="c1"># Create some missing in-sample days</span>
<span class="n">missing_days_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">alen</span><span class="p">(</span><span class="n">y_obs</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">y_obs_df</span><span class="p">[</span><span class="n">missing_days_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

<span class="n">_</span><span class="p">,</span> <span class="n">D_df</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">dmatrices</span><span class="p">(</span><span class="s2">&quot;y ~ C(y.index.weekday)&quot;</span><span class="p">,</span>
                          <span class="n">y_obs_df</span><span class="o">.</span><span class="n">notnull</span><span class="p">(),</span>
                          <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;dataframe&#39;</span><span class="p">)</span>
</pre></div>


<p>Again, with PyMC3 our model and its extension are easily expressed, and the missing observations will be sampled automatically.</p>
<div class="highlight"><pre><span></span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;FAST_RUN&quot;</span>
<span class="k">del</span> <span class="n">model</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">tau2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">1e1</span><span class="p">,</span>
                     <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],))</span>
    <span class="n">mu_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">D_df</span><span class="p">,</span> <span class="n">beta</span><span class="p">))</span> <span class="o">*</span> <span class="n">theta</span>

    <span class="n">Y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="n">sigma2</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">y_obs_df</span><span class="o">.</span><span class="n">icol</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">sample_steps</span> <span class="o">=</span> <span class="p">[</span><span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">([</span><span class="n">theta</span><span class="p">]),</span>
                    <span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">([</span><span class="n">beta</span><span class="p">])]</span>
    <span class="k">if</span> <span class="n">Y</span><span class="o">.</span><span class="n">missing_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">sample_steps</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pm</span><span class="o">.</span><span class="n">Metropolis</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">missing_values</span><span class="p">)]</span>
    <span class="n">sample_traces</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">sample_steps</span><span class="p">)</span>
</pre></div>


<p>The posterior predictive results are plotted below.</p>
<div class="highlight"><pre><span></span><span class="n">ppc_samples</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">sample_traces</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="n">ppc_y_samples</span> <span class="o">=</span> <span class="n">ppc_samples</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span>

<span class="n">ppc_mean_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ppc_y_samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                           <span class="n">index</span><span class="o">=</span><span class="n">ext_sim_index</span><span class="p">,</span>
                           <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">r&#39;$E[Y \mid y]$&#39;</span><span class="p">])</span>
<span class="n">ppc_hpd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_y_samples</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span>
                       <span class="n">index</span><span class="o">=</span><span class="n">ext_sim_index</span><span class="p">)</span>

<span class="n">y_obs_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">subplots</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">y_obs_df</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">missing_days_idx</span><span class="p">],</span> <span class="o">*</span><span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span><span class="o">.</span><span class="n">get_ybound</span><span class="p">(),</span>
           <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">y_obs_df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                 <span class="n">ppc_hpd</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                 <span class="n">ppc_hpd</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">r&#39;$(Y \mid y)$ 95\</span><span class="si">% i</span><span class="s1">nterval&#39;</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ppc_mean_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_temp_ppc_plot_1.png" alt="Posterior predictive results for the stochastic X model" /></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','AMSmath.js','AMSsymbols.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

                    var disqus_identifier = 'regarding-statistical-model-specification-and-sample-results';
                var disqus_url = 'https://brandonwillard.github.io/regarding-statistical-model-specification-and-sample-results.html';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="/images/profile-pic.png"/>
        </p>
    <p>
        <strong>About Brandon T. Willard</strong><br/>
        applied math/stats person
    </p>
</div>
<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="http://linkedin.com/pub/brandon-willard/10/bb4/468/"><i class="fa fa-linkedin-square fa-lg"></i> linkedin</a></li>
                <li class="list-group-item"><a href="https://scholar.google.com/citations?user=g0oUxG4AAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-lg"></i> google scholar</a></li>
                <li class="list-group-item"><a href="https://plus.google.com/+brandonwillard"><i class="fa fa-google-plus-square fa-lg"></i> google+</a></li>
                <li class="list-group-item"><a href="https://bitbucket.org/brandonwillard"><i class="fa fa-bitbucket-square fa-lg"></i> bitbucket</a></li>
                <li class="list-group-item"><a href="https://github.com/brandonwillard"><i class="fa fa-github-square fa-lg"></i> github</a></li>
              </ul>
            </li>



            <li class="list-group-item"><a href="https://brandonwillard.github.io/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
                <ul class="list-group " id="tags">
                </ul>
            </li>
    </ul>
</section>
            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2016 Brandon T. Willard
            &middot; Powered by <a href="https://github.com/DandyDev/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://brandonwillard.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://brandonwillard.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://brandonwillard.github.io/theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->

</body>
</html>