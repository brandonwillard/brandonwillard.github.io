<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Brandon T. Willard</title><link>https://brandonwillard.github.io/</link><description></description><lastBuildDate>Mon, 06 Mar 2017 00:00:00 -0600</lastBuildDate><item><title>More Proximal Estimation</title><link>https://brandonwillard.github.io/more-proximal-estimation.html</link><description>&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;More Proximal Estimation&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code &gt; span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code &gt; span.dt { color: #0057ae; } /* DataType */
code &gt; span.dv { color: #b08000; } /* DecVal */
code &gt; span.bn { color: #b08000; } /* BaseN */
code &gt; span.fl { color: #b08000; } /* Float */
code &gt; span.cn { color: #aa5500; } /* Constant */
code &gt; span.ch { color: #924c9d; } /* Char */
code &gt; span.sc { color: #3daee9; } /* SpecialChar */
code &gt; span.st { color: #bf0303; } /* String */
code &gt; span.vs { color: #bf0303; } /* VerbatimString */
code &gt; span.ss { color: #ff5500; } /* SpecialString */
code &gt; span.im { color: #ff5500; } /* Import */
code &gt; span.co { color: #898887; } /* Comment */
code &gt; span.do { color: #607880; } /* Documentation */
code &gt; span.an { color: #ca60ca; } /* Annotation */
code &gt; span.cv { color: #0095ff; } /* CommentVar */
code &gt; span.ot { color: #006e28; } /* Other */
code &gt; span.fu { color: #644a9b; } /* Function */
code &gt; span.va { color: #0057ae; } /* Variable */
code &gt; span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code &gt; span.op { color: #1f1c1b; } /* Operator */
code &gt; span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code &gt; span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code &gt; span.pp { color: #006e28; } /* Preprocessor */
code &gt; span.at { color: #0057ae; } /* Attribute */
code &gt; span.re { color: #0057ae; } /* RegionMarker */
code &gt; span.in { color: #b08000; } /* Information */
code &gt; span.wa { color: #bf0303; } /* Warning */
code &gt; span.al { color: #bf0303; font-weight: bold; } /* Alert */
code &gt; span.er { color: #bf0303; text-decoration: underline; } /* Error */
code &gt; span. { color: #1f1c1b; } /* Normal */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;More Proximal Estimation&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–03–06&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The focal point of this short exposition will be an elaboration of &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt; via the numeric implementation of the basic &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; penalization problem, &lt;span class="math display"&gt;\[\begin{equation}
\operatorname*{argmin}_{\beta} \left\{
  \frac{1}{2} \|y - X \beta\|^2_2
    + \lambda \|\beta\|_1
  \right\}
  \;,
  \label{eq:lasso}
\end{equation}\]&lt;/span&gt; alongside some additional, related topics concerning automation and symbolic computation. Again, our framing of the problem is in terms of proximal methods &lt;span class="citation" data-cites="parikh_proximal_2014 combettes_proximal_2011"&gt;(Parikh and Boyd 2014; Combettes and Pesquet 2011)&lt;/span&gt;, how they relate to some common estimation practices–specifically, coordinate descent and line-searching–and their implementation in Theano &lt;span class="citation" data-cites="bergstra_theano_2010"&gt;(Bergstra et al. 2010)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We start by focusing on the connections with coordinate-wise path estimation of Equation &lt;span class="math inline"&gt;\(\eqref{eq:lasso}\)&lt;/span&gt;–and related problems–in the statistics literature. The goal is one among many &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt; steps intended to provide details of the already well established relationship between these problems and proximal methods. Minimally, these connections can contextualize the past developments and direction of work in this area, and–ideally–open up avenues for readily drawing from established work in proximal algorithms across numerous fields.&lt;/p&gt;
&lt;p&gt;Although Equation &lt;span class="math inline"&gt;\(\eqref{eq:lasso}\)&lt;/span&gt; is strictly &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt;–or Lasso, we simply allude to the relatively minor alterations that produce the ElasticNet, grouped Lasso and related models. Under the proximal framework, the distinctions between these models do not generally lead to major theoretical re-workings, or the need to consider principles not already found within the proximal methods.&lt;/p&gt;
&lt;p&gt;For example, the transition from standard Lasso to grouped Lasso may not be trivial, but–within the proximal framework–the change amounts to an intermediate operator composition step &lt;span class="citation" data-cites="argyriou_efficient_2011 hu_proximal"&gt;(Argyriou et al. 2011; Hu, Li, and Yang, n.d.)&lt;/span&gt;. This change can be introduced into existing proximal algorithms without significant alteration to the established steps. In some cases the exact change can be entirely transparent and only result in nominally different arguments to existing functions.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="proximal-and-computational-components" class="level1"&gt;
&lt;h1&gt;Proximal and Computational Components&lt;/h1&gt;
&lt;p&gt;First, the workhorse of the proximal framework: the &lt;em&gt;proximal operator&lt;/em&gt;.&lt;/p&gt;
&lt;div class="Def" markdown="" env-number="1" title-name="[Proximal Operator]"&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{prox}_{\phi}(x) =
    \operatorname*{argmin}_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspired by Equation &lt;span class="math inline"&gt;\(\eqref{eq:lasso}\)&lt;/span&gt;, we produce a toy dataset as follows:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared

M &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;50&lt;/span&gt;
M_nonzero &lt;span class="op"&gt;=&lt;/span&gt; M &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="op"&gt;//&lt;/span&gt; &lt;span class="dv"&gt;10&lt;/span&gt;

beta_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M)
beta_true[:M_nonzero] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(M_nonzero)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;

N &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(beta_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.4&lt;/span&gt;)
X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(N, M)
mu_true &lt;span class="op"&gt;=&lt;/span&gt; X.dot(beta_true)
y &lt;span class="op"&gt;=&lt;/span&gt; mu_true &lt;span class="op"&gt;+&lt;/span&gt; sc.stats.norm.rvs(np.zeros(N), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

&lt;span class="co"&gt;# Estimation starting parameters...&lt;/span&gt;
beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;]).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)

&lt;span class="co"&gt;# Gradient [starting] step size&lt;/span&gt;
alpha_0 &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;. &lt;span class="op"&gt;/&lt;/span&gt; np.linalg.norm(X, &lt;span class="dv"&gt;2&lt;/span&gt;)&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;
&lt;span class="co"&gt;# np.linalg.matrix_rank(X)&lt;/span&gt;

&lt;span class="co"&gt;# Regularization value heuristic&lt;/span&gt;
&lt;span class="co"&gt;# beta_ols = np.linalg.lstsq(X, y)[0]&lt;/span&gt;
&lt;span class="co"&gt;# lambda_max = 0.1 * np.linalg.norm(beta_ols, np.inf)&lt;/span&gt;
lambda_max &lt;span class="op"&gt;=&lt;/span&gt; np.linalg.norm(X.T.dot(y), np.inf)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;, we can start with a model defined within a system like PyMC3 &lt;span class="citation" data-cites="pymc3"&gt;(&lt;span class="citeproc-not-found" data-reference-id="pymc3"&gt;&lt;strong&gt;???&lt;/strong&gt;&lt;/span&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:
    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,
                         shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])
    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,
                     shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A system that worked in this way might then arrive at the necessary steps toward estimation through a sophisticated means of identifying the underlying &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; estimation problem.&lt;/p&gt;
&lt;p&gt;For simplicity we’ll just assume that all components of the estimation problem are know–i.e. loss and penalty functions. The proximal operator that arises in this standard example is the &lt;em&gt;soft thresholding&lt;/em&gt; operator. In Theano, it can be implemented with the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; tt_soft_threshold(beta_, lambda_):
    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="remark" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;This operator can take other forms, and the one used here is likely not the best. For instance, our use of &lt;code&gt;maximum&lt;/code&gt; can be replaced by other conditional-like statements taking the general form &lt;span class="math display"&gt;\[\begin{equation*}
\operatorname{S}(z, \lambda) =
    \begin{cases}
     {\mathop{\mathrm{sgn}}}(\beta) (\beta - \lambda) &amp;amp; \beta &amp;gt; \lambda
     \\
     0 &amp;amp; \text{otherwise}
    \end{cases}
    \;.
\end{equation*}\]&lt;/span&gt; If we were to–say–multiply the output of this operator with another, more difficult to compute result, then we might also wish to extend this multiplication into the definition of the operator and avoid its computation in the &lt;span class="math inline"&gt;\(\beta \leq \lambda\)&lt;/span&gt; case.&lt;/p&gt;
&lt;p&gt;Barring any reuses of this quantity, or a need to preserve undefined results produced by an expensive product with zero, we would ideally like a “compiler” to make such an optimization itself. It isn’t clear how a standard compiler–or interpreter/hybrid–could safely make this optimization, whereas it does seem more reasonable as a symbolic/Theano optimization.&lt;/p&gt;
&lt;p&gt;Optimizations like this are–I think–a necessary step to enable expressive, generalized methods, truly rapid prototyping at the math level.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, assuming that we’ve obtained the relevant loss and penalty functions–for example, in PyMC3–then we can proceed to setting up the exact context of our proximal problem.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone

&lt;span class="co"&gt;# Clone the negative log-likelihood of our observation model.&lt;/span&gt;
nlogl_rv &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt
nlogl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(nlogl_rv)
nlogl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;-logl&amp;quot;&lt;/span&gt;
beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_inputs([nlogl])[&lt;span class="dv"&gt;4&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="proximal-gradient" class="level1"&gt;
&lt;h1&gt;Proximal Gradient&lt;/h1&gt;
&lt;p&gt;In what follows it will be convenient to generalize a bit and work in terms of arbitrary loss and penalty functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, respectively, which in our case corresponds to &lt;span class="math display"&gt;\[\begin{equation*}
\begin{gathered}
  l(\beta) = \frac12 \|y - X \beta\|^2_2, \quad
  \text{and}\;
  \phi(\beta) = \|\beta\|_1
  \;.\end{gathered}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proximal gradient &lt;span class="citation" data-cites="combettes_proximal_2011"&gt;(Combettes and Pesquet 2011)&lt;/span&gt; algorithm is a staple of the proximal framework that provides solutions to problems of the form &lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{argmin}_\beta \left\{
    l(\beta) + \lambda \phi(\beta)
  \right\}
  \;.
\end{equation*}\]&lt;/span&gt; when both &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are lower semi-continuous convex functions, and &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; is differentiable with Lipschitz gradient.&lt;/p&gt;
&lt;p&gt;The solution is given as the following fixed-point: &lt;span class="math display"&gt;\[\begin{equation}
\beta = \operatorname*{prox}_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}\]&lt;/span&gt; The constant step size &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; is related to the Lipschitz constant of &lt;span class="math inline"&gt;\(\nabla l\)&lt;/span&gt;, but can more generally be a sequence obeying certain constraints. Since our &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; under consideration is &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt;, we have the incredibly standard &lt;span class="math inline"&gt;\(\nabla l(\beta) = X^\top (X \beta - y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;section id="implementation" class="level2"&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;As in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;, we provide an implementation of a proximal gradient step.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; function &lt;span class="im"&gt;as&lt;/span&gt; tt_function
&lt;span class="im"&gt;from&lt;/span&gt; theano.&lt;span class="bu"&gt;compile&lt;/span&gt;.nanguardmode &lt;span class="im"&gt;import&lt;/span&gt; NanGuardMode

tt_func_mode &lt;span class="op"&gt;=&lt;/span&gt; NanGuardMode(nan_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;,
                            inf_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                            big_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)


&lt;span class="kw"&gt;def&lt;/span&gt; prox_gradient_step(loss, beta_tt, prox_func,
                       alpha_tt&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;, lambda_tt&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;,
                       return_loss_grad&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                       tt_func_kwargs&lt;span class="op"&gt;=&lt;/span&gt;{&lt;span class="st"&gt;&amp;#39;mode&amp;#39;&lt;/span&gt;: tt_func_mode}
                       ):
    &lt;span class="co"&gt;r&amp;quot;&amp;quot;&amp;quot; Creates a function that produces a proximal gradient step.&lt;/span&gt;

&lt;span class="co"&gt;    Arguments&lt;/span&gt;
&lt;span class="co"&gt;    =========&lt;/span&gt;
&lt;span class="co"&gt;    loss: TensorVariable&lt;/span&gt;
&lt;span class="co"&gt;        Continuously differentiable &amp;quot;loss&amp;quot; function in the objective&lt;/span&gt;
&lt;span class="co"&gt;        function.&lt;/span&gt;
&lt;span class="co"&gt;    beta_tt: TensorVariable&lt;/span&gt;
&lt;span class="co"&gt;        Variable argument of the loss function.&lt;/span&gt;
&lt;span class="co"&gt;    prox_fn: function&lt;/span&gt;
&lt;span class="co"&gt;        Function that computes the proximal operator for the &amp;quot;penalty&amp;quot;&lt;/span&gt;
&lt;span class="co"&gt;        function.  Must take two parameters: the first a&lt;/span&gt;
&lt;span class="co"&gt;TensorVariable&lt;/span&gt;
&lt;span class="co"&gt;        of the gradient step, the second a float or Scalar value.&lt;/span&gt;
&lt;span class="co"&gt;    alpha_tt: float, Scalar (optional)&lt;/span&gt;
&lt;span class="co"&gt;        Gradient step size.&lt;/span&gt;
&lt;span class="co"&gt;    lambda_tt: float, Scalar (optional)&lt;/span&gt;
&lt;span class="co"&gt;        Additional scalar value passed to `prox_fn`.&lt;/span&gt;
&lt;span class="co"&gt;        &lt;/span&gt;&lt;span class="al"&gt;TODO&lt;/span&gt;&lt;span class="co"&gt;: Not sure if this should be here; is redundant.&lt;/span&gt;
&lt;span class="co"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    loss_grad &lt;span class="op"&gt;=&lt;/span&gt; tt.grad(loss, wrt&lt;span class="op"&gt;=&lt;/span&gt;beta_tt)
    loss_grad.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;loss_grad&amp;quot;&lt;/span&gt;

    &lt;span class="cf"&gt;if&lt;/span&gt; alpha_tt &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:
        alpha_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;)
        alpha_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;
    &lt;span class="cf"&gt;if&lt;/span&gt; lambda_tt &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:
        lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;)
        lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;

    beta_grad_step &lt;span class="op"&gt;=&lt;/span&gt; beta_tt &lt;span class="op"&gt;-&lt;/span&gt; alpha_tt &lt;span class="op"&gt;*&lt;/span&gt; loss_grad
    beta_grad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_grad_step&amp;quot;&lt;/span&gt;

    prox_grad_step &lt;span class="op"&gt;=&lt;/span&gt; prox_func(beta_grad_step, lambda_tt &lt;span class="op"&gt;*&lt;/span&gt; alpha_tt)
    prox_grad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;prox_grad_step&amp;quot;&lt;/span&gt;

    inputs &lt;span class="op"&gt;=&lt;/span&gt; []
    updates &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(beta_tt, tt.sharedvar.SharedVariable):
        updates &lt;span class="op"&gt;=&lt;/span&gt; [(beta_tt, prox_grad_step)]
    &lt;span class="cf"&gt;else&lt;/span&gt;:
        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [beta_tt]
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="kw"&gt;not&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(alpha_tt, tt.sharedvar.SharedVariable):
        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [alpha_tt]
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="kw"&gt;not&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(lambda_tt, tt.sharedvar.SharedVariable):
        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [lambda_tt]

    prox_grad_step_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function(inputs,
                                    prox_grad_step,
                                    updates&lt;span class="op"&gt;=&lt;/span&gt;updates,
                                    &lt;span class="op"&gt;**&lt;/span&gt;tt_func_kwargs)

    res &lt;span class="op"&gt;=&lt;/span&gt; (prox_grad_step_fn,)
    &lt;span class="cf"&gt;if&lt;/span&gt; return_loss_grad:
        res &lt;span class="op"&gt;+=&lt;/span&gt; (loss_grad,)

    &lt;span class="cf"&gt;return&lt;/span&gt; res&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function could also be implemented in a less “stateful” fashion, in which the terms are given as arguments to each function call–instead of shared variable.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="step-sizes" class="level2"&gt;
&lt;h2&gt;Step Sizes&lt;/h2&gt;
&lt;p&gt;A critical aspect of the proximal gradient approach involves the use of appropriate step sizes, &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt;. They needn’t always be fixed values, and, because of this, we can search for an appropriate value during estimation. These values have an obvious connection to the performance of a method–beyond basic guarantees of convergence–and sometimes Most often, there are ranges of such values that are easily specified by the Lipschitz properties of the operators involved. The same function-analytic considerations underlie the classical line-search methods in iterative optimization, give meaning to “tuning parameters”, and often highlight the need for more “mathematical” considerations in practice.&lt;/p&gt;
&lt;p&gt;In the spirit of more mathematical considerations and their automation, one particularly relevant direction can be found in Theano’s experimental matrix “Hints”. The ideas behind &lt;code&gt;theano.sandbox.linalg.ops.{psd, spectral_radius_bound}&lt;/code&gt; encompass some of the machinery needed to automatically determine applicable and efficient &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; constants and sequences.&lt;/p&gt;
&lt;p&gt;In our example, we use the standard backtracking line-search.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; backtracking_search(beta_, alpha_,
                        prox_fn, loss_fn, loss_grad_fn,
                        lambda_&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, bt_rate&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt;, obj_tol&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e-5&lt;/span&gt;):
    &lt;span class="co"&gt;# alpha_start = alpha_&lt;/span&gt;
    z &lt;span class="op"&gt;=&lt;/span&gt; beta_
    beta_start_ &lt;span class="op"&gt;=&lt;/span&gt; beta_
    loss_start_ &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(beta_)
    loss_grad_start_ &lt;span class="op"&gt;=&lt;/span&gt; loss_grad_fn(beta_)
    &lt;span class="cf"&gt;while&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;:

        beta_ &lt;span class="op"&gt;=&lt;/span&gt; beta_start_ &lt;span class="op"&gt;-&lt;/span&gt; alpha_ &lt;span class="op"&gt;*&lt;/span&gt; loss_grad_start_
        z &lt;span class="op"&gt;=&lt;/span&gt; prox_fn(beta_, alpha_ &lt;span class="op"&gt;*&lt;/span&gt; lambda_)
        &lt;span class="co"&gt;# x.step = x.last - alpha.run * x.grad&lt;/span&gt;
        &lt;span class="co"&gt;# z = fprox.fun(x.step, alpha.run * lambda)&lt;/span&gt;

        loss_z &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(z)
        step_diff &lt;span class="op"&gt;=&lt;/span&gt; z &lt;span class="op"&gt;-&lt;/span&gt; beta_start_
        loss_diff &lt;span class="op"&gt;=&lt;/span&gt; loss_z &lt;span class="op"&gt;-&lt;/span&gt; loss_start_
        line_diff &lt;span class="op"&gt;=&lt;/span&gt; alpha_ &lt;span class="op"&gt;*&lt;/span&gt; (loss_diff &lt;span class="op"&gt;-&lt;/span&gt;
loss_grad_start_.T.dot(step_diff))
        line_diff &lt;span class="op"&gt;-=&lt;/span&gt; step_diff.T.dot(step_diff) &lt;span class="op"&gt;/&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;.
        &lt;span class="co"&gt;# f.line.diff = alpha.run * (loss.fun(z) - loss.fun(x.last) -&lt;/span&gt;
        &lt;span class="co"&gt;#                     crossprod(x.grad, z - x.last)) -&lt;/span&gt;
crossprod(z &lt;span class="op"&gt;-&lt;/span&gt; x.last)&lt;span class="op"&gt;/&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;

        &lt;span class="cf"&gt;if&lt;/span&gt; line_diff &lt;span class="op"&gt;&amp;lt;=&lt;/span&gt; obj_tol:
            &lt;span class="cf"&gt;return&lt;/span&gt; z, alpha_, loss_z

        alpha_ &lt;span class="op"&gt;*=&lt;/span&gt; bt_rate
        &lt;span class="cf"&gt;assert&lt;/span&gt; alpha_ &lt;span class="op"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;invalid step size: &lt;/span&gt;&lt;span class="sc"&gt;{}&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;&lt;/span&gt;.&lt;span class="bu"&gt;format&lt;/span&gt;(alpha_)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="remark" markdown="" env-number="2" title-name=""&gt;
&lt;p&gt;Routines like this that make use of the gradient and other quantities might also be good candidates for execution in Theano, if only because of the graph optimizations that are able to remedy obviously redundant computations.&lt;/p&gt;
&lt;p&gt;In this vein, we could consider performing the line-search, and/or the entire optimization loop, within a Theano &lt;code&gt;scan&lt;/code&gt; operation. We could also create &lt;code&gt;Op&lt;/code&gt;s that represents gradient and line-search step. These might make graph construction much simpler, and be more suited for the current optimization framework.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;scan&lt;/code&gt; and tighter Theano integration may not on average produce better results than our current use of its compiled functions, we still wish to emphasize the possibilities.&lt;/p&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="examples" class="level1"&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;First, we need to set up the basic functions, which–in this case–are constructed from the Theano graphs.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;)
lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;

prox_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt, lambda_tt],
                      tt_soft_threshold(beta_tt, lambda_tt))

prox_grad_step_fn, loss_grad &lt;span class="op"&gt;=&lt;/span&gt; prox_gradient_step(
    nlogl, beta_tt, tt_soft_threshold,
    return_loss_grad&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

loss_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt], nlogl)
loss_grad_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt], loss_grad)

cols_fns &lt;span class="op"&gt;=&lt;/span&gt; [
    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: i, &lt;span class="vs"&gt;r&amp;#39;$i$&amp;#39;&lt;/span&gt;),
    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: np.asscalar(loss_fn(b)),
        &lt;span class="vs"&gt;r&amp;#39;$l(\beta^{(i)})$&amp;#39;&lt;/span&gt;),
    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: np.linalg.norm(b &lt;span class="op"&gt;-&lt;/span&gt; beta_true, &lt;span class="dv"&gt;2&lt;/span&gt;),
        &lt;span class="vs"&gt;r&amp;#39;$\|\beta^{(i)} - \beta^*\|^2_2$&amp;#39;&lt;/span&gt;)
]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a baseline comparison–and sanity check–we’ll use the &lt;code&gt;cvxpy&lt;/code&gt; library &lt;span class="citation" data-cites="diamond_cvxpy:_2016"&gt;(Diamond and Boyd 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; cvxpy &lt;span class="im"&gt;as&lt;/span&gt; cvx

beta_var_cvx &lt;span class="op"&gt;=&lt;/span&gt; cvx.Variable(M, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;)
lambda_cvx &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max &lt;span class="op"&gt;*&lt;/span&gt; N

cvx_obj &lt;span class="op"&gt;=&lt;/span&gt; cvx.Minimize(&lt;span class="fl"&gt;0.5&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; cvx.sum_squares(y &lt;span class="op"&gt;-&lt;/span&gt; X &lt;span class="op"&gt;*&lt;/span&gt; beta_var_cvx)
                       &lt;span class="op"&gt;+&lt;/span&gt; lambda_cvx &lt;span class="op"&gt;*&lt;/span&gt; cvx.norm(beta_var_cvx, &lt;span class="dv"&gt;1&lt;/span&gt;) )
cvx_prob &lt;span class="op"&gt;=&lt;/span&gt; cvx.Problem(cvx_obj)

_ &lt;span class="op"&gt;=&lt;/span&gt; cvx_prob.solve(solver&lt;span class="op"&gt;=&lt;/span&gt;cvx.CVXOPT, verbose&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

beta_cvx &lt;span class="op"&gt;=&lt;/span&gt; np.asarray(beta_var_cvx.value).squeeze()
loss_cvx &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(beta_cvx)
beta_cvx_err &lt;span class="op"&gt;=&lt;/span&gt; np.linalg.norm(beta_cvx &lt;span class="op"&gt;-&lt;/span&gt; beta_true, &lt;span class="dv"&gt;2&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We now have the necessary pieces to perform an example estimation. We’ll start with an exceedingly large step size and let backtracking line-search find a good value.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; ProxGradient(&lt;span class="bu"&gt;object&lt;/span&gt;):

    &lt;span class="kw"&gt;def&lt;/span&gt; &lt;span class="fu"&gt;__init__&lt;/span&gt;(&lt;span class="va"&gt;self&lt;/span&gt;, y, X, beta_0,
                 prox_fn_, loss_fn_, loss_grad_fn_,
                 alpha_0):

        &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;=&lt;/span&gt; y
        &lt;span class="va"&gt;self&lt;/span&gt;.X &lt;span class="op"&gt;=&lt;/span&gt; X
        &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val &lt;span class="op"&gt;=&lt;/span&gt; alpha_0
        &lt;span class="va"&gt;self&lt;/span&gt;.beta_0 &lt;span class="op"&gt;=&lt;/span&gt; beta_0
        &lt;span class="va"&gt;self&lt;/span&gt;.N, &lt;span class="va"&gt;self&lt;/span&gt;.M &lt;span class="op"&gt;=&lt;/span&gt; X.shape
        &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_ &lt;span class="op"&gt;=&lt;/span&gt; prox_fn_
        &lt;span class="va"&gt;self&lt;/span&gt;.loss_fn_ &lt;span class="op"&gt;=&lt;/span&gt; loss_fn_
        &lt;span class="va"&gt;self&lt;/span&gt;.loss_grad_fn_ &lt;span class="op"&gt;=&lt;/span&gt; loss_grad_fn_

    &lt;span class="kw"&gt;def&lt;/span&gt; step(&lt;span class="va"&gt;self&lt;/span&gt;, beta):
        beta_val &lt;span class="op"&gt;=&lt;/span&gt; np.copy(beta)

        beta_val, &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val, _ &lt;span class="op"&gt;=&lt;/span&gt; backtracking_search(
            beta_val, &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val,
            &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_, &lt;span class="va"&gt;self&lt;/span&gt;.loss_fn_, &lt;span class="va"&gt;self&lt;/span&gt;.loss_grad_fn_)

        &lt;span class="cf"&gt;return&lt;/span&gt; beta_val&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)
lambda_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max
pg_step &lt;span class="op"&gt;=&lt;/span&gt; ProxGradient(y, X, beta_0,
                       &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_val &lt;span class="op"&gt;*&lt;/span&gt; a),
                       loss_fn, loss_grad_fn, &lt;span class="dv"&gt;10&lt;/span&gt;)

pg_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns &lt;span class="op"&gt;+&lt;/span&gt; [(&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: pg_step.alpha_val,
&lt;span class="vs"&gt;r&amp;#39;$\alpha$&amp;#39;&lt;/span&gt;)]
pg_est_data, _ &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(pg_step, loss_fn, pg_cols_fns)
pg_ls_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(pg_est_data)
&lt;span class="co"&gt;# pg_ls_data = pg_ls_data.append(pg_est_data, ignore_index=True)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_ls_plot"&gt;&lt;span id="fig:pg_ls_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{2}\label{fig:pg_ls_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_ls_plot_1.png" title="fig:" alt="Minimization by proximal gradient with backtracking line-search." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;span class="math inline"&gt;\(\ref{fig:pg_ls_plot}\)&lt;/span&gt; shows the results for proximal gradient steps alongside the step size changes due to backtracking line-search. Regarding the latter, for our example a sufficient step size is found in the first few iterations, so the overall result isn’t too interesting. Fortunately, this sort of behaviour isn’t uncommon, and it makes line-search quite effective in practice.&lt;/p&gt;
&lt;p&gt;The other most noticeable result in Figure &lt;span class="math inline"&gt;\(\ref{fig:pg_ls_plot}\)&lt;/span&gt; is the quick decrease in loss for the first 20 or so iterations, then the very gradual convergence iterates &lt;span class="math inline"&gt;\(\beta^{(i)}\)&lt;/span&gt;. This is nice property to contrast with the coordinate-wise estimation in the next section, so we’ll revisit that; however, it’s worth mentioning now that the choice of stopping criteria should probably be informed by the optimization method used and its convergence properties–among other things.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="coordinate-wise-estimation" class="level1"&gt;
&lt;h1&gt;Coordinate-wise Estimation&lt;/h1&gt;
&lt;p&gt;Given that our loss is a composition of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and a linear operator of finite dimension, we can conveniently exploit some conditional separability to obtain simple estimation steps in each coordinate. This effectively characterizes coordinate–or cyclic–descent in our case, and it shows up as a common technique in the estimation of &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; models &lt;span class="citation" data-cites="friedman_pathwise_2007 mazumder_regularization_2009 scikit-learn_sklearn.linear_model.elasticnet_2017"&gt;(Friedman et al. 2007; Mazumder, Hastie, and Tibshirani 2009; scikit-learn 2017)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From a more statistical perspective, the basics of coordinate-wise methods begin with the “partial residuals”, &lt;span class="math inline"&gt;\(r_{-m} \in {{\mathbb{R}}}^{N}\)&lt;/span&gt;, discussed in &lt;span class="citation" data-cites="friedman_pathwise_2007"&gt;Friedman et al. (2007)&lt;/span&gt; and implicitly defined for our problem by &lt;span class="math display"&gt;\[\begin{equation}
\begin{aligned}
    \beta^*
    &amp;amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|
    y - X(\beta - e_m \beta_m)
        - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
      \right\}
    \\
    &amp;amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|r_{-m} - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
    \right\}
  \;.
  \end{aligned}
  \label{eq:partial_resid}
\end{equation}\]&lt;/span&gt; The last expression hints at the most basic idea behind the coordinate-wise approach: conditional minimization in each &lt;span class="math inline"&gt;\(m\)&lt;/span&gt;. Its exact solution in each coordinate is given by the aforementioned soft thresholding function, which–as we’ve already stated–is a proximal operator. In symbols, &lt;span class="math inline"&gt;\(\operatorname*{prox}_{\lambda \left|\cdot\right|}(x) = \operatorname{S}_\lambda(x)\)&lt;/span&gt;, where the latter is the soft thresholding operator.&lt;/p&gt;
&lt;p&gt;Now, if we wanted to relate this type of coordinate-descent to the much more general statement of the proximal gradient solution in Equation &lt;span class="math inline"&gt;\(\eqref{eq:forward-backward}\)&lt;/span&gt;, we would use a result like the following:&lt;/p&gt;
&lt;div id="eq:prox_grad_descent" class="proposition" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;&lt;span id="eq:prox_grad_descent_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{1}\label{eq:prox_grad_descent}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; such that &lt;span class="math inline"&gt;\({{\bf 1}}^\top X e_m = 0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(e^\top_m X^\top X e_m = 1\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(m \in \{1, \dots, M\}\)&lt;/span&gt;, the coordinate-wise step of the Lasso in &lt;span class="citation" data-cites="friedman_pathwise_2007"&gt;Friedman et al. (2007 Equation (9))&lt;/span&gt;, &lt;span class="math display"&gt;\[\begin{equation*}
\beta_m = \operatorname{S}_{\lambda}\left[
      \sum_{n}^N X_{n,m} \left(
      y_n - \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime}
      \right)
    \right]
    \;,
\end{equation*}\]&lt;/span&gt; has a proximal gradient fixed-point solution under a Euclidean basis decomposition with the form &lt;span class="math display"&gt;\[\begin{equation*}
\beta =
    \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
      e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
    \right] e_m
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We start with an expansion of the terms in &lt;span class="math inline"&gt;\(\operatorname*{prox}_{\lambda \phi} \equiv \operatorname{S}_\lambda\)&lt;/span&gt;. After simplifying the notation with &lt;span class="math display"&gt;\[\begin{equation*}
\begin{gathered}
    \sum^N_{n} X_{n,m} z_n = e^\top_m X^\top z, \quad \text{and} \quad
    \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime} =
    X \left(\beta - \beta_m e_m \right)
    \;,
  \end{gathered}
\end{equation*}\]&lt;/span&gt; the expanded argument of &lt;span class="math inline"&gt;\(\operatorname{S}\)&lt;/span&gt; reduces to &lt;span class="math display"&gt;\[\begin{equation*}
\begin{aligned}
      e^\top_m X^\top \left(y - X\left( \beta - e_m \beta_m\right)\right)
      &amp;amp;= e^\top_m X^\top X e_m \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;amp;= \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;amp;= e^\top_m \left(\beta + X^\top \left(y - X \beta\right)\right)
    \end{aligned}
\end{equation*}\]&lt;/span&gt; where the last step follows from &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; standardization. This establishes the relationship with only component-wise. The connection to full vector &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; iterations in comes from the following orthogonal basis property of proximal operators: &lt;span class="math display"&gt;\[\begin{equation}
\operatorname*{prox}_{\lambda \phi \circ e^\top_m}(z) =
    \sum^M_m \operatorname*{prox}_{\lambda \phi}\left(e^\top_m z\right) e_m
    \;.
    \label{eq:prox_ortho_basis}
\end{equation}\]&lt;/span&gt; This together with &lt;span class="math inline"&gt;\(z = \beta - \alpha \nabla l(\beta)\)&lt;/span&gt; yields the proximal gradient fixed-point statement, i.e. &lt;span class="math display"&gt;\[\begin{equation*}
\begin{aligned}
      \beta
      &amp;amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
    e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
      \right] e_m
      \\
      &amp;amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left(
      \beta_m + \alpha e_m^\top X^\top \left(y - X \beta \right)
      \right) e_m
      \;.
    \end{aligned}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Association with the proximal gradient algorithm can provide a fairly general means of expand and characterizing existing coordinate descent approaches. Perhaps the most readily available improvements involve sequence acceleration &lt;span class="citation" data-cites="beck_fast_2014"&gt;(Beck and Teboulle 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;div id="rem:bases" class="remark" markdown="" env-number="3" title-name=""&gt;
&lt;p&gt;&lt;span id="rem:bases_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{3}\label{rem:bases}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The property in can be extended to other orthonormal bases. This might be a means of relating &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;-based orthogonalizations of a regression problem, enforce different forms of sparsity on &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; estimates, or to simply investigate convergence properties.&lt;/p&gt;
&lt;/div&gt;
&lt;section id="implementation-1" class="level2"&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;The following performs a standard form of coordinate descent:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; CoordDescent(&lt;span class="bu"&gt;object&lt;/span&gt;):

    &lt;span class="kw"&gt;def&lt;/span&gt; &lt;span class="fu"&gt;__init__&lt;/span&gt;(&lt;span class="va"&gt;self&lt;/span&gt;, y, X, beta_0, prox_fn_, col_seq&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;):

        &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;=&lt;/span&gt; y
        &lt;span class="va"&gt;self&lt;/span&gt;.X &lt;span class="op"&gt;=&lt;/span&gt; X
        &lt;span class="va"&gt;self&lt;/span&gt;.beta_0 &lt;span class="op"&gt;=&lt;/span&gt; beta_0
        &lt;span class="va"&gt;self&lt;/span&gt;.N, &lt;span class="va"&gt;self&lt;/span&gt;.M &lt;span class="op"&gt;=&lt;/span&gt; X.shape
        &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;=&lt;/span&gt; np.dot(&lt;span class="va"&gt;self&lt;/span&gt;.X, &lt;span class="va"&gt;self&lt;/span&gt;.beta_0)
        &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_ &lt;span class="op"&gt;=&lt;/span&gt; prox_fn_

        &lt;span class="co"&gt;# (Inverse) 2-norm of each column/feature, i.e.&lt;/span&gt;
        &lt;span class="co"&gt;#   np.reciprocal(np.diag(np.dot(X.T, X)))&lt;/span&gt;
        &lt;span class="co"&gt;#&lt;/span&gt;
        &lt;span class="co"&gt;# Notice how much larger each coordinate step is compared&lt;/span&gt;
        &lt;span class="co"&gt;# to `alpha_0`.&lt;/span&gt;
        &lt;span class="va"&gt;self&lt;/span&gt;.alpha_vals &lt;span class="op"&gt;=&lt;/span&gt; np.reciprocal((&lt;span class="va"&gt;self&lt;/span&gt;.X&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;).&lt;span class="bu"&gt;sum&lt;/span&gt;(axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;))

        &lt;span class="cf"&gt;if&lt;/span&gt; col_seq &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:
            &lt;span class="va"&gt;self&lt;/span&gt;.col_seq &lt;span class="op"&gt;=&lt;/span&gt; np.arange(&lt;span class="va"&gt;self&lt;/span&gt;.M)

    &lt;span class="kw"&gt;def&lt;/span&gt; reset(&lt;span class="va"&gt;self&lt;/span&gt;):
        &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;=&lt;/span&gt; np.dot(&lt;span class="va"&gt;self&lt;/span&gt;.X, &lt;span class="va"&gt;self&lt;/span&gt;.beta_0)

    &lt;span class="kw"&gt;def&lt;/span&gt; step(&lt;span class="va"&gt;self&lt;/span&gt;, beta):
        beta_val &lt;span class="op"&gt;=&lt;/span&gt; np.copy(beta)

        &lt;span class="cf"&gt;for&lt;/span&gt; j &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.col_seq:
            X_j &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.X[:, j]
            alpha_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.alpha_vals[j]

            &lt;span class="co"&gt;# A little cheaper to just subtract the column&amp;#39;s&lt;/span&gt;
contribution...
            &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;-=&lt;/span&gt; X_j &lt;span class="op"&gt;*&lt;/span&gt; beta_val[j]

            Xt_r &lt;span class="op"&gt;=&lt;/span&gt; np.dot(X_j.T, &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;-&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.Xb) &lt;span class="op"&gt;*&lt;/span&gt; alpha_val
            beta_val[j] &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_(np.atleast_1d(Xt_r),
alpha_val)

            &lt;span class="co"&gt;# ...and add the updated column back.&lt;/span&gt;
            &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;+=&lt;/span&gt; X_j &lt;span class="op"&gt;*&lt;/span&gt; beta_val[j]

        &lt;span class="va"&gt;self&lt;/span&gt;.beta_last &lt;span class="op"&gt;=&lt;/span&gt; beta_val

        &lt;span class="cf"&gt;return&lt;/span&gt; beta_val&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)
lambda_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max
cd_step &lt;span class="op"&gt;=&lt;/span&gt; CoordDescent(y, X, beta_0,
                       &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_val &lt;span class="op"&gt;*&lt;/span&gt; a))

cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns &lt;span class="op"&gt;+&lt;/span&gt; [(&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: j, &lt;span class="st"&gt;&amp;quot;replication&amp;quot;&lt;/span&gt;)]

pg_coord_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame()
&lt;span class="cf"&gt;for&lt;/span&gt; j &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;range&lt;/span&gt;(&lt;span class="dv"&gt;15&lt;/span&gt;):
    est_data, _ &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(cd_step, loss_fn, cd_cols_fns)
    pg_coord_data &lt;span class="op"&gt;=&lt;/span&gt; pg_coord_data.append(est_data,
                                         ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
    &lt;span class="co"&gt;# Reset internal state of our step method, since we&amp;#39;re&lt;/span&gt;
    &lt;span class="co"&gt;# running multiple replications.&lt;/span&gt;
    cd_step.reset()
    np.random.shuffle(cd_step.col_seq)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_coord_plot"&gt;&lt;span id="fig:pg_coord_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{3}\label{fig:pg_coord_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_coord_plot_1.png" title="fig:" alt="Minimization by coordinate descent." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;span class="math inline"&gt;\(\ref{fig:pg_coord_plot}\)&lt;/span&gt; demonstrates how well coordinate descent can work on some problems. The number of iterations between coordinate descent and “batch” proximal gradient is fairly noticeable in this example, yet both approach effectively the same limits as &lt;code&gt;cvx&lt;/code&gt;. It seems like the same ideas behind most batched vs. non-batched gradient and sampling methods (e.g. batched Gibbs sampling) are in effect here, as well. In those cases, spectral properties of the &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;, or &lt;span class="math inline"&gt;\(X^\top X\)&lt;/span&gt;, space often play a big part, and this idea connects back to our comment in &lt;span class="math inline"&gt;\(\ref{rem:bases}\)&lt;/span&gt;.&lt;/p&gt;
&lt;section id="regularization-paths" class="level3"&gt;
&lt;h3&gt;Regularization Paths&lt;/h3&gt;
&lt;p&gt;Also, due to the relatively fast convergence of coordinate descent, the method is a little more suitable for the computation of regularization paths– i.e. varying &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; between iterations. There is much more to this topic, but for simplicity let’s just say that each &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; step has a “warm-start” from the previous descent iteration.&lt;/p&gt;
&lt;p&gt;Next, we make a small extension to demonstrate the computation of regularization paths–using &lt;code&gt;lasso_path&lt;/code&gt; for comparison.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; sklearn.linear_model &lt;span class="im"&gt;import&lt;/span&gt; lasso_path, enet_path

beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)

lambda_path, beta_path, _ &lt;span class="op"&gt;=&lt;/span&gt; lasso_path(X, y)
path_len &lt;span class="op"&gt;=&lt;/span&gt; np.alen(lambda_path)

beta_last &lt;span class="op"&gt;=&lt;/span&gt; beta_0
pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame()
&lt;span class="cf"&gt;for&lt;/span&gt; i, lambda_ &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;enumerate&lt;/span&gt;(lambda_path):
    cd_path_step &lt;span class="op"&gt;=&lt;/span&gt; CoordDescent(y, X, beta_last,
                        &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_ &lt;span class="op"&gt;*&lt;/span&gt; a))

    cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns[&lt;span class="dv"&gt;1&lt;/span&gt;:] &lt;span class="op"&gt;+&lt;/span&gt; [
        (&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: lambda_, &lt;span class="vs"&gt;r&amp;#39;$\lambda$&amp;#39;&lt;/span&gt;)]
    est_data, beta_last &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(cd_path_step, loss_fn,
                                        cd_cols_fns,
                                        stop_tol&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e-4&lt;/span&gt;,
                                        stop_loss&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

    pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.append(est_data.iloc[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, :],
                                       ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns[&lt;span class="dv"&gt;1&lt;/span&gt;:] &lt;span class="op"&gt;+&lt;/span&gt; [
    (&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: lambda_path[args[&lt;span class="dv"&gt;0&lt;/span&gt;]], &lt;span class="vs"&gt;r&amp;#39;$\lambda$&amp;#39;&lt;/span&gt;)]

iter_values &lt;span class="op"&gt;=&lt;/span&gt; []
&lt;span class="cf"&gt;for&lt;/span&gt; i, beta_ &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;enumerate&lt;/span&gt;(beta_path.T):
    iter_values.append([col_fn(i, beta_)
                        &lt;span class="cf"&gt;for&lt;/span&gt; col_fn, _ &lt;span class="kw"&gt;in&lt;/span&gt; cd_cols_fns])

sklearn_path_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(iter_values,
                                 columns&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="bu"&gt;zip&lt;/span&gt;(&lt;span class="op"&gt;*&lt;/span&gt;cd_cols_fns)[&lt;span class="dv"&gt;1&lt;/span&gt;])
sklearn_path_data &lt;span class="op"&gt;=&lt;/span&gt; sklearn_path_data.assign(
    replication&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;, &lt;span class="bu"&gt;type&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;sklearn&amp;#39;&lt;/span&gt;)

pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.assign(&lt;span class="bu"&gt;type&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;pg&amp;#39;&lt;/span&gt;)
pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.append(sklearn_path_data,
                                   ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_path_plot"&gt;&lt;span id="fig:pg_path_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{4}\label{fig:pg_path_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_path_plot_1.png" title="fig:" alt="Regularization paths via coordinate descent." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We would like to note that related developments in symbolic computing and automated optimization can be found in &lt;span class="citation" data-cites="wytock_new_2016"&gt;(Wytock et al. 2016)&lt;/span&gt;. The particular value and distinction we’ve attempted to emphasized in symbolic tools is not just their ability to act as a compiler at the math level, but more for their ability to encode the potentially necessary relationships found in the mathematical characterizations of optimization problems. This could imply–say–Theano optimizations that make direct use of the Moreau-Fenchel theorem, and automate consideration for multiple estimation methods via splitting (e.g. ADMM, Douglas-Rachford, etc.)–perhaps making decisions based on operator/matrix properties one could only reasonably specific in this context.&lt;/p&gt;
&lt;p&gt;Maybe small steps in this direction could even motivate much larger changes in our standard tools–like full support for sub-differentials and set-valued results, instead of the overly restrictive point-wise characterizations currently ubiquitous. A similar idea is present in the symbolic calculation of limits addressed using filters &lt;span class="citation" data-cites="beeson_meaning_2005"&gt;(Beeson and Wiedijk 2005)&lt;/span&gt;, and is somewhat analogous to the more modern jump from matrix and linear algebra libraries to tensor libraries. Our use of the proximal framework is, in part, motivated by its near seamless use &lt;em&gt;and&lt;/em&gt; simultaneous bypassing of set-valued maps–in implementation, at least. In a sense, frameworks like these could be minor abstractions to libraries with set-valued support.&lt;/p&gt;
&lt;p&gt;In future installments we’ll attempt to delve into the details of these features.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-argyriou_efficient_2011"&gt;
&lt;p&gt;Argyriou, Andreas, Charles A. Micchelli, Massimiliano Pontil, Lixin Shen, and Yuesheng Xu. 2011. “Efficient First Order Methods for Linear Composite Regularizers,” 1–19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-beck_fast_2014"&gt;
&lt;p&gt;Beck, Amir, and Marc Teboulle. 2014. “A Fast Dual Proximal Gradient Algorithm for Convex Minimization and Applications.” &lt;em&gt;Operations Research Letters&lt;/em&gt; 42 (1): 1–6. &lt;a href="http://www.sciencedirect.com/science/article/pii/S0167637713001454" class="uri"&gt;http://www.sciencedirect.com/science/article/pii/S0167637713001454&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-beeson_meaning_2005"&gt;
&lt;p&gt;Beeson, Michael, and Freek Wiedijk. 2005. “The Meaning of Infinity in Calculus and Computer Algebra Systems.” &lt;em&gt;Journal of Symbolic Computation&lt;/em&gt;, Automated reasoning and computer algebra systems (ar-ca)AR-ca, 39 (5): 523–38. doi:&lt;a href="https://doi.org/10.1016/j.jsc.2004.12.002"&gt;10.1016/j.jsc.2004.12.002&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-bergstra_theano_2010"&gt;
&lt;p&gt;Bergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. “Theano: A CPU and GPU Math Expression Compiler.” In &lt;em&gt;Proceedings of the Python for Scientific Computing Conference (SciPy)&lt;/em&gt;. Austin, TX.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-combettes_proximal_2011"&gt;
&lt;p&gt;Combettes, Patrick L, and Jean-Christophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” &lt;em&gt;Fixed-Point Algorithms for Inverse Problems in Science and Engineering&lt;/em&gt;, 185–212.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-diamond_cvxpy:_2016"&gt;
&lt;p&gt;Diamond, Steven, and Stephen Boyd. 2016. “CVXPY: A Python-Embedded Modeling Language for Convex Optimization.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 17 (83): 1–5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-friedman_pathwise_2007"&gt;
&lt;p&gt;Friedman, Jerome, Trevor Hastie, Holger Höfling, Robert Tibshirani, and others. 2007. “Pathwise Coordinate Optimization.” &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 1 (2): 302–32. &lt;a href="http://projecteuclid.org/euclid.aoas/1196438020" class="uri"&gt;http://projecteuclid.org/euclid.aoas/1196438020&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-hu_proximal"&gt;
&lt;p&gt;Hu, YH, C Li, and XQ Yang. n.d. “Proximal Gradient Algorithm for Group Sparse Optimization.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mazumder_regularization_2009"&gt;
&lt;p&gt;Mazumder, Rahul, Trevor Hastie, and Rob Tibshirani. 2009. “Regularization Methods for Learning Incomplete Matrices.” &lt;em&gt;arXiv Preprint arXiv:0906.2034&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/0906.2034" class="uri"&gt;https://arxiv.org/abs/0906.2034&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-parikh_proximal_2014"&gt;
&lt;p&gt;Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” &lt;em&gt;Foundations and Trends in Optimization&lt;/em&gt; 1 (3): 123–231. doi:&lt;a href="https://doi.org/10.1561/2400000003"&gt;10.1561/2400000003&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. doi:&lt;a href="https://doi.org/10.1214/15-STS530"&gt;10.1214/15-STS530&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-scikit-learn_sklearn.linear_model.elasticnet_2017"&gt;
&lt;p&gt;scikit-learn. 2017. “Sklearn.Linear_model.ElasticNet Scikit-Learn 0.19.Dev0 Documentation.” &lt;a href="http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet" class="uri"&gt;http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-willard_role_2017"&gt;
&lt;p&gt;Willard, Brandon T. 2017. “A Role for Symbolic Computation in the General Estimation of Statistical Models.” &lt;a href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html" class="uri"&gt;https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-wytock_new_2016"&gt;
&lt;p&gt;Wytock, Matt, Steven Diamond, Felix Heide, and Stephen Boyd. 2016. “A New Architecture for Optimization Modeling Frameworks.” &lt;em&gt;arXiv Preprint arXiv:1609.03488&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/1609.03488" class="uri"&gt;https://arxiv.org/abs/1609.03488&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brandon T. Willard</dc:creator><pubDate>Mon, 06 Mar 2017 00:00:00 -0600</pubDate><guid isPermaLink="false">tag:brandonwillard.github.io,2017-03-06:more-proximal-estimation.html</guid></item><item><title>A Role for Symbolic Computation in the General Estimation of Statistical Models</title><link>https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</link><description>&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code &gt; span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code &gt; span.dt { color: #0057ae; } /* DataType */
code &gt; span.dv { color: #b08000; } /* DecVal */
code &gt; span.bn { color: #b08000; } /* BaseN */
code &gt; span.fl { color: #b08000; } /* Float */
code &gt; span.cn { color: #aa5500; } /* Constant */
code &gt; span.ch { color: #924c9d; } /* Char */
code &gt; span.sc { color: #3daee9; } /* SpecialChar */
code &gt; span.st { color: #bf0303; } /* String */
code &gt; span.vs { color: #bf0303; } /* VerbatimString */
code &gt; span.ss { color: #ff5500; } /* SpecialString */
code &gt; span.im { color: #ff5500; } /* Import */
code &gt; span.co { color: #898887; } /* Comment */
code &gt; span.do { color: #607880; } /* Documentation */
code &gt; span.an { color: #ca60ca; } /* Annotation */
code &gt; span.cv { color: #0095ff; } /* CommentVar */
code &gt; span.ot { color: #006e28; } /* Other */
code &gt; span.fu { color: #644a9b; } /* Function */
code &gt; span.va { color: #0057ae; } /* Variable */
code &gt; span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code &gt; span.op { color: #1f1c1b; } /* Operator */
code &gt; span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code &gt; span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code &gt; span.pp { color: #006e28; } /* Preprocessor */
code &gt; span.at { color: #0057ae; } /* Attribute */
code &gt; span.re { color: #0057ae; } /* RegionMarker */
code &gt; span.in { color: #b08000; } /* Information */
code &gt; span.wa { color: #bf0303; } /* Warning */
code &gt; span.al { color: #bf0303; font-weight: bold; } /* Alert */
code &gt; span.er { color: #bf0303; text-decoration: underline; } /* Error */
code &gt; span. { color: #1f1c1b; } /* Normal */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–01–18&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this document we describe how symbolic computation can be used to provide generalizable statistical estimation through a combination of existing open source frameworks.&lt;/p&gt;
&lt;p&gt;Specifically, we consider the optimization problems resulting from models with non-smooth objective functions. These problems arise in the context of regularization and shrinkage, and here we’ll address their automated application within the &lt;em&gt;proximal framework&lt;/em&gt; &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt;. In &lt;span class="citation" data-cites="polson_proximal_2015"&gt;Polson, Scott, and Willard (2015)&lt;/span&gt; we outlined a set of seemingly disparate optimization techniques within the fields of statistics, computer vision, and machine learning, that are unified by the types of functional bounds employed, the tools of convex analysis and the language of operator theory. These methods–and the concepts behind them–have found much success in recent times and admit quite a few interesting paths for research.&lt;/p&gt;
&lt;p&gt;In at least a few cases, the work required to produce a proximal algorithm overlaps with some highly functional areas in computer algebra and symbolic mathematics. For instance, some proximal operators–the main ingredient within proximal algorithms–can be solved exactly with symbolic differentiation and algebraic equation solvers. With perhaps only the addition of sophisticated variable assumptions and an ability to manipulate inequalities, even larger &lt;em&gt;classes&lt;/em&gt; of proximal operators could be solved. We refer the reader to the table in &lt;span class="citation" data-cites="polson_proximal_2015"&gt;Polson, Scott, and Willard (2015)&lt;/span&gt; for examples of operators that are currently solved–by hand–using roughly the same means.&lt;/p&gt;
&lt;p&gt;The kind of automation proposed here only begins to answer a problem that arises somewhat naturally in these areas: how does one provide access to methods that produce–or apply to–numerous distinct problems and solutions. Instead of the common attempt to implement each model or method separately, and then combine them under a loosely organized API or function interface, the symbolic approach offers wider applicability and concise implementations in terms of code that maps directly to the mathematics. Also, it is one of the few practical means of including the higher and lower level considerations made by professionals at all stages of statistical modeling: mathematical, numerical, computational (e.g. distributed environments, concurrency, etc.) Some steps toward these broader goals are within reach and worth taking now.&lt;/p&gt;
&lt;p&gt;That said, statistical modeling and estimation as a whole should seriously consider aligning more with the current tools and offerings of symbolic mathematics. Relative to the subject matter here, symbolic integration provides an excellent example. In computer algebra systems, mappings between basic functional forms and their generalized hypergeometric equivalents are used to exploit convenient convolution identities. In the same vein, it might be possible to use analogous tables for solutions to a wide variety of non-smooth models. We outline how this might be done in the following sections.&lt;/p&gt;
&lt;section id="a-context" class="level2"&gt;
&lt;h2&gt;A Context&lt;/h2&gt;
&lt;p&gt;Much recent work in statistical modeling and estimation has revolved around the desire for sparsity, regularization and efficient [automatic] model selection. This is, in some sense, an objective shared with the more specialized areas of Deep Learning and Compressed Sensing. In the former case, we can point to Dropout &lt;span class="citation" data-cites="srivastava_dropout_2014"&gt;(Srivastava et al. 2014)&lt;/span&gt; and, in the latter, &lt;span class="math inline"&gt;\(\ell_p\)&lt;/span&gt; regularization &lt;span class="citation" data-cites="donoho_compressed_2006"&gt;(Donoho 2006)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without delving into those topics here, we’ll simply assume that a practitioner intends to produce a sparse estimate for a model that results in LASSO. First, some setup:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np
&lt;span class="im"&gt;import&lt;/span&gt; scipy &lt;span class="im"&gt;as&lt;/span&gt; sc

&lt;span class="im"&gt;import&lt;/span&gt; pymc3 &lt;span class="im"&gt;as&lt;/span&gt; pm
&lt;span class="im"&gt;import&lt;/span&gt; theano
&lt;span class="im"&gt;import&lt;/span&gt; theano.tensor &lt;span class="im"&gt;as&lt;/span&gt; tt

theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;FAST_COMPILE&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre class="text"&gt;&lt;code&gt;Couldn&amp;#39;t import dot_parser, loading of dot files will not be possible.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using PyMC3, the Bayes version of the LASSO &lt;span class="citation" data-cites="park_bayesian_2008"&gt;(Park and Casella 2008)&lt;/span&gt; model is easily specified.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared

mu_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(&lt;span class="dv"&gt;100&lt;/span&gt;)
mu_true[:&lt;span class="dv"&gt;20&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(&lt;span class="dv"&gt;20&lt;/span&gt;)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;

X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(&lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(mu_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.7&lt;/span&gt;), np.alen(mu_true))
y &lt;span class="op"&gt;=&lt;/span&gt; sc.stats.norm.rvs(loc&lt;span class="op"&gt;=&lt;/span&gt;X.dot(mu_true), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:
    &lt;span class="co"&gt;# Would be nice if we could pass the symbolic y_tt.shape, so&lt;/span&gt;
    &lt;span class="co"&gt;# that our model would automatically conform to changes in&lt;/span&gt;
    &lt;span class="co"&gt;# the shared variables X_tt.&lt;/span&gt;
    &lt;span class="co"&gt;# See https://github.com/pymc-devs/pymc3/pull/1125&lt;/span&gt;
    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])
    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,
                     shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The negative total log likelihood in our example problem has a non-smooth &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; term. The standard means of estimating a [MAP] solution to this problem usually involves the soft-thresholding operator, which is a type of proximal operator. This operator is cheap to compute, so that–among other things–makes the proximal approaches that use it quite appealing.&lt;/p&gt;
&lt;p&gt;Moving on, let’s say we wanted to produce a MAP estimate in this PyMC3 context. A function is already provided for this generic task: &lt;code&gt;find_MAP&lt;/code&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; lasso_model:
    params_0 &lt;span class="op"&gt;=&lt;/span&gt; pm.find_MAP(&lt;span class="bu"&gt;vars&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;[beta_rv])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In our run of the above, an exception is thrown due to the &lt;code&gt;nan&lt;/code&gt; values that arise within the gradient evaluation.&lt;/p&gt;
&lt;p&gt;More directly, we can inspect the gradient at &lt;span class="math inline"&gt;\(\beta = 0, 1\)&lt;/span&gt; to demonstrate the same.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;start &lt;span class="op"&gt;=&lt;/span&gt; pm.Point({&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;: np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])}, model&lt;span class="op"&gt;=&lt;/span&gt;lasso_model)
bij &lt;span class="op"&gt;=&lt;/span&gt; pm.DictToArrayBijection(pm.ArrayOrdering(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;),
start)
logp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastlogp)
dlogp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastdlogp(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;))

&lt;span class="co"&gt;# Could also inspect the log likelihood of the prior:&lt;/span&gt;
&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;

grad_at_0 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.zeros_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))
grad_at_1 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.ones_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_0)))
&lt;span class="dv"&gt;100&lt;/span&gt;
&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_1)))
&lt;span class="dv"&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="the-proximal-context" class="level1"&gt;
&lt;h1&gt;The Proximal Context&lt;/h1&gt;
&lt;p&gt;The general form of what we’re calling a &lt;em&gt;proximal problem&lt;/em&gt; mirrors a penalized likelihood, i.e. &lt;span class="math display"&gt;\[\begin{equation}
\beta^* = \operatorname*{argmin}_\beta \left\{ l(\beta) + \gamma \phi(\beta) \right\}
  \;,
  \label{eq:prox_problem}
\end{equation}\]&lt;/span&gt; where the entire sum is commonly associated with the negative log likelihood and the functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; with observation and prior terms, or loss and penalty terms, respectively. Within the proximal framework, &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are usually convex and lower semi-continuous–although quite a few properties and results can still hold for non-convex functions.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;proximal operator&lt;/em&gt; is an explicit form of Equation &lt;span class="math inline"&gt;\(\eqref{eq:prox_problem}\)&lt;/span&gt; that has &lt;span class="math inline"&gt;\(l(\beta) = \frac{1}{2} (\beta - z)^2\)&lt;/span&gt;. It is a defining part of the intermediate steps within most proximal algorithms. Exact solutions to proximal operators exist for many &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;. These are the elements that could exist in a table, many entries of which could be generated automatically, in analogy to symbolic integration.&lt;/p&gt;
&lt;p&gt;The proximal operator relevant to our example, the soft-threshold operator, is implement in Theano with something like the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
beta_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.r_[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;1&lt;/span&gt;,
&lt;span class="dv"&gt;10&lt;/span&gt;].astype(tt.config.floatX)

lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array(&lt;span class="fl"&gt;0.5&lt;/span&gt;).astype(tt.config.floatX)

&lt;span class="kw"&gt;def&lt;/span&gt; soft_threshold(beta_, lambda_):
    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(soft_threshold(beta_tt, lambda_tt).tag.test_value)
[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;9.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="fl"&gt;0.5&lt;/span&gt;  &lt;span class="fl"&gt;9.5&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Operators like these can be composed with a gradient step to produce a &lt;em&gt;proximal gradient&lt;/em&gt; algorithm.&lt;/p&gt;
&lt;p&gt;Besides the proximal operator for &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, the steps in a proximal gradient algorithm are very straightforward and rely primarily on the gradient of &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt;. When considering this quantity, a tangible benefit of symbolic computation becomes apparent; complicated gradients can be computed automatically and efficiently. With [backtracking] line search to handle unknown step sizes, the proximal gradient alone provides a surprisingly general means of sparse estimation.&lt;/p&gt;
&lt;p&gt;Here is an implementation of a proximal gradient step:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; function &lt;span class="im"&gt;as&lt;/span&gt; tt_function

&lt;span class="kw"&gt;def&lt;/span&gt; prox_grad_step(logl, beta_tt, lambda_1_tt, gamma_prox_tt,
                   prox_func&lt;span class="op"&gt;=&lt;/span&gt;soft_threshold):
    &lt;span class="co"&gt;# Negative log-likelihood without non-smooth (\ell_1) term:&lt;/span&gt;
    logl_grad &lt;span class="op"&gt;=&lt;/span&gt; tt.grad(logl, wrt&lt;span class="op"&gt;=&lt;/span&gt;beta_tt)
    logl_grad.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl_grad&amp;quot;&lt;/span&gt;

    &lt;span class="im"&gt;from&lt;/span&gt; theano.&lt;span class="bu"&gt;compile&lt;/span&gt;.nanguardmode &lt;span class="im"&gt;import&lt;/span&gt; NanGuardMode
    tt_func_mode &lt;span class="op"&gt;=&lt;/span&gt; NanGuardMode(nan_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;,
                                inf_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                                big_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)

    beta_var_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;beta_tt.dtype)
    beta_var_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; beta_tt.get_value()
    grad_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_var_tt], logl_grad,
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode,
                            givens&lt;span class="op"&gt;=&lt;/span&gt;{beta_tt: beta_var_tt})

    beta_quad_step &lt;span class="op"&gt;=&lt;/span&gt; beta_tt &lt;span class="op"&gt;-&lt;/span&gt; lambda_1_tt &lt;span class="op"&gt;*&lt;/span&gt; logl_grad
    beta_quad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_quad_step&amp;quot;&lt;/span&gt;

    beta_prox &lt;span class="op"&gt;=&lt;/span&gt; prox_func(beta_quad_step, gamma_prox_tt &lt;span class="op"&gt;*&lt;/span&gt; lambda_1_tt)
    beta_prox.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_prox&amp;quot;&lt;/span&gt;

    r_tt &lt;span class="op"&gt;=&lt;/span&gt; y_tt &lt;span class="op"&gt;-&lt;/span&gt; X_tt.dot(beta_tt)

    prox_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([],
                            [beta_prox, logl, logl_grad,
tt.mean(r_tt&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;)],
                            updates&lt;span class="op"&gt;=&lt;/span&gt;[(beta_tt, beta_prox)],
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode)

    &lt;span class="cf"&gt;return&lt;/span&gt; (prox_step, grad_step)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="the-symbolic-operations" class="level1"&gt;
&lt;h1&gt;The Symbolic Operations&lt;/h1&gt;
&lt;p&gt;In order to employ a lookup table, or to even identify a proximal problem and check that its conditions (e.g. convexity) are satisfied, we need to obtain the exact forms of each component: &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;. We start with the determination of &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, we’re able to tease apart our &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi(\beta)\)&lt;/span&gt; using the symbolic log likelihoods for the organizational designations of &lt;em&gt;observed&lt;/em&gt; and unobserved PyMC3 random variables.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone

logl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt,
                {beta_rv: beta_tt})
logl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, let’s assume we’re extending &lt;code&gt;find_MAP&lt;/code&gt; with some generality, so that distinguishing &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; in Equation &lt;span class="math inline"&gt;\(\eqref{eq:prox_problem}\)&lt;/span&gt; using these designations isn’t reliable. This is necessary for cases in which a user specifies custom distributions or a potential function. In either case, to achieve our desired functionality we need to operate at a more symbolic level.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;At this point, it is extremely worthwhile to browse the &lt;a href="http://deeplearning.net/software/theano/extending/graphstructures.html"&gt;Theano documentation&lt;/a&gt; regarding graphs and their constituent objects.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The total log likelihood is a good place to start. Let’s look at symbolic graphs produced by ours.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pp &lt;span class="im"&gt;as&lt;/span&gt; tt_pp
&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pprint &lt;span class="im"&gt;as&lt;/span&gt; tt_pprint&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(tt_pp(lasso_model.logpt))
(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(((&lt;span class="op"&gt;-&lt;/span&gt;log(TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;-&lt;/span&gt; (&lt;span class="op"&gt;|&lt;/span&gt;(&lt;span class="op"&gt;\&lt;/span&gt;beta &lt;span class="op"&gt;-&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;})&lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;})))) &lt;span class="op"&gt;+&lt;/span&gt;
Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(switch(TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;},
(((TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} &lt;span class="op"&gt;*&lt;/span&gt; ((y &lt;span class="op"&gt;-&lt;/span&gt; (X &lt;span class="op"&gt;\&lt;/span&gt;dot &lt;span class="op"&gt;\&lt;/span&gt;beta)) &lt;span class="op"&gt;**&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;+&lt;/span&gt; log(TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;})) &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;}),
TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf}))))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#pretty-printing"&gt;pretty printed&lt;/a&gt; Theano graph tells us, among other things, that we have the anticipated sum of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; terms.&lt;/p&gt;
&lt;p&gt;As with most graphs produced by symbolic algebra systems, we need to consider exactly how its operations are arranged, so that we can develop a means of matching general structures. The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#debug-print"&gt;debug printout&lt;/a&gt; is better for this.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(lasso_model.logpt)
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; N]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; R] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; U] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; W] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; Z] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BA] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; BB]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; BC] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; BD]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BE] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BF] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BG] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BH]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BI] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BJ]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BK] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; BL]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that the top-most operator is an &lt;code&gt;Elemwise&lt;/code&gt; that applies the scalar &lt;code&gt;add&lt;/code&gt; operation. This is the &lt;span class="math inline"&gt;\(+\)&lt;/span&gt; in &lt;span class="math inline"&gt;\(l(\beta) + \phi(\beta)\)&lt;/span&gt;. If we were to consider the inputs to this operator as our candidates for &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, then we might find this pair with the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(lasso_model.logpt.owner.inputs)
[Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;, Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using these two terms, we might simply search for an absolute value operator.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; get_abs_between(input_node):
    &lt;span class="co"&gt;# Get all the operations in the sub-tree between our input and the&lt;/span&gt;
    &lt;span class="co"&gt;# log likelihood output node.&lt;/span&gt;
    term_ops &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(tt.gof.graph.ops([input_node],
[lasso_model.logpt]))

    &lt;span class="co"&gt;# Is there an absolute value in there?&lt;/span&gt;
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;filter&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.op &lt;span class="kw"&gt;is&lt;/span&gt; tt.abs_, term_ops)

abs_res &lt;span class="op"&gt;=&lt;/span&gt; [(get_abs_between(in_), in_)
           &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt; lasso_model.logpt.owner.inputs]

&lt;span class="cf"&gt;for&lt;/span&gt; r_ &lt;span class="kw"&gt;in&lt;/span&gt; abs_res:
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(r_[&lt;span class="dv"&gt;0&lt;/span&gt;]) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;:
        phi &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]
    &lt;span class="cf"&gt;else&lt;/span&gt;:
        logp &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(logp)
Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; E]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; M]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; O]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; P]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; R]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; U]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; W]
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(phi)
Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; G]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; K]
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; M]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; O]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From these terms one can similarly attempt to determine multiplicative constants, or parts thereof (e.g. &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The above approach is too limiting; we need something more robust. The above logic will fail on graphs that are constructed differently (e.g. producing equivalent, but different, representations via associativity) or when our naive use of &lt;code&gt;theano.gof.graph.ops&lt;/code&gt; is compromised by certain types of intermediate operations (e.g. non-distributed, non-affine). These are only a few of the many weaknesses inherent to the naive approach above. Furthermore, sufficient coverage of all the necessary conditions–using the same approach–is likely to result in complicated, less approachable code.&lt;/p&gt;
&lt;p&gt;What we need to identify the more general patterns suitable for our goal is mostly covered within the areas of graph unification and logic programming. Luckily, Theano has some basic unification capabilities that we’re able to deploy immediately.&lt;/p&gt;
&lt;p&gt;As an example, we’ll jump right to the creation of a &lt;a href="http://deeplearning.net/software/theano/optimizations.html"&gt;graph optimization&lt;/a&gt;. This is the context in which much of these symbolic operations are better suited; especially if we are required to alter the graph (or a copy thereof) during our search for terms. Consider the &lt;code&gt;phi&lt;/code&gt; variable; the printouts show an unnecessary subtraction (with &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;). Clearly this part–and the entire graph–hasn’t been simplified. Standard simplifications already exist for these sorts of things, and most are performed in tandem.&lt;/p&gt;
&lt;p&gt;Within the graph optimization framework we have the &lt;code&gt;PatternSub&lt;/code&gt; local optimization. It provides basic unification. To demonstrate the kind of operations that could be used to pre-condition a graph and robustly determine a set of supported &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, we’ll make replacement patterns for multiplicative distribution across two forms of addition: &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt;. These searches and replacements can be applied to an objective function until it is in a sufficiently reduced form.&lt;/p&gt;
&lt;p&gt;In the following, we’ll simply demonstrate the steps involved in programming the necessary logic.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_a_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;5&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;)
test_b_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;2&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;)
test_c_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(np.r_[&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="dv"&gt;2&lt;/span&gt;], name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;)

test_exprs_tt &lt;span class="op"&gt;=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; test_b_tt,)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_b_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_c_tt),)

mul_dist_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)
mul_dist_pat_tt &lt;span class="op"&gt;+=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.add, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.add, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next step involves the repeated application of these operations, so that a non-trivial graph can be completely transformed/reduced in some way. We achieve this with the &lt;code&gt;EquilibriumOptimizer&lt;/code&gt; class.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_sub_eqz_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(mul_dist_pat_tt,
max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

test_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph(
    tt.gof.graph.inputs(test_exprs_tt), test_exprs_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(test_fgraph_tt)
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, when we apply the optimization, the &lt;code&gt;FunctionGraph&lt;/code&gt; should have applied the replacements:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_fgraph_opt &lt;span class="op"&gt;=&lt;/span&gt; test_sub_eqz_opt_tt.optimize(test_fgraph_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(test_fgraph_tt)
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;10&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;12&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;11&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; P] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is much more to consider in the examples above. Nonetheless, standalone libraries, like &lt;a href="https://github.com/logpy/logpy/"&gt;LogPy&lt;/a&gt; and SymPy, can be adapted to Theano graphs to provide the functionality necessary to cover most of these considerations.&lt;/p&gt;
&lt;p&gt;Finally, let’s briefly imagine how convexity could be determined symbolically. For differentiable terms, we could start with a simple second derivative test. Within Theano, a “second derivative” can be obtained using the &lt;code&gt;hessian&lt;/code&gt; function, and within &lt;code&gt;theano.sandbox.linalg&lt;/code&gt; are &lt;code&gt;Optimizer&lt;/code&gt; hints for matrix positivity and other properties relevant to determining convexity using this simple idea.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="2" title-name=""&gt;
&lt;p&gt;Other great examples of linear algebra themed optimizations are in &lt;code&gt;theano.sandbox.linalg&lt;/code&gt;: for instance, &lt;code&gt;no_transpose_symmetric&lt;/code&gt;. Some of these demonstrate exactly how straight-forward it can be to add algebraic considerations.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Although our convexity testing idea is far too simple for many &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, the point we want to make is that the basic code necessary for simple tests like this may already be in place. However, with the logic programming mentioned earlier in this section, comes the possibility of implementing aspects of the convex function calculus, by which one can determine convexity for many more classes of functions.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We’ve sketched out the concepts and mechanism with which one can develop a robust estimation platform that can be transparently guided by the more abstract mathematics frameworks from which new, efficient methods are produced.&lt;/p&gt;
&lt;p&gt;Some key steps in the process will most likely require the integration of a symbolic algebra system, so that a much wider array of algebraic machinery can be leveraged to assess–say–convexity of the terms or to solve the proximal operators themselves. Connections between Theano, SymPy and LogPy have already been explored in &lt;span class="citation" data-cites="rocklin_mathematically_2013"&gt;Rocklin (2013)&lt;/span&gt;, as well as many other important aspects of the topics discussed here.&lt;/p&gt;
&lt;p&gt;Additionally, more advanced proximal algorithms exist to improve upon the convergence and stability of the most basic proximal gradient given here. These algorithms often involve operator splitting, which requires careful consideration regarding the exact type of splitting and on which terms it is performed. Within this area are the familiar convex-conjugate approaches; these too could be approached by symbolic solvers, or simply addressed by [partially] generated tables.&lt;/p&gt;
&lt;p&gt;Overall, there appear to be many avenues to explore just within the space of proximal algorithms and modern symbolic systems. Not all of this work necessitates the inclusion of fully featured symbolic algebra systems; much can be done with the symbolic tools of Theano alone. Furthermore, there are specialized, lightweight logic programming systems–like LogPy–that can serve as a step before full symbolic algebra integration.&lt;/p&gt;
&lt;p&gt;Besides the automation of proximal algorithms themselves, there are areas of application involving very large and complicated models or graphs–such as the ones arising in Deep Learning. How might we consider the operator splitting of ADMM within deeply layered or hierarchical models &lt;span class="citation" data-cites="polson_statistical_2015"&gt;(Polson, Willard, and Heidari 2015)&lt;/span&gt;? At which levels and on which terms should the splitting be performed? Beyond simply trying to solve the potentially intractable mathematics arising from related questions, with the symbolic capabilities described here, we can at least begin to experiment with the questions.&lt;/p&gt;
&lt;p&gt;Before closing, a very related–and at least as interesting–set of ideas is worth mentioning: the possibility of encoding more symbolic knowledge into probabilistic programming platforms like PyMC3. Using the same optimization mechanisms as the examples here, simple distributional relationships can be encoded. For instance, the convolution of normally distributed random variables:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;mu_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_X&amp;#39;&lt;/span&gt;)
mu_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_X&amp;#39;&lt;/span&gt;)
sd_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;2&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

mu_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_Y&amp;#39;&lt;/span&gt;)
mu_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_Y&amp;#39;&lt;/span&gt;)
sd_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;0.5&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; conv_model:
    X_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, mu_X, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_X, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu_Y, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_Y, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Z_rv &lt;span class="op"&gt;=&lt;/span&gt; X_rv &lt;span class="op"&gt;+&lt;/span&gt; Y_rv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We create a Theano &lt;code&gt;Op&lt;/code&gt; to handle the convolution.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; NormConvOp(tt.Op):
    __props__ &lt;span class="op"&gt;=&lt;/span&gt; ()

    &lt;span class="kw"&gt;def&lt;/span&gt; make_node(&lt;span class="va"&gt;self&lt;/span&gt;, &lt;span class="op"&gt;*&lt;/span&gt;inputs):
        name_new &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;str&lt;/span&gt;.join(&lt;span class="st"&gt;&amp;#39;+&amp;#39;&lt;/span&gt;, [&lt;span class="bu"&gt;getattr&lt;/span&gt;(in_, &lt;span class="st"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;) &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt;
inputs])
        mu_new &lt;span class="op"&gt;=&lt;/span&gt; tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.mu &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt; inputs])
        sd_new &lt;span class="op"&gt;=&lt;/span&gt; tt.sqrt(tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.sd&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt;
inputs]))
        conv_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(name_new, mu&lt;span class="op"&gt;=&lt;/span&gt;mu_new, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_new,
                            &lt;span class="co"&gt;# Is this another place where&lt;/span&gt;
automatically&lt;span class="op"&gt;/&lt;/span&gt;Theano managed
                            &lt;span class="co"&gt;# shapes are really needed.  For now, we&lt;/span&gt;
hack it.
                            shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))

        &lt;span class="cf"&gt;return&lt;/span&gt; tt.Apply(&lt;span class="va"&gt;self&lt;/span&gt;, inputs, [conv_rv])

    &lt;span class="kw"&gt;def&lt;/span&gt; perform(&lt;span class="va"&gt;self&lt;/span&gt;, node, inputs, output_storage):
        z &lt;span class="op"&gt;=&lt;/span&gt; output_storage[&lt;span class="dv"&gt;0&lt;/span&gt;]
        z[&lt;span class="dv"&gt;0&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.add(&lt;span class="op"&gt;*&lt;/span&gt;inputs)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all that’s needed is a &lt;code&gt;PatternSub&lt;/code&gt; like before.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; is_normal_dist(x):
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;hasattr&lt;/span&gt;(x, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;) &lt;span class="kw"&gt;and&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(x.distribution,
pm.Normal)

norm_conv_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.add,
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;,
      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)},
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;,
      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)}
     ),
    (NormConvOp(), &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;)),)

norm_conv_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(norm_conv_pat_tt,
                                                   max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

Z_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph([X_rv, Y_rv], [Z_rv])

&lt;span class="co"&gt;# We lose the `FreeRV.distribution` attribute when cloning the graph&lt;/span&gt;
&lt;span class="co"&gt;# with `theano.gof.graph.clone_get_equiv` in `FunctionGraph`, so this&lt;/span&gt;
&lt;span class="co"&gt;# hackishly reattaches that information:&lt;/span&gt;
_ &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="bu"&gt;setattr&lt;/span&gt;(g_in, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;, s_in.distribution)
     &lt;span class="cf"&gt;for&lt;/span&gt; s_in, g_in &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;zip&lt;/span&gt;([X_rv, Y_rv], Z_fgraph_tt.inputs)]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; conv_model:
    _ &lt;span class="op"&gt;=&lt;/span&gt; norm_conv_opt_tt.optimize(Z_fgraph_tt)

norm_conv_var_dist &lt;span class="op"&gt;=&lt;/span&gt; Z_fgraph_tt.outputs[&lt;span class="dv"&gt;0&lt;/span&gt;].distribution&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The resulting graph:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(Z_fgraph_tt)
NormConvOp [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;X+Y&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Y [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and the convolution’s parameters (for the test values):&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.mu.tag.test_value)
[ &lt;span class="dv"&gt;2&lt;/span&gt;.]
&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.sd.tag.test_value)
[ &lt;span class="fl"&gt;2.06155281&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;More sophisticated routines–like the example above–could implement parameter expansions, efficient re-parameterizations and equivalent scale mixture forms in an effort to optimize a graph for sampling or point evaluation. Objectives for these optimizations could be straightforward and computationally based (e.g. reducing the number of operations in computations of the log likelihood and other quantities) or more statistically focused (e.g. highly efficient sampling, improve mixing). These ideas are most definitely not new–one example is given by &lt;span class="citation" data-cites="mohasel_afshar_probabilistic_2016"&gt;Mohasel Afshar (2016)&lt;/span&gt; for symbolic Gibbs sampling, but we hope the examples given here make the point that the tools are readily available and quite accessible.&lt;/p&gt;
&lt;p&gt;We’ll end on a much more spacey consideration. Namely, that this is a context in which we can start experimenting rapidly with objectives over the space of estimation routines. This space is generated by–but not limited to–the variety of symbolic representations, re-parameterizations, etc., mentioned above. It does not necessarily require the complete estimation of a model at each step, nor even the numeric value of quantities like the gradient or Hessian. It may involve them, but not their evaluation; perhaps, instead, symbolic comparisons of competing gradients and Hessians arising from different representations. What we’re describing lies somewhere between the completely numeric assessments common today, and the entirely symbolic work found within the theorems and manipulations of the mathematics we use to derive methods.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-donoho_compressed_2006"&gt;
&lt;p&gt;Donoho, David L. 2006. “Compressed Sensing.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 52 (4): 1289–1306. &lt;a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066" class="uri"&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mohasel_afshar_probabilistic_2016"&gt;
&lt;p&gt;Mohasel Afshar, Hadi. 2016. “Probabilistic Inference in Piecewise Graphical Models.” &lt;a href="https://digitalcollections.anu.edu.au/handle/1885/107386" class="uri"&gt;https://digitalcollections.anu.edu.au/handle/1885/107386&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-park_bayesian_2008"&gt;
&lt;p&gt;Park, Trevor, and George Casella. 2008. “The Bayesian Lasso.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 103 (482): 681–86. &lt;a href="http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337" class="uri"&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. doi:&lt;a href="https://doi.org/10.1214/15-STS530"&gt;10.1214/15-STS530&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_statistical_2015"&gt;
&lt;p&gt;Polson, Nicholas G., Brandon T. Willard, and Massoud Heidari. 2015. “A Statistical Theory of Deep Learning via Proximal Splitting.” &lt;em&gt;arXiv Preprint arXiv:1509.06061&lt;/em&gt;. &lt;a href="http://arxiv.org/abs/1509.06061" class="uri"&gt;http://arxiv.org/abs/1509.06061&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rocklin_mathematically_2013"&gt;
&lt;p&gt;Rocklin, Matthew. 2013. “Mathematically Informed Linear Algebra Codes Through Term Rewriting.” PhD thesis, PhD Thesis, August. &lt;a href="http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf" class="uri"&gt;http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-srivastava_dropout_2014"&gt;
&lt;p&gt;Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” &lt;em&gt;The Journal of Machine Learning Research&lt;/em&gt; 15 (1): 1929–58. &lt;a href="http://dl.acm.org/citation.cfm?id=2670313" class="uri"&gt;http://dl.acm.org/citation.cfm?id=2670313&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brandon T. Willard</dc:creator><pubDate>Wed, 18 Jan 2017 00:00:00 -0600</pubDate><guid isPermaLink="false">tag:brandonwillard.github.io,2017-01-18:a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</guid></item><item><title>SymPy Expression Tree Manipulation</title><link>https://brandonwillard.github.io/sympy-expression-tree-manipulation.html</link><description>&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon Willard" /&gt;
  &lt;title&gt;SymPy Expression Tree Manipulation&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #ffffff; color: #1f1c1b; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #ffffff; color: #a0a0a0; border-right: 1px solid #a0a0a0; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #1f1c1b; background-color: #ffffff; }
code &gt; span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code &gt; span.dt { color: #0057ae; } /* DataType */
code &gt; span.dv { color: #b08000; } /* DecVal */
code &gt; span.bn { color: #b08000; } /* BaseN */
code &gt; span.fl { color: #b08000; } /* Float */
code &gt; span.cn { color: #aa5500; } /* Constant */
code &gt; span.ch { color: #924c9d; } /* Char */
code &gt; span.sc { color: #3daee9; } /* SpecialChar */
code &gt; span.st { color: #bf0303; } /* String */
code &gt; span.vs { color: #bf0303; } /* VerbatimString */
code &gt; span.ss { color: #ff5500; } /* SpecialString */
code &gt; span.im { color: #ff5500; } /* Import */
code &gt; span.co { color: #898887; } /* Comment */
code &gt; span.do { color: #607880; } /* Documentation */
code &gt; span.an { color: #ca60ca; } /* Annotation */
code &gt; span.cv { color: #0095ff; } /* CommentVar */
code &gt; span.ot { color: #006e28; } /* Other */
code &gt; span.fu { color: #644a9b; } /* Function */
code &gt; span.va { color: #0057ae; } /* Variable */
code &gt; span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code &gt; span.op { color: #1f1c1b; } /* Operator */
code &gt; span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code &gt; span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code &gt; span.pp { color: #006e28; } /* Preprocessor */
code &gt; span.at { color: #0057ae; } /* Attribute */
code &gt; span.re { color: #0057ae; } /* RegionMarker */
code &gt; span.in { color: #b08000; } /* Information */
code &gt; span.wa { color: #bf0303; } /* Warning */
code &gt; span.al { color: #bf0303; font-weight: bold; } /* Alert */
code &gt; span.er { color: #bf0303; text-decoration: underline; } /* Error */
code &gt; span. { color: #1f1c1b; } /* Normal */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;SymPy Expression Tree Manipulation&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2016–10–27&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;p&gt;I’ve been working on some extensions to our special function computations in &lt;a href="https://arxiv.org/abs/1605.04796"&gt;Prediction risk for global-local shrinkage regression&lt;/a&gt; and decided to employ &lt;a href="https://github.com/sympy/sympy"&gt;SymPy&lt;/a&gt; as much as possible. Out of this came an &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-python-pkg/src/master/hsplus/horn_symbolic.py"&gt;implementation&lt;/a&gt; of a bivariate confluent hypergeometric function: the &lt;a href="https://en.wikipedia.org/wiki/Humbert_series"&gt;Humbert&lt;/a&gt; &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt;. This, and some numeric implementations, are available in a &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-python-pkg"&gt;Python package&lt;/a&gt; and an &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-r-pkg"&gt;R package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the course of this work there are expectations that appear as ratios of &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; functions, so it’s helpful to have a symbolic replacement routine to identify them. &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.match"&gt;Pattern matching&lt;/a&gt;, &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.find"&gt;finding&lt;/a&gt;, substitution and &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.replace"&gt;replacement&lt;/a&gt; are fairly standard in SymPy, so nothing special there; however, when you want something specific, it can get rather tricky.&lt;/p&gt;
&lt;p&gt;Personally, I’ve found the approach offered by the &lt;a href="https://github.com/sympy/sympy/tree/master/sympy/strategies"&gt;&lt;code&gt;sympy.strategies&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/sympy/sympy/tree/master/sympy/unify"&gt;&lt;code&gt;sympy.unify&lt;/code&gt;&lt;/a&gt; frameworks the most appealing. See the original discussion &lt;a href="https://groups.google.com/d/msg/sympy/fspCavhbd9I/vrzUitvgiuYJ"&gt;here&lt;/a&gt;. The reason for their appeal is mostly due to their organization of the processes behind expression tree traversal and manipulation. It’s much easier to see how a very specific and non-trivial simplification or replacement could be accomplished and iteratively improved. These points are made very well in the posts &lt;a href="http://matthewrocklin.com/blog/tags.html#SymPy-ref"&gt;here&lt;/a&gt;, so check them out.&lt;/p&gt;
&lt;p&gt;Let’s say we want to write a function &lt;code&gt;as_expectations&lt;/code&gt; that takes a &lt;code&gt;sympy.Expr&lt;/code&gt; and replaces ratios of &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; functions according to the following pattern: &lt;span class="math display"&gt;\[\begin{equation}
E[X^n] = \frac{\Phi_1(\alpha, \beta, \gamma + n; x, y)}{\Phi_1(\alpha, \beta, \gamma; x, y)}
\;.
\label{eq:expectation}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As an example, let’s set up a situation in which &lt;code&gt;as_expectations&lt;/code&gt; would be used, and, from there, attempt to construct our function. Naturally, this will involve a test expression with terms that we know match Equation &lt;span class="math inline"&gt;\(\eqref{eq:expectation}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; sympy &lt;span class="im"&gt;as&lt;/span&gt; sp

&lt;span class="im"&gt;from&lt;/span&gt; hsplus.horn_symbolic &lt;span class="im"&gt;import&lt;/span&gt; HornPhi1

a, b, g, z_1, z_2 &lt;span class="op"&gt;=&lt;/span&gt; sp.symbols(&lt;span class="st"&gt;&amp;#39;a, b, g, z_1, z_2&amp;#39;&lt;/span&gt;, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
phi1_1 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g,), z_1, z_2)

n &lt;span class="op"&gt;=&lt;/span&gt; sp.Dummy(&lt;span class="st"&gt;&amp;#39;n&amp;#39;&lt;/span&gt;, integer&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;, positive&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
i &lt;span class="op"&gt;=&lt;/span&gt; sp.Dummy(&lt;span class="st"&gt;&amp;#39;i&amp;#39;&lt;/span&gt;, integer&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;, nonnegative&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

phi1_2 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n,), z_1, z_2)
phi1_3 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n &lt;span class="op"&gt;-&lt;/span&gt; i,), z_1, z_2)

r_1 &lt;span class="op"&gt;=&lt;/span&gt; phi1_2&lt;span class="op"&gt;/&lt;/span&gt;phi1_1
r_2 &lt;span class="op"&gt;=&lt;/span&gt; phi1_3&lt;span class="op"&gt;/&lt;/span&gt;phi1_1

expr &lt;span class="op"&gt;=&lt;/span&gt; a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;-&lt;/span&gt; b &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;/&lt;/span&gt; g &lt;span class="op"&gt;+&lt;/span&gt; sp.Sum(z_1&lt;span class="op"&gt;/&lt;/span&gt;z_2 &lt;span class="op"&gt;*&lt;/span&gt; r_2 &lt;span class="op"&gt;-&lt;/span&gt; &lt;span class="dv"&gt;3&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; r_2, (i, &lt;span class="dv"&gt;0&lt;/span&gt;,
n))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our test expression &lt;code&gt;expr&lt;/code&gt; looks like this&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(expr, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\frac{a \operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}} + \sum_{i=0}^{n}
\left(\frac{z_{1} \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad - i + n + g, \quad z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right) - \frac{b
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)}}{g
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ratios &lt;code&gt;r_1&lt;/code&gt; and &lt;code&gt;r_2&lt;/code&gt; should both be replaced by a symbol for &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt;, for &lt;span class="math inline"&gt;\(m = n\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(m = n - i\)&lt;/span&gt; when &lt;span class="math inline"&gt;\(i &amp;lt; n\)&lt;/span&gt; respectively. We could allow &lt;span class="math inline"&gt;\(E[X^0]\)&lt;/span&gt;, I suppose, but–for a more interesting discussion–let’s not.&lt;/p&gt;
&lt;p&gt;We start by creating a SymPy pattern that expresses the mathematical form of &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt; in Equation &lt;span class="math inline"&gt;\(\eqref{eq:expectation}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;pnames &lt;span class="op"&gt;=&lt;/span&gt; (&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z_1&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z_2&amp;#39;&lt;/span&gt;)
phi1_wild_args_n &lt;span class="op"&gt;=&lt;/span&gt; sp.symbols(&lt;span class="st"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;.join(n_ &lt;span class="op"&gt;+&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;_w&amp;#39;&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; n_ &lt;span class="kw"&gt;in&lt;/span&gt; pnames),
                              cls&lt;span class="op"&gt;=&lt;/span&gt;sp.Wild, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

n_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;n_w&amp;#39;&lt;/span&gt;,
              properties&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.is_integer &lt;span class="kw"&gt;and&lt;/span&gt; x.is_positive,),
              exclude&lt;span class="op"&gt;=&lt;/span&gt;(phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;],))

phi1_wild_d &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1(phi1_wild_args_n[&lt;span class="dv"&gt;0&lt;/span&gt;:&lt;span class="dv"&gt;2&lt;/span&gt;],
                       phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;:&lt;span class="dv"&gt;3&lt;/span&gt;],
                       &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n[&lt;span class="dv"&gt;3&lt;/span&gt;:&lt;span class="dv"&gt;5&lt;/span&gt;])

phi1_wild_n &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1(phi1_wild_args_n[&lt;span class="dv"&gt;0&lt;/span&gt;:&lt;span class="dv"&gt;2&lt;/span&gt;],
                       (phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;] &lt;span class="op"&gt;+&lt;/span&gt; n_w,),
                       &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n[&lt;span class="dv"&gt;3&lt;/span&gt;:&lt;span class="dv"&gt;5&lt;/span&gt;])

C_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;C_w&amp;#39;&lt;/span&gt;, exclude&lt;span class="op"&gt;=&lt;/span&gt;[sp.S.Zero])
E_pattern &lt;span class="op"&gt;=&lt;/span&gt; phi1_wild_n &lt;span class="op"&gt;/&lt;/span&gt; phi1_wild_d

E_fn &lt;span class="op"&gt;=&lt;/span&gt; sp.Function(&lt;span class="st"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we find an &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt; we’ll replace it with the symbolic function &lt;code&gt;E_fn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we focus on only one of the terms (one we know matches &lt;code&gt;E_pattern&lt;/code&gt;), &lt;code&gt;r_1&lt;/code&gt;, we should find that our pattern suffices:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; r_1.match(E_pattern)
{n_w_: _n, z_2_w_: z_2, z_1_w_: z_1, a_w_: a, g_w_: g, b_w_: b}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, building up to the complexity of &lt;code&gt;expr&lt;/code&gt;, we see that a simple product doesn’t:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).match(E_pattern)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Basically, the product has introduced some problems that arise from associativity. Here are the details for the root expression tree:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).func
&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kw"&gt;class&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;sympy.core.mul.Mul&amp;#39;&lt;/span&gt;&lt;span class="op"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).args
(a, &lt;span class="dv"&gt;1&lt;/span&gt;&lt;span class="op"&gt;/&lt;/span&gt;HornPhi1(a, b, g, z_1, z_2), HornPhi1(a, b, _n &lt;span class="op"&gt;+&lt;/span&gt; g, z_1, z_2))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The root operation is multiplication and the operation’s arguments are all terms in the product/division.&lt;/p&gt;
&lt;p&gt;Any complete search for matches to &lt;code&gt;E_pattern&lt;/code&gt; would have to consider all possible combinations of terms in &lt;code&gt;(a * r_1).args&lt;/code&gt;, i.e. all possible groupings that arise due to associativity. The simple inclusion of another &lt;code&gt;Wild&lt;/code&gt; term causes the match to succeed, since SymPy’s basic pattern matching does account for associativity in this case.&lt;/p&gt;
&lt;p&gt;Here are a few explicit ways to make the match work:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).match(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern)
{a_w_: a, n_w_: _n, g_w_: g, z_2_w_: z_2, C_w_: a, b_w_: b, z_1_w_:
z_1}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or as a replacement:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and via &lt;code&gt;rewriterule&lt;/code&gt;:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; sympy.unify.rewrite &lt;span class="im"&gt;import&lt;/span&gt; rewriterule
rl &lt;span class="op"&gt;=&lt;/span&gt; rewriterule(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern,
                 C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w, &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n),
                 phi1_wild_args_n &lt;span class="op"&gt;+&lt;/span&gt; (n_w, C_w))
res &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(rl(a &lt;span class="op"&gt;*&lt;/span&gt; r_1))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left [ a E{\left (n,a,b,g,z_{1},z_{2} \right )}\right ]
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The advantage in using &lt;code&gt;rewriterule&lt;/code&gt; is that multiple matches will be returned. If we add another &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; in the numerator, so there are multiple possible &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt;, we get&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;phi1_4 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n &lt;span class="op"&gt;+&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;,), z_1, z_2)

res &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(rl(a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;*&lt;/span&gt; phi1_4))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left [ a \operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n +
g, \quad z_{1}, \quad z_{2}\right )\right)} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n + 1,a,b,g,z_{1},z_{2}
\right )}, \quad a \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad n + g + 1, \quad z_{1}, \quad z_{2}\right )\right)} E{\left
(n,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n + 1,a,b,g,z_{1},z_{2}
\right )}, \quad a \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad n + g, \quad z_{1}, \quad z_{2}\right )\right)} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g + 1, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n,a,b,g,z_{1},z_{2} \right
)}\right ]
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;FYI: the associativity of terms inside the function arguments is causing the seemingly duplicate results.&lt;/p&gt;
&lt;p&gt;Naive use of &lt;code&gt;Expr.replace&lt;/code&gt; doesn’t give all results; instead, it does something likely unexpected:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;*&lt;/span&gt; phi1_4).replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern,
                                 C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w, &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )} \operatorname{\Phi_1}{\left(\left ( a,
\quad b, \quad g, \quad z_{1}, \quad z_{2}\right )\right)}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Returning to our more complicated &lt;code&gt;expr&lt;/code&gt;…Just because we can match products doesn’t mean we’re finished, since we still need a good way to traverse the entire expression tree and match the sub-trees. More importantly, adding the multiplicative &lt;code&gt;Wild&lt;/code&gt; term &lt;code&gt;C_w&lt;/code&gt; is more of a hack than a direct solution, since we don’t want the matched contents of &lt;code&gt;C_w&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;Expr.replace/xreplace&lt;/code&gt; will match sub-expressions, we found above that it produces some odd results. Those results persist when applied to more complicated expressions:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; expr.replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} - \frac{b}{g} E{\left
(n,a,b,g,z_{1},z_{2} \right )} + \sum_{i=0}^{n} \left(\frac{z_{1}
E{\left (n,a,b,- i + g,z_{1},z_{2} \right )}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + g, \quad
z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3 E{\left (n,a,b,- i +
g,z_{1},z_{2} \right )} \operatorname{\Phi_1}{\left(\left ( a, \quad
b, \quad - i + g, \quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right)
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, it looks like the matching was a little too liberal and introduced extra &lt;code&gt;E&lt;/code&gt; and &lt;code&gt;HornPhi1&lt;/code&gt; terms. This is to be expected from the &lt;code&gt;Wild&lt;/code&gt; matching in SymPy; it needs us to specify what &lt;em&gt;not&lt;/em&gt; to match, as well. Our “fix” that introduced &lt;code&gt;C_w&lt;/code&gt; is the exact source of the problem, but we can tell it not to match &lt;code&gt;HornPhi1&lt;/code&gt; terms and get better results:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;C_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;C_w&amp;#39;&lt;/span&gt;, exclude&lt;span class="op"&gt;=&lt;/span&gt;[sp.S.Zero, HornPhi1])
res &lt;span class="op"&gt;=&lt;/span&gt; expr.replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} - \frac{b}{g} E{\left
(n,a,b,g,z_{1},z_{2} \right )} + \sum_{i=0}^{n} \left(\frac{z_{1}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right)
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ve stopped it from introducing those superfluous &lt;code&gt;E&lt;/code&gt; terms, but we’re still not getting replacements for the &lt;code&gt;HornPhi1&lt;/code&gt; ratios in the sums. Let’s single out those terms and see what’s going on:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; r_2.find(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern)
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left\{\right\}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The constrained integer &lt;code&gt;Wild&lt;/code&gt; term, &lt;code&gt;n_w&lt;/code&gt;, probably isn’t matching. Given the form of our pattern, &lt;code&gt;n_w&lt;/code&gt; should match &lt;code&gt;n - i&lt;/code&gt;, but &lt;code&gt;n - i&lt;/code&gt; isn’t strictly positive, as required:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; (n &lt;span class="op"&gt;-&lt;/span&gt; i).is_positive &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
&lt;span class="va"&gt;False&lt;/span&gt;
&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; sp.ask(sp.Q.positive(n &lt;span class="op"&gt;-&lt;/span&gt; i)) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
&lt;span class="va"&gt;False&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math inline"&gt;\(n &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(i &amp;gt;= 0\)&lt;/span&gt;, the only missing piece is that &lt;span class="math inline"&gt;\(n &amp;gt; i\)&lt;/span&gt;. The most relevant mechanism in SymPy to assess this information is the &lt;a href="http://docs.sympy.org/dev/modules/assumptions/index.html"&gt;&lt;code&gt;sympy.assumptions&lt;/code&gt;&lt;/a&gt; interface. We could add and retrieve the assumption &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; via &lt;code&gt;sympy.assume.global_assumptions&lt;/code&gt;, or perform these operations inside of a Python &lt;code&gt;with&lt;/code&gt; block, etc. This context management, via &lt;code&gt;sympy.assumptions.assume.AssumptionsContext&lt;/code&gt;, would have to be performed manually, since I am not aware of any such mechanism offered by &lt;code&gt;Sum&lt;/code&gt; and/or &lt;code&gt;Basic.replace&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, these ideas sound good, but aren’t implemented:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; sp.ask(sp.Q.positive(n &lt;span class="op"&gt;-&lt;/span&gt; i), sp.Q.is_true(n &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; i)) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
&lt;span class="va"&gt;False&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the documentation for &lt;code&gt;sympy.assumptions.ask.ask&lt;/code&gt;; it explicitely states that inequalities aren’t handled, yet.&lt;/p&gt;
&lt;p&gt;We could probably perform a manual reworking of &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; to &lt;code&gt;sympy.Q.is_true(n - i &amp;gt; 0)&lt;/code&gt;, which is of course equivalent to &lt;code&gt;sympy.Q.positive(n - i)&lt;/code&gt;: the result we want.&lt;/p&gt;
&lt;p&gt;If one were to provide this functionality, there’s still the question of how the relevant &lt;code&gt;AssumptionsContext&lt;/code&gt;s would be created and passed around/nested during the subexpression replacements. There is no apparent means of adding this sort of functionality through the &lt;code&gt;Basic.replace&lt;/code&gt; interface, so this path looks less appealing. However, nesting &lt;code&gt;with&lt;/code&gt; blocks from strategies in &lt;code&gt;sympy.strategies&lt;/code&gt; does seem quite possible. For example, in &lt;code&gt;sympy.strategies.traverse.sall&lt;/code&gt;, one could possibly wrap the &lt;code&gt;return&lt;/code&gt; statement after the &lt;code&gt;map(rule, ...)&lt;/code&gt; call in a &lt;code&gt;with sympy.assuming(...):&lt;/code&gt; block that contains the assumptions for any variables arising as, say, the index of a &lt;code&gt;Sum&lt;/code&gt;–like in our case. In this scenario, code in the subexpressions would be able to ask questions like &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; without altering the global assumptions context or the objects involved.&lt;/p&gt;
&lt;p&gt;Anyway, that’s all I wanted to cover here. Perhaps later I’ll post a hack for the assumptions approach, but–at the very least–I’ll try to follow up with a more direct solution that uses &lt;code&gt;sympy.strategies&lt;/code&gt;.&lt;/p&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brandon Willard</dc:creator><pubDate>Thu, 27 Oct 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:brandonwillard.github.io,2016-10-27:sympy-expression-tree-manipulation.html</guid></item></channel></rss>