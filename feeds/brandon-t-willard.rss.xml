<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Brandon T. Willard</title><link>https://brandonwillard.github.io/</link><description></description><lastBuildDate>Wed, 18 Jan 2017 00:00:00 -0600</lastBuildDate><item><title>A Role for Symbolic Computation in the General Estimation of Statistical Models</title><link>https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</link><description>&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #303030; color: #cccccc; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #cccccc; background-color: #303030; }
code &gt; span.kw { color: #f0dfaf; } /* Keyword */
code &gt; span.dt { color: #dfdfbf; } /* DataType */
code &gt; span.dv { color: #dcdccc; } /* DecVal */
code &gt; span.bn { color: #dca3a3; } /* BaseN */
code &gt; span.fl { color: #c0bed1; } /* Float */
code &gt; span.ch { color: #dca3a3; } /* Char */
code &gt; span.st { color: #cc9393; } /* String */
code &gt; span.co { color: #7f9f7f; } /* Comment */
code &gt; span.ot { color: #efef8f; } /* Other */
code &gt; span.al { color: #ffcfaf; } /* Alert */
code &gt; span.fu { color: #efef8f; } /* Function */
code &gt; span.er { color: #c3bf9f; } /* Error */
code &gt; span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
code &gt; span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code &gt; span.sc { color: #dca3a3; } /* SpecialChar */
code &gt; span.vs { color: #cc9393; } /* VerbatimString */
code &gt; span.ss { color: #cc9393; } /* SpecialString */
code &gt; span.im { } /* Import */
code &gt; span.va { } /* Variable */
code &gt; span.cf { color: #f0dfaf; } /* ControlFlow */
code &gt; span.op { color: #f0efd0; } /* Operator */
code &gt; span.bu { } /* BuiltIn */
code &gt; span.ex { } /* Extension */
code &gt; span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code &gt; span.at { } /* Attribute */
code &gt; span.do { color: #7f9f7f; } /* Documentation */
code &gt; span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code &gt; span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code &gt; span.in { color: #7f9f7f; font-weight: bold; } /* Information */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–01–18&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this document we describe how symbolic computation can be used to provide generalizable statistical estimation through a combination of existing open source frameworks.&lt;/p&gt;
&lt;p&gt;Specifically, we consider the optimization problems resulting from models with non-smooth objective functions. These problems arise in the context of regularization and shrinkage, and here we’ll address their automated application within the &lt;em&gt;proximal framework&lt;/em&gt; &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt;. In &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt; we outlined a set of seemingly disparate optimization techniques within the fields of statistics, computer vision, and machine learning, that are unified by the types of functional bounds employed, the tools of convex analysis and the language of operator theory. These methods–and the concepts behind them–have found much success in recent times and admit quite a few interesting paths for research.&lt;/p&gt;
&lt;p&gt;In at least a few cases, the work required to produce a proximal algorithm overlaps with some highly functional areas in computer algebra and symbolic mathematics. For instance, some proximal operators–the main ingredient within proximal algorithms–can be solved exactly with symbolic differentiation and algebraic equation solvers. With perhaps only the addition of sophisticated variable assumptions and an ability to manipulate inequalities, even larger &lt;em&gt;classes&lt;/em&gt; of proximal operators could be solved. We refer the reader to the table in &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt; for examples of operators that are currently solved–by hand–using roughly the same means.&lt;/p&gt;
&lt;p&gt;The kind of automation proposed here only begins to answer a problem that arises somewhat naturally in these areas: how does one provide access to methods that produce–or apply to–numerous distinct problems and solutions. Instead of the common attempt to implement each model or method separately, and then combine them under a loosely organized API or function interface, the symbolic approach offers wider applicability and concise implementations in terms of code that maps directly to the mathematics. Also, it is one of the few practical means of including the higher and lower level considerations made by professionals at all stages of statistical modeling: mathematical, numerical, computational (e.g. distributed environments, concurrency, etc.) Some steps toward these broader goals are within reach and worth taking now.&lt;/p&gt;
&lt;p&gt;That said, statistical modeling and estimation as a whole should seriously consider aligning more with the current tools and offerings of symbolic mathematics. Relative to the subject matter here, symbolic integration provides an excellent example. In computer algebra systems, mappings between basic functional forms and their generalized hypergeometric equivalents are used to exploit convenient convolution identities. In the same vein, it might be possible to use analogous tables for solutions to a wide variety of non-smooth models. We outline how this might be done in the following sections.&lt;/p&gt;
&lt;section id="a-context" class="level2"&gt;
&lt;h2&gt;A Context&lt;/h2&gt;
&lt;p&gt;Much recent work in statistical modeling and estimation has revolved around the desire for sparsity, regularization and efficient [automatic] model selection. This is, in some sense, an objective shared with the more specialized areas of Deep Learning and Compressed Sensing. In the latter case, we can point to Dropout &lt;span class="citation" data-cites="srivastava_dropout_2014"&gt;(Srivastava et al. 2014)&lt;/span&gt; and, in the former, &lt;span class="math inline"&gt;\(\ell_p\)&lt;/span&gt; regularization &lt;span class="citation" data-cites="donoho_compressed_2006"&gt;(Donoho 2006)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without delving into those topics here, we’ll simply assume that a practitioner intends to produce a sparse estimate for a model that results in LASSO. First, some setup:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np
&lt;span class="im"&gt;import&lt;/span&gt; scipy &lt;span class="im"&gt;as&lt;/span&gt; sc
&lt;span class="im"&gt;import&lt;/span&gt; pandas &lt;span class="im"&gt;as&lt;/span&gt; pd

&lt;span class="im"&gt;import&lt;/span&gt; pymc3 &lt;span class="im"&gt;as&lt;/span&gt; pm
&lt;span class="im"&gt;import&lt;/span&gt; theano
&lt;span class="im"&gt;import&lt;/span&gt; theano.tensor &lt;span class="im"&gt;as&lt;/span&gt; tt

&lt;span class="im"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class="im"&gt;as&lt;/span&gt; plt
&lt;span class="im"&gt;import&lt;/span&gt; seaborn &lt;span class="im"&gt;as&lt;/span&gt; sb

&lt;span class="co"&gt;# plt.style.use(&amp;#39;ggplot&amp;#39;)&lt;/span&gt;
plt.rc(&lt;span class="st"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;, usetex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;FAST_COMPILE&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using PyMC3, the Bayes version of the LASSO &lt;span class="citation" data-cites="park_bayesian_2008"&gt;(Park and Casella 2008)&lt;/span&gt; model is easily specified.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared

mu_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(&lt;span class="dv"&gt;100&lt;/span&gt;)
mu_true[:&lt;span class="dv"&gt;20&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(&lt;span class="dv"&gt;20&lt;/span&gt;)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;

X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(&lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(mu_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.7&lt;/span&gt;), np.alen(mu_true))
y &lt;span class="op"&gt;=&lt;/span&gt; sc.stats.norm.rvs(loc&lt;span class="op"&gt;=&lt;/span&gt;X.dot(mu_true), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:
    &lt;span class="co"&gt;# Would be nice if we could pass the symbolic y_tt.shape, so&lt;/span&gt;
    &lt;span class="co"&gt;# that our model would automatically conform to changes in&lt;/span&gt;
    &lt;span class="co"&gt;# the shared variables X_tt.&lt;/span&gt;
    &lt;span class="co"&gt;# See https://github.com/pymc-devs/pymc3/pull/1125&lt;/span&gt;
    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])
    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,
shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The negative total log likelihood in our example problem has a non-smooth &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; term. The standard means of estimating a [MAP] solution to this problem usually involves the soft-thresholding operator, which is a type of proximal operator. This operator is cheap to compute, so that–among other things–makes the proximal approaches that use it quite appealing.&lt;/p&gt;
&lt;p&gt;Moving on, let’s say we wanted to produce a MAP estimate in this PyMC3 context. A function is already provided for this generic task: &lt;code&gt;find_MAP&lt;/code&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; lasso_model:
    params_0 &lt;span class="op"&gt;=&lt;/span&gt; pm.find_MAP(&lt;span class="bu"&gt;vars&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;[beta_rv])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In our run of the above, an exception is thrown due to the &lt;code&gt;nan&lt;/code&gt; values that arise within the gradient evaluation.&lt;/p&gt;
&lt;p&gt;More directly, we can inspect the gradient at &lt;span class="math inline"&gt;\(\beta = 0, 1\)&lt;/span&gt; to demonstrate the same.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;start &lt;span class="op"&gt;=&lt;/span&gt; pm.Point({&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;: np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])}, model&lt;span class="op"&gt;=&lt;/span&gt;lasso_model)
bij &lt;span class="op"&gt;=&lt;/span&gt; pm.DictToArrayBijection(pm.ArrayOrdering(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;),
start)
logp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastlogp)
dlogp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastdlogp(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;))

&lt;span class="co"&gt;# Could also inspect the log likelihood of the prior:&lt;/span&gt;
&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;
&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;

grad_at_0 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.zeros_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))
grad_at_1 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.ones_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;1&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_0)))
Out[&lt;span class="dv"&gt;1&lt;/span&gt;]: &lt;span class="dv"&gt;100&lt;/span&gt;
In [&lt;span class="dv"&gt;2&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_1)))
Out[&lt;span class="dv"&gt;2&lt;/span&gt;]: &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="the-proximal-context" class="level1"&gt;
&lt;h1&gt;The Proximal Context&lt;/h1&gt;
&lt;p&gt;The general form of what we’re calling a &lt;em&gt;proximal problem&lt;/em&gt; mirrors a penalized likelihood, i.e. &lt;span class="math display"&gt;\[\begin{equation}
\beta^* = \operatorname*{argmin}_\beta \left\{ l(\beta) + \gamma \phi(\beta) \right\}
  \;,
  \label{eq:prox_problem}
\end{equation}\]&lt;/span&gt; where the entire sum is commonly associated with the negative log likelihood and the functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; with observation and prior terms, or loss and penalty terms, respectively. Within the proximal framework, &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are usually convex and lower semi-continuous–although quite a few properties and results can still hold for non-convex functions.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;proximal operator&lt;/em&gt; is an explicit form of &lt;span class="math inline"&gt;\(\ref{eq:prox_problem}\)&lt;/span&gt; that has &lt;span class="math inline"&gt;\(l(\beta) = \frac{1}{2} (\beta - z)^2\)&lt;/span&gt;. It is a defining part of the intermediate steps within most proximal algorithms. Exact solutions to proximal operators exist for many &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;. These are the elements that could exist in a table, many entries of which could be generated automatically, in analogy to symbolic integration.&lt;/p&gt;
&lt;p&gt;The proximal operator relevant to our example, the soft-threshold operator, is implement in Theano with something like the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
beta_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.r_[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;1&lt;/span&gt;,
&lt;span class="dv"&gt;10&lt;/span&gt;].astype(tt.config.floatX)

lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array(&lt;span class="fl"&gt;0.5&lt;/span&gt;).astype(tt.config.floatX)

&lt;span class="kw"&gt;def&lt;/span&gt; soft_threshold(beta_, lambda_):
    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;3&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(soft_threshold(beta_tt, lambda_tt).tag.test_value)
Out[&lt;span class="dv"&gt;3&lt;/span&gt;]: [&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;9.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="fl"&gt;0.5&lt;/span&gt;  &lt;span class="fl"&gt;9.5&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Operators like these can be composed with a gradient step to produce a &lt;em&gt;proximal gradient&lt;/em&gt; algorithm.&lt;/p&gt;
&lt;p&gt;Besides the proximal operator for &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, the steps in a proximal gradient algorithm are very straightforward and rely primarily on the gradient of &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt;. When considering this quantity, a tangible benefit of symbolic computation becomes apparent; complicated gradients can be computed automatically and efficiently. With [backtracking] line search to handle unknown step sizes, the proximal gradient alone provides a surprisingly general means of sparse estimation.&lt;/p&gt;
&lt;p&gt;Here is an implementation of a proximal gradient step:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; function &lt;span class="im"&gt;as&lt;/span&gt; tt_function

&lt;span class="kw"&gt;def&lt;/span&gt; prox_grad_step(logl, beta_tt, lambda_1_tt, gamma_prox_tt,
                   prox_func&lt;span class="op"&gt;=&lt;/span&gt;soft_threshold):
    &lt;span class="co"&gt;# Negative log-likelihood without non-smooth (\ell_1) term:&lt;/span&gt;
    logl_grad &lt;span class="op"&gt;=&lt;/span&gt; tt.grad(logl, wrt&lt;span class="op"&gt;=&lt;/span&gt;beta_tt)
    logl_grad.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl_grad&amp;quot;&lt;/span&gt;

    &lt;span class="im"&gt;from&lt;/span&gt; theano.&lt;span class="bu"&gt;compile&lt;/span&gt;.nanguardmode &lt;span class="im"&gt;import&lt;/span&gt; NanGuardMode
    tt_func_mode &lt;span class="op"&gt;=&lt;/span&gt; NanGuardMode(nan_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;,
                                inf_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                                big_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)

    beta_var_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;beta_tt.dtype)
    beta_var_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; beta_tt.get_value()
    grad_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_var_tt], logl_grad,
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode,
                            givens&lt;span class="op"&gt;=&lt;/span&gt;{beta_tt: beta_var_tt})

    beta_quad_step &lt;span class="op"&gt;=&lt;/span&gt; beta_tt &lt;span class="op"&gt;-&lt;/span&gt; lambda_1_tt &lt;span class="op"&gt;*&lt;/span&gt; logl_grad
    beta_quad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_quad_step&amp;quot;&lt;/span&gt;

    beta_prox &lt;span class="op"&gt;=&lt;/span&gt; prox_func(beta_quad_step, gamma_prox_tt &lt;span class="op"&gt;*&lt;/span&gt; lambda_1_tt)
    beta_prox.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_prox&amp;quot;&lt;/span&gt;

    r_tt &lt;span class="op"&gt;=&lt;/span&gt; y_tt &lt;span class="op"&gt;-&lt;/span&gt; X_tt.dot(beta_tt)

    prox_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([],
                            [beta_prox, logl, logl_grad,
tt.mean(r_tt&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;)],
                            updates&lt;span class="op"&gt;=&lt;/span&gt;[(beta_tt, beta_prox)],
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode)

    &lt;span class="cf"&gt;return&lt;/span&gt; (prox_step, grad_step)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="the-symbolic-operations" class="level1"&gt;
&lt;h1&gt;The Symbolic Operations&lt;/h1&gt;
&lt;p&gt;In order to employ a lookup table, or to even identify a proximal problem and check that its conditions (e.g. convexity) are satisfied, we need to obtain the exact forms of each component: &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;. We start with the determination of &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, we’re able to tease apart our &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi(\beta)\)&lt;/span&gt; using the symbolic log likelihoods for the organizational designations of &lt;em&gt;observed&lt;/em&gt; and unobserved PyMC3 random variables.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone

logl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt,
                {beta_rv: beta_tt})
logl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, let’s assume we’re extending &lt;code&gt;find_MAP&lt;/code&gt; with some generality, so that distinguishing &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; in &lt;span class="math inline"&gt;\(\ref{eq:prox_problem}\)&lt;/span&gt; using these designations isn’t reliable. This is necessary for cases in which a user specifies custom distributions or a potential function. In either case, to achieve our desired functionality we need to operate at a more symbolic level.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;At this point, it is extremely worthwhile to browse the &lt;a href="http://deeplearning.net/software/theano/extending/graphstructures.html"&gt;Theano documentation&lt;/a&gt; regarding graphs and their constituent objects.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The total log likelihood is a good place to start. Let’s look at symbolic graphs produced by ours.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pp &lt;span class="im"&gt;as&lt;/span&gt; tt_pp
&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pprint &lt;span class="im"&gt;as&lt;/span&gt; tt_pprint&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;4&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(tt_pp(lasso_model.logpt))
Out[&lt;span class="dv"&gt;4&lt;/span&gt;]:
(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(((&lt;span class="op"&gt;-&lt;/span&gt;log(TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;-&lt;/span&gt; (&lt;span class="op"&gt;|&lt;/span&gt;(&lt;span class="op"&gt;\&lt;/span&gt;beta &lt;span class="op"&gt;-&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;})&lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;})))) &lt;span class="op"&gt;+&lt;/span&gt;
Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(switch(TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;},
(((TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} &lt;span class="op"&gt;*&lt;/span&gt; ((y &lt;span class="op"&gt;-&lt;/span&gt; (X &lt;span class="op"&gt;\&lt;/span&gt;dot &lt;span class="op"&gt;\&lt;/span&gt;beta)) &lt;span class="op"&gt;**&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;+&lt;/span&gt; log(TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;})) &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;}),
TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf}))))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#pretty-printing"&gt;pretty printed&lt;/a&gt; Theano graph tells us, among other things, that we have the anticipated sum of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; terms.&lt;/p&gt;
&lt;p&gt;As with most graphs produced by symbolic algebra systems, we need to consider exactly how its operations are arranged, so that we can develop a means of matching general structures. The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#debug-print"&gt;debug printout&lt;/a&gt; is better for this.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;5&lt;/span&gt;]: tt.printing.debugprint(lasso_model.logpt)
Out[&lt;span class="dv"&gt;5&lt;/span&gt;]: Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; N]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; R] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; U] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; W] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; Z] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BA] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; BB]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; BC] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; BD]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BE] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BF] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BG] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BH]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BI] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BJ]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BK] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; BL]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that the top-most operator is an &lt;code&gt;Elemwise&lt;/code&gt; that applies the scalar &lt;code&gt;add&lt;/code&gt; operation. This is the &lt;span class="math inline"&gt;\(+\)&lt;/span&gt; in &lt;span class="math inline"&gt;\(l(\beta) + \phi(\beta)\)&lt;/span&gt;. If we were to consider the inputs to this operator as our candidates for &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, then we might find the pair with the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;6&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(lasso_model.logpt.owner.inputs)
Out[&lt;span class="dv"&gt;6&lt;/span&gt;]: [Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;, Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using these two terms, we might simply search for an absolute value operator.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; get_abs_between(input_node):
    &lt;span class="co"&gt;# Get all the operations in the sub-tree between our input and the&lt;/span&gt;
    &lt;span class="co"&gt;# log likelihood output node.&lt;/span&gt;
    term_ops &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(tt.gof.graph.ops([input_node],
[lasso_model.logpt]))

    &lt;span class="co"&gt;# Is there an absolute value in there?&lt;/span&gt;
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;filter&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.op &lt;span class="op"&gt;is&lt;/span&gt; tt.abs_, term_ops)

abs_res &lt;span class="op"&gt;=&lt;/span&gt; [(get_abs_between(in_), in_)
           &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt; lasso_model.logpt.owner.inputs]

&lt;span class="cf"&gt;for&lt;/span&gt; r_ &lt;span class="op"&gt;in&lt;/span&gt; abs_res:
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(r_[&lt;span class="dv"&gt;0&lt;/span&gt;]) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;:
        phi &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]
    &lt;span class="cf"&gt;else&lt;/span&gt;:
        logp &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;7&lt;/span&gt;]: tt.printing.debugprint(logp)
Out[&lt;span class="dv"&gt;7&lt;/span&gt;]: Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; E]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; M]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; O]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; P]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; R]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; U]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; W]
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
In [&lt;span class="dv"&gt;8&lt;/span&gt;]: tt.printing.debugprint(phi)
Out[&lt;span class="dv"&gt;8&lt;/span&gt;]: Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; G]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; K]
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; M]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; O]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From these terms one can similarly attempt to determine multiplicative constants, or parts thereof (e.g. &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The above approach is too limiting; we need something more robust. The above logic will fail on graphs that are constructed differently (e.g. producing equivalent, but different, representations via associativity) or when our naive use of &lt;code&gt;theano.gof.graph.ops&lt;/code&gt; is compromised by certain types of intermediate operations (e.g. non-distributed, non-affine). These are only a few of the many weaknesses inherent to the naive approach above. Furthermore, sufficient coverage of all the necessary conditions–using the same approach–is likely to result in complicated, less approachable code.&lt;/p&gt;
&lt;p&gt;What we need to identify the more general patterns suitable for our goal is mostly covered within the areas of graph unification and logic programming. Luckily, Theano has some basic unification capabilities that we’re able to deploy immediately.&lt;/p&gt;
&lt;p&gt;As an example, we’ll jump right to the creation of a &lt;a href="http://deeplearning.net/software/theano/optimizations.html"&gt;graph optimization&lt;/a&gt;. This is the context in which much of these symbolic operations are better suited; especially if we are required to alter the graph (or a copy thereof) during our search for terms. Consider the &lt;code&gt;phi&lt;/code&gt; variable; the printouts show an unnecessary subtraction (with &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;). Clearly this part–and the entire graph–hasn’t been simplified. Standard simplifications already exist for these sorts of things, and most are performed in tandem.&lt;/p&gt;
&lt;p&gt;Within the graph optimization framework we have the &lt;code&gt;PatternSub&lt;/code&gt; local optimization. It provides basic unification. To demonstrate the kind of operations that could be used to pre-condition a graph and robustly determine a set of supported &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, we’ll make replacement patterns for multiplicative distribution across two forms of addition: &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt;. These searches and replacements can be applied to an objective function until it is in a sufficiently reduced form.&lt;/p&gt;
&lt;p&gt;In the following, we’ll simply demonstrate the steps involved in programming the necessary logic.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_a_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;5&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;)
test_b_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;2&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;)
test_c_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(np.r_[&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="dv"&gt;2&lt;/span&gt;], name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;)

test_exprs_tt &lt;span class="op"&gt;=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; test_b_tt,)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_b_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_c_tt),)

mul_dist_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)
mul_dist_pat_tt &lt;span class="op"&gt;+=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.add, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.add, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next step involves the repeated application of these operations, so that a non-trivial graph can be completely transformed/reduced in some way. We achieve this with the &lt;code&gt;EquilibriumOptimizer&lt;/code&gt; class.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_sub_eqz_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(mul_dist_pat_tt,
max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

test_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph(
    tt.gof.graph.inputs(test_exprs_tt), test_exprs_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;9&lt;/span&gt;]: tt.printing.debugprint(test_fgraph_tt)
Out[&lt;span class="dv"&gt;9&lt;/span&gt;]: Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, when we apply the optimization, the &lt;code&gt;FunctionGraph&lt;/code&gt; should have applied the replacements:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_fgraph_opt &lt;span class="op"&gt;=&lt;/span&gt; test_sub_eqz_opt_tt.optimize(test_fgraph_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;10&lt;/span&gt;]: tt.printing.debugprint(test_fgraph_tt)
Out[&lt;span class="dv"&gt;10&lt;/span&gt;]: Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;10&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;12&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;11&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; P] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is much more to consider in the examples above. Nonetheless, standalone libraries, like &lt;a href="https://github.com/logpy/logpy/"&gt;LogPy&lt;/a&gt; and SymPy, can be adapted to Theano graphs to provide the functionality necessary to cover most of these considerations.&lt;/p&gt;
&lt;p&gt;Finally, let’s briefly imagine how convexity could be determined symbolically. For differentiable terms, we could start with a simple second derivative test. Within Theano, a “second derivative” can be obtained using the &lt;code&gt;hessian&lt;/code&gt; function, and within &lt;code&gt;theano.sandbox.linalg&lt;/code&gt; are &lt;code&gt;Optimizer&lt;/code&gt; hints for matrix positivity and other properties relevant to determining convexity using this simple idea.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="2" title-name=""&gt;
&lt;p&gt;Other great examples of linear algebra themed optimizations are in &lt;code&gt;theano.sandbox.linalg&lt;/code&gt;: for example, &lt;code&gt;no_transpose_symmetric&lt;/code&gt;. Some of these demonstrate exactly how straight-forward it can be to add algebraic considerations.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Although our convexity testing idea is far too simple for many &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, the point we want to make is that the basic code necessary for simple tests like this may already be in place. However, with the logic programming mentioned earlier in this section, comes the possibility of implementing aspects of the convex function calculus, by which one can determine convexity for many more classes of functions.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We’ve sketched out the concepts and mechanism with which one can develop a robust estimation platform that can be transparently guided by the more abstract mathematics frameworks from which new, efficient methods are produced.&lt;/p&gt;
&lt;p&gt;Some key steps in the process will most likely require the integration of a symbolic algebra system, so that a much wider array of algebraic machinery can be leveraged to assess–say–convexity of the terms or to solve the proximal operators themselves. Connections between Theano, SymPy and LogPy have already been explored in &lt;span class="citation" data-cites="rocklin_mathematically_2013"&gt;(Rocklin 2013)&lt;/span&gt;, as well as many other important aspects of the topics discussed here.&lt;/p&gt;
&lt;p&gt;Additionally, more advanced proximal algorithms exist to improve upon the convergence and stability of the most basic proximal gradient given here. These algorithms often involve operator splitting, which requires careful consideration regarding the exact type of splitting and on which terms it is performed. Within this area are the familiar convex-conjugate approaches; these too could be approached by symbolic solvers, or simply addressed by [partially] generated tables.&lt;/p&gt;
&lt;p&gt;Overall, there appear to be many avenues to explore just within the space of proximal algorithms and modern symbolic systems. Not all of this work necessitates the inclusion of fully featured symbolic algebra systems; much can be done with the symbolic tools of Theano alone. Furthermore, there are specialized, lightweight logic programming systems–like LogPy–that can serve as a step before full symbolic algebra integration.&lt;/p&gt;
&lt;p&gt;Besides the automation of proximal algorithms themselves, there are areas of application involving very large and complicated models or graphs–such as the ones arising in Deep Learning. How might we consider the operator splitting of ADMM within deeply layered or hierarchical models &lt;span class="citation" data-cites="polson_statistical_2015"&gt;(Polson, Willard, and Heidari 2015)&lt;/span&gt;? At which levels and on which terms should the splitting be performed? Beyond simply trying to solve the potentially intractable mathematics arising from related questions, with the symbolic capabilities described here, we can at least begin to experiment with the questions.&lt;/p&gt;
&lt;p&gt;Before closing, a very related–and at least as interesting–set of ideas is worth mentioning: the possibility of encoding more symbolic knowledge into probabilistic programming platforms like PyMC3. Using the same optimization mechanisms as the examples here, simple distributional relationships can be encoded. For instance, the convolution of normally distributed random variables:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;mu_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_X&amp;#39;&lt;/span&gt;)
mu_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_X&amp;#39;&lt;/span&gt;)
sd_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;2&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

mu_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_Y&amp;#39;&lt;/span&gt;)
mu_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_Y&amp;#39;&lt;/span&gt;)
sd_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;0.5&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; conv_model:
    X_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, mu_X, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_X, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu_Y, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_Y, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Z_rv &lt;span class="op"&gt;=&lt;/span&gt; X_rv &lt;span class="op"&gt;+&lt;/span&gt; Y_rv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We create a Theano &lt;code&gt;Op&lt;/code&gt; to handle the convolution.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; NormConvOp(tt.Op):
    __props__ &lt;span class="op"&gt;=&lt;/span&gt; ()

    &lt;span class="kw"&gt;def&lt;/span&gt; make_node(&lt;span class="va"&gt;self&lt;/span&gt;, &lt;span class="op"&gt;*&lt;/span&gt;inputs):
        name_new &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;str&lt;/span&gt;.join(&lt;span class="st"&gt;&amp;#39;+&amp;#39;&lt;/span&gt;, [&lt;span class="bu"&gt;getattr&lt;/span&gt;(in_, &lt;span class="st"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;) &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt;
inputs])
        mu_new &lt;span class="op"&gt;=&lt;/span&gt; tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.mu &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt; inputs])
        sd_new &lt;span class="op"&gt;=&lt;/span&gt; tt.sqrt(tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.sd&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt;
inputs]))
        conv_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(name_new, mu&lt;span class="op"&gt;=&lt;/span&gt;mu_new, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_new,
                            &lt;span class="co"&gt;# Is this another place where&lt;/span&gt;
automatically&lt;span class="op"&gt;/&lt;/span&gt;Theano managed
                            &lt;span class="co"&gt;# shapes are really needed.  For now, we&lt;/span&gt;
hack it.
                            shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))

        &lt;span class="cf"&gt;return&lt;/span&gt; tt.Apply(&lt;span class="va"&gt;self&lt;/span&gt;, inputs, [conv_rv])

    &lt;span class="kw"&gt;def&lt;/span&gt; perform(&lt;span class="va"&gt;self&lt;/span&gt;, node, inputs, output_storage):
        z &lt;span class="op"&gt;=&lt;/span&gt; output_storage[&lt;span class="dv"&gt;0&lt;/span&gt;]
        z[&lt;span class="dv"&gt;0&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.add(&lt;span class="op"&gt;*&lt;/span&gt;inputs)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all that’s needed is a &lt;code&gt;PatternSub&lt;/code&gt; like before.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; is_normal_dist(x):
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;hasattr&lt;/span&gt;(x, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;) &lt;span class="op"&gt;and&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(x.distribution,
pm.Normal)

norm_conv_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.add,
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;,
      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)},
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;,
      &lt;span class="co"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)}
     ),
    (NormConvOp(), &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;)),)

norm_conv_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(norm_conv_pat_tt,
                                                   max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

Z_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph([X_rv, Y_rv], [Z_rv])

&lt;span class="co"&gt;# We lose the `FreeRV.distribution` attribute when cloning the graph&lt;/span&gt;
&lt;span class="co"&gt;# with `theano.gof.graph.clone_get_equiv` in `FunctionGraph`, so this&lt;/span&gt;
&lt;span class="co"&gt;# hackishly reattaches that information:&lt;/span&gt;
_ &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="bu"&gt;setattr&lt;/span&gt;(g_in, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;, s_in.distribution)
     &lt;span class="cf"&gt;for&lt;/span&gt; s_in, g_in &lt;span class="op"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;zip&lt;/span&gt;([X_rv, Y_rv], Z_fgraph_tt.inputs)]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; conv_model:
    _ &lt;span class="op"&gt;=&lt;/span&gt; norm_conv_opt_tt.optimize(Z_fgraph_tt)

norm_conv_var_dist &lt;span class="op"&gt;=&lt;/span&gt; Z_fgraph_tt.outputs[&lt;span class="dv"&gt;0&lt;/span&gt;].distribution&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The resulting graph:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;11&lt;/span&gt;]: tt.printing.debugprint(Z_fgraph_tt)
Out[&lt;span class="dv"&gt;11&lt;/span&gt;]: NormConvOp [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;X+Y&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Y [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and the convolution’s parameters (for the test values):&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;12&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.mu.tag.test_value)
Out[&lt;span class="dv"&gt;12&lt;/span&gt;]: [ &lt;span class="dv"&gt;2&lt;/span&gt;.]
In [&lt;span class="dv"&gt;13&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.sd.tag.test_value)
Out[&lt;span class="dv"&gt;13&lt;/span&gt;]: [ &lt;span class="fl"&gt;2.06155281&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;More sophisticated routines–like the example above–could implement parameter expansions, efficient re-parameterizations and equivalent scale mixture forms in an effort to optimize a graph for sampling or point evaluation. Objectives for these optimizations could be straightforward and computationally based (e.g. reducing the number of operations in computations of the log likelihood and other quantities) or more statistically focused (e.g. highly efficient sampling, improve mixing). These ideas are most definitely not new–one example is given by &lt;span class="citation" data-cites="mohasel_afshar_probabilistic_2016"&gt;(Mohasel Afshar 2016)&lt;/span&gt; for symbolic Gibbs sampling, but we hope the examples given here make the point that the tools are readily available and quite accessible.&lt;/p&gt;
&lt;p&gt;We’ll end on a much more spacey consideration. Namely, that this is a context in which we can start experimenting rapidly with objectives over the space of estimation routines. This space is generated by–but not limited to–the variety of symbolic representations, re-parameterizations, etc., mentioned above. It does not necessarily require the complete estimation of a model at each step, nor even the numeric value of quantities like the gradient or Hessian. It may involve them, but not their evaluation; perhaps, instead, symbolic comparisons of competing gradients and Hessians arising from different representations. What we’re describing lies somewhere between the completely numeric assessments common today, and the entirely symbolic work found within the theorems and manipulations of the mathematics we use to derive methods.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-donoho_compressed_2006"&gt;
&lt;p&gt;Donoho, David L. 2006. “Compressed Sensing.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 52 (4): 1289–1306. &lt;a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066" class="uri"&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mohasel_afshar_probabilistic_2016"&gt;
&lt;p&gt;Mohasel Afshar, Hadi. 2016. “Probabilistic Inference in Piecewise Graphical Models.” &lt;a href="https://digitalcollections.anu.edu.au/handle/1885/107386" class="uri"&gt;https://digitalcollections.anu.edu.au/handle/1885/107386&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-park_bayesian_2008"&gt;
&lt;p&gt;Park, Trevor, and George Casella. 2008. “The Bayesian Lasso.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 103 (482): 681–86. &lt;a href="http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337" class="uri"&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. doi:&lt;a href="https://doi.org/10.1214/15-STS530"&gt;10.1214/15-STS530&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_statistical_2015"&gt;
&lt;p&gt;Polson, Nicholas G., Brandon T. Willard, and Massoud Heidari. 2015. “A Statistical Theory of Deep Learning via Proximal Splitting.” &lt;em&gt;ArXiv Preprint ArXiv:1509.06061&lt;/em&gt;. &lt;a href="http://arxiv.org/abs/1509.06061" class="uri"&gt;http://arxiv.org/abs/1509.06061&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rocklin_mathematically_2013"&gt;
&lt;p&gt;Rocklin, Matthew. 2013. “Mathematically Informed Linear Algebra Codes Through Term Rewriting.” PhD thesis, PhD Thesis, August. &lt;a href="http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf" class="uri"&gt;http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-srivastava_dropout_2014"&gt;
&lt;p&gt;Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” &lt;em&gt;The Journal of Machine Learning Research&lt;/em&gt; 15 (1): 1929–58. &lt;a href="http://dl.acm.org/citation.cfm?id=2670313" class="uri"&gt;http://dl.acm.org/citation.cfm?id=2670313&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brandon T. Willard</dc:creator><pubDate>Wed, 18 Jan 2017 00:00:00 -0600</pubDate><guid isPermaLink="false">tag:brandonwillard.github.io,2017-01-18:a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</guid></item></channel></rss>