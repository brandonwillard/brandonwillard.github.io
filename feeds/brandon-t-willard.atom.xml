<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Brandon T. Willard - Brandon T. Willard</title><link href="https://brandonwillard.github.io/" rel="alternate"></link><link href="https://brandonwillard.github.io/feeds/brandon-t-willard.atom.xml" rel="self"></link><id>https://brandonwillard.github.io/</id><updated>2018-12-23T00:00:00-06:00</updated><entry><title>Readable Strings and Relational Programming in Hy</title><link href="https://brandonwillard.github.io/readable-strings-and-relational-programming-in-hy.html" rel="alternate"></link><published>2018-12-20T00:00:00-06:00</published><updated>2018-12-23T00:00:00-06:00</updated><author><name>Brandon T. Willard</name></author><id>tag:brandonwillard.github.io,2018-12-20:/readable-strings-and-relational-programming-in-hy.html</id><summary type="html"></summary><content type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;Readable Strings and Relational Programming in Hy&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  &lt;/style&gt;
  &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;Readable Strings and Relational Programming in Hy&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2018–12–20&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;div class="abstract"&gt;
&lt;p&gt;Just some thoughts on a generalized &lt;code&gt;repr&lt;/code&gt; for Hy and some connections with relational programming.&lt;/p&gt;
&lt;/div&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the past few months, I’ve been working on &lt;a href="https://github.com/hylang/hy"&gt;Hy&lt;/a&gt; a lot. It’s been great for translating symbolic computation ideas originating in the Lisp community or simply performing the generic meta-programming inherent to the subject.&lt;/p&gt;
&lt;p&gt;One feature I’ve been missing the most is “readable” print-outs from the REPL. In this case, “readable” means “a string that can be &lt;code&gt;eval&lt;/code&gt;’ed to [re-]produce the object it’s meant to represent”. &lt;a href="https://docs.python.org/3/library/functions.html#repr"&gt;Python calls the function(s) that produce these strings “&lt;code&gt;repr&lt;/code&gt;”s&lt;/a&gt; and provides a generic &lt;code&gt;repr&lt;/code&gt; function–with limited Python “readability” guarantees–and a &lt;code&gt;__repr__&lt;/code&gt; property for object/class-level customization.&lt;/p&gt;
&lt;div class="example" data-markdown="" data-env-number="1"&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb1-1" data-line-number="1"&gt;test_obj &lt;span class="op"&gt;=&lt;/span&gt; {&lt;span class="st"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;: &lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="st"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;: [&lt;span class="dv"&gt;2&lt;/span&gt;, &lt;span class="dv"&gt;3&lt;/span&gt;]}&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-3" data-line-number="3"&gt;&lt;span class="co"&gt;# Produce a readable string using `repr`&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-4" data-line-number="4"&gt;obj_repr_str &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;repr&lt;/span&gt;(test_obj)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-5" data-line-number="5"&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(obj_repr_str)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-6" data-line-number="6"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-7" data-line-number="7"&gt;&lt;span class="co"&gt;# Re-create the object from its readable string form&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-8" data-line-number="8"&gt;obj_from_repr &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;eval&lt;/span&gt;(obj_repr_str)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-9" data-line-number="9"&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(obj_from_repr)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-11" data-line-number="11"&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(test_obj &lt;span class="op"&gt;==&lt;/span&gt; obj_from_repr)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb2"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb2-1" data-line-number="1"&gt;{&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;: &lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;: [&lt;span class="dv"&gt;2&lt;/span&gt;, &lt;span class="dv"&gt;3&lt;/span&gt;]}&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-2" data-line-number="2"&gt;{&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;: &lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;: [&lt;span class="dv"&gt;2&lt;/span&gt;, &lt;span class="dv"&gt;3&lt;/span&gt;]}&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-3" data-line-number="3"&gt;&lt;span class="va"&gt;True&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-4" data-line-number="4"&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There’s already a &lt;code&gt;hy.contrib.hy-repr&lt;/code&gt; module that gets most of the way there, but it doesn’t implement the Python standard library’s &lt;code&gt;reprlib.Repr&lt;/code&gt;. The class &lt;code&gt;reprlib.Repr&lt;/code&gt; implements limits for the display lengths of the strings it produces, and its source code provides a few standard library implementations of primitive object &lt;code&gt;repr&lt;/code&gt;s–which require only trivial changes to produce the desired Hy syntax.&lt;/p&gt;
&lt;p&gt;For these reasons–and an overall interest in using and translating more of the Python standard library to Hy–I decided to try a quick refactoring of &lt;code&gt;hy.contrib.hy-repr&lt;/code&gt; that implements &lt;code&gt;reprlib.Repr&lt;/code&gt;.&lt;/p&gt;
&lt;section id="the-problems" class="level2"&gt;
&lt;h2&gt;The Problem(s)&lt;/h2&gt;
&lt;p&gt;The translation of Hy AST to string form is fairly straight-forward. In most cases, one only needs to change the &lt;code&gt;repr&lt;/code&gt;s for Python primitives and basic function calls (e.g. from &lt;code&gt;func(1)&lt;/code&gt; to &lt;code&gt;(func 1)&lt;/code&gt;); however, changing just a couple lines in &lt;code&gt;repr&lt;/code&gt;/&lt;code&gt;__repr__&lt;/code&gt; functions for all the Python builtins is very annoying.&lt;/p&gt;
&lt;p&gt;Furthermore, what about those custom object &lt;code&gt;__repr__&lt;/code&gt; methods? While one might be able to manually patch most–if not all–of the (Python-implemented) standard library objects, there are far too many 3rd-party library &lt;code&gt;__repr__&lt;/code&gt;s with exactly the same trivial function-call form that can’t reasonably be patched.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="some-approaches" class="level2"&gt;
&lt;h2&gt;Some approaches&lt;/h2&gt;
&lt;p&gt;The first few things that come to mind when considering a more general approach to Python-to-Hy &lt;code&gt;__repr__&lt;/code&gt; translation involve some use of the existing &lt;code&gt;repr&lt;/code&gt; code. That might come in the form of string manipulation of &lt;code&gt;repr&lt;/code&gt; output, which &lt;code&gt;hy.contrib.hy-repr&lt;/code&gt; already does in some cases, or quite possibly some use of a &lt;code&gt;repr&lt;/code&gt; function’s source or code object.&lt;/p&gt;
&lt;p&gt;The latter seems like it has the potential to be more thorough and far-reaching, but also considerably more involved and computationally inefficient. Unfortunately, similar things can be said about the regex approach. Although it does seem a little easier to implement and–for limited cases–efficient enough for most purposes, it also comes across as much more brittle.&lt;/p&gt;
&lt;p&gt;Fortunately, the latter is unnecessary, because, when the existing &lt;code&gt;repr&lt;/code&gt; output is Python readable, it can be parsed by &lt;code&gt;ast.parse&lt;/code&gt;. The function &lt;code&gt;ast.parse&lt;/code&gt; effectively handles the regex work and yields the bulk of information needed for a Hy &lt;code&gt;repr&lt;/code&gt; string: the function name and its (positional and keyword) arguments.&lt;/p&gt;
&lt;div class="example" data-markdown="" data-env-number="2"&gt;
&lt;p&gt;Let’s say we implement our own object and &lt;code&gt;repr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(defclass TestClass [object]
  (defn --init-- [self arg1 arg2 &amp;amp;optional kwarg1 kwarg2]
    (setv self.arg1 arg1
          self.arg2 arg2
          self.kwarg1 kwarg1
          self.kwarg2 kwarg2))
  (defn --repr-- [self]
    (.format &amp;quot;TestClass({}, {}, kwarg1={}, kwarg2={})&amp;quot;
             #* (lfor a [self.arg1 self.arg2
                         self.kwarg1 self.kwarg2]
                      (repr a)))))

(setv test-obj (TestClass 1 {&amp;quot;a&amp;quot; 1 &amp;quot;b&amp;quot; 2} :kwarg1 1 :kwarg2 &amp;quot;ok&amp;quot;))
(print (repr test-obj))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="sourceCode" id="cb4"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb4-1" data-line-number="1"&gt;TestClass(&lt;span class="dv"&gt;1&lt;/span&gt;, {&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;: &lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;: &lt;span class="dv"&gt;2&lt;/span&gt;}, kwarg1&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, kwarg2&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;ok&amp;#39;&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since the results are readable, we can do the following:&lt;/p&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(import ast astor)
(setv repr-ast (ast.parse (repr test-obj) :mode &amp;quot;eval&amp;quot;))
(print (astor.dump repr-ast))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="sourceCode" id="cb6"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb6-1" data-line-number="1"&gt;Expression(&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-2" data-line-number="2"&gt;    body&lt;span class="op"&gt;=&lt;/span&gt;Call(func&lt;span class="op"&gt;=&lt;/span&gt;Name(&lt;span class="bu"&gt;id&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;TestClass&amp;#39;&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-3" data-line-number="3"&gt;              args&lt;span class="op"&gt;=&lt;/span&gt;[Num(n&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-4" data-line-number="4"&gt;                    Dict(keys&lt;span class="op"&gt;=&lt;/span&gt;[Str(s&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;), Str(s&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;)],&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-5" data-line-number="5"&gt;                         values&lt;span class="op"&gt;=&lt;/span&gt;[Num(n&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;), Num(n&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;)])],&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-6" data-line-number="6"&gt;              keywords&lt;span class="op"&gt;=&lt;/span&gt;[keyword(arg&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;kwarg1&amp;#39;&lt;/span&gt;, value&lt;span class="op"&gt;=&lt;/span&gt;Num(n&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;)),&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-7" data-line-number="7"&gt;                        keyword(arg&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;kwarg2&amp;#39;&lt;/span&gt;, value&lt;span class="op"&gt;=&lt;/span&gt;Str(s&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;ok&amp;#39;&lt;/span&gt;))]))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="an-implemented-partial-solution" class="level1"&gt;
&lt;h1&gt;An Implemented Partial Solution&lt;/h1&gt;
&lt;p&gt;With existing &lt;code&gt;repr&lt;/code&gt; output converted to Python AST by Python itself (using &lt;code&gt;ast.parse&lt;/code&gt;), we can produce readable Hy strings from the resulting AST objects.&lt;/p&gt;
&lt;p&gt;In this scenario, we need only be concerned with the conversion of Python AST into readable Hy strings. This works like an inverse to the Hy compiler: in other words, a Hy decompiler. For &lt;code&gt;repr&lt;/code&gt; purposes, only function call statements and their arguments need to be decompiled. Unfortunately, function arguments can consist of arbitrary Python/Hy objects, and that’s how the decompilation responsibilities start to expand. If we limit our scope to a reasonable subset of Python builtins/primitives, the results can still be quite effective, and won’t require a complete decompiler.&lt;/p&gt;
&lt;p&gt;On the down-side, if a Hy &lt;code&gt;repr&lt;/code&gt; implementation overrides the built-in &lt;code&gt;repr&lt;/code&gt;, then arguments in existing &lt;code&gt;repr&lt;/code&gt;/&lt;code&gt;__repr__&lt;/code&gt;s might already be converted by the overridden &lt;code&gt;repr&lt;/code&gt;; however, the results from &lt;code&gt;ast.parse&lt;/code&gt; will undo/discard those results. Even so, custom class &lt;code&gt;__repr__&lt;/code&gt;s aren’t guaranteed to use the built-in &lt;code&gt;repr&lt;/code&gt; on their arguments, so attempts to salvage already-converted &lt;code&gt;repr&lt;/code&gt; output are undeniably fraught with complications.&lt;/p&gt;
&lt;div class="example" data-markdown="" data-env-number="3"&gt;
&lt;p&gt;Working from the &lt;code&gt;repr&lt;/code&gt;-produced AST above, I mocked-up a quick prototype for a generic Python-to-Hy conversion function.&lt;/p&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(import ast)
(import builtins)

(import [hy.contrib.hy-repr [hy-repr :as -hy-repr]])

(defn ast-funcall-to-hy [ast-obj repr1
                         &amp;amp;optional [level 1]]
  &amp;quot;Turn Python `ast.Call` expressions into Hy `repr` strings.

XXX: Only a very minimal subset of Python-to-Hy AST is implemented.

This can be used to turn a \&amp;quot;readable\&amp;quot; `repr` result, via an actual \&amp;quot;read\&amp;quot; by
`ast.parse`, to Python AST then Hy AST.
&amp;quot;
  (assert (and (instance? ast.Expression ast-obj)
               (instance? ast.Call ast-obj.body)))
  (setv func-name (. ast-obj body func id))
  (setv eval-fn (fn [o]
                  (if (instance? ast.Name o)
                      o.id
                      (repr1 (ast.literal-eval o) (dec level)))))
  (setv func-args (lfor a (. ast-obj body args) (eval-fn a)))
  (setv func-kwargs (lfor k (. ast-obj body keywords)
                          (.format &amp;quot;:{} {}&amp;quot; k.arg (eval-fn k.value))))
  (.format &amp;quot;({})&amp;quot; (.join &amp;quot; &amp;quot; (+ [func-name] func-args func-kwargs))))


(setv test-ast (ast.parse &amp;quot;range(x, y, blah=1, bloh=\&amp;quot;ok\&amp;quot;)&amp;quot; :mode &amp;quot;eval&amp;quot;))
(print (ast-funcall-to-hy test-ast (fn [x &amp;amp;rest y] (-hy-repr x))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(range x y :blah 1 :bloh &amp;quot;ok&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ast-funcall-to-hy&lt;/code&gt; is an extremely narrow decompiler that only handles readable function calls (represented by &lt;code&gt;ast.Call&lt;/code&gt; nodes), but, as part of a fallback sequence in a Hy &lt;code&gt;repr&lt;/code&gt; implementation, it’s still pretty useful.&lt;/p&gt;
&lt;p&gt;A function like &lt;code&gt;ast-funcall-to-hy&lt;/code&gt; can be used in &lt;code&gt;repr&lt;/code&gt; logic as follows:&lt;/p&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(defn hy-repr [x &amp;amp;optional [level 1] [-repr (fn [x &amp;amp;rest y] (-hy-repr x))]]
  &amp;quot;Use `builtin.repr` results to generate readable Hy `repr` strings for cases
we haven&amp;#39;t covered explicitly.
&amp;quot;
  (try
    (setv s (builtins.repr x))
    (when (not (.startswith s &amp;quot;&amp;lt;&amp;quot;))
      (do
        (setv repr-ast (ast.parse s :mode &amp;quot;eval&amp;quot;))
        (setv s (ast-funcall-to-hy repr-ast -repr))))
    s
    (except [Exception]
      (.format &amp;quot;&amp;lt;{} instance at {}&amp;gt;&amp;quot; x.__class__.__name__ (id x)))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, for the example class, &lt;code&gt;TestClass&lt;/code&gt;, we can demonstrate automatic conversion of its Python &lt;code&gt;__repr__&lt;/code&gt; implementation.&lt;/p&gt;
&lt;pre class="hy"&gt;&lt;code&gt;(setv test-ast (TestClass 1 {&amp;quot;a&amp;quot; 2 &amp;quot;b&amp;quot; 3} :kwarg1 1 :kwarg2 &amp;quot;ok&amp;quot;))
(print (.format &amp;quot;before: {}\nafter: {}&amp;quot;
                (repr test-ast)
                (hy-repr test-ast)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="text"&gt;&lt;code&gt;before: TestClass(1, {&amp;#39;a&amp;#39;: 2, &amp;#39;b&amp;#39;: 3}, kwarg1=1, kwarg2=&amp;#39;ok&amp;#39;)
after: (TestClass 1 {&amp;quot;a&amp;quot; 2  &amp;quot;b&amp;quot; 3} :kwarg1 1 :kwarg2 &amp;quot;ok&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;section id="a-use-for-relational-programming" class="level1"&gt;
&lt;h1&gt;A use for relational programming&lt;/h1&gt;
&lt;p&gt;While considering all this, I kept thinking about how nice it would be to have a “bijective” compiler; in other words, the existing Hy compiler, which translates Hy-to-Python, &lt;strong&gt;and&lt;/strong&gt; a Python-to-Hy (de)compiler. With a Python-to-Hy AST compiler, we could more broadly convert Python AST output–like the kind in our example above–to a &lt;code&gt;repr&lt;/code&gt;/readable string in Hy.&lt;/p&gt;
&lt;p&gt;The idea isn’t too crazy, especially since one can easily work backward from a lot of the logic in the existing Hy compiler. There will be some edge cases that result in non-bijective translations (i.e. some round-trip Hy/Python translations might only be &lt;strong&gt;equal&lt;/strong&gt; and not exactly &lt;strong&gt;equivalent&lt;/strong&gt;), but this isn’t necessarily a blocking issue. Decisions regarding “canonical” or reduced forms of Hy/Python AST might be necessary, especially if the resulting AST is intended to be more human readable than not.&lt;/p&gt;
&lt;p&gt;Perhaps what’s more discouraging is the effort it would take to ensure that the compilation processes going both ways are–and stay–coherent during the course of development. For instance, when changes are made to the standard compilation process (i.e. Hy-to-Python), it’s likely that changes and tests would also be needed for the other direction.&lt;/p&gt;
&lt;p&gt;This is where a paradigm like relational programming is particularly appealing: it provides a language for defining–and means for computing–the maps&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
  \text{Hy Syntax}
  \longleftrightarrow \text{Python AST}
  \longleftrightarrow \text{Python Syntax}
  \;
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;in a cohesive way.&lt;/p&gt;
&lt;p&gt;My relational programming DSL of choice, &lt;a href="http://minikanren.org"&gt;miniKanren&lt;/a&gt;, already has an implementation in Hy: &lt;a href="https://github.com/algernon/adderall"&gt;&lt;code&gt;loghyc&lt;/code&gt; (and to be formally known as &lt;code&gt;adderall&lt;/code&gt;)&lt;/a&gt;. We’ve been using it to perform static code analysis and refactoring in the project &lt;a href="https://github.com/hylang/hydiomatic"&gt;&lt;code&gt;hydiomatic&lt;/code&gt;&lt;/a&gt;, so there’s also a precedent for parsing Hy syntax in a relational context.&lt;/p&gt;
&lt;p&gt;The missing/next step would be to output Python AST (instead of more Hy forms, like &lt;code&gt;hydiomatic&lt;/code&gt; produces, for example).&lt;/p&gt;
&lt;p&gt;Perhaps, in a follow-up, I’ll illustrate how this can be done.&lt;/p&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</content><category term="hy"></category><category term="relational programming"></category><category term="python"></category></entry><entry><title>Data Science at Citybase</title><link href="https://brandonwillard.github.io/data-science-at-citybase.html" rel="alternate"></link><published>2018-12-18T00:00:00-06:00</published><updated>2018-12-23T00:00:00-06:00</updated><author><name>Brandon T. Willard</name></author><id>tag:brandonwillard.github.io,2018-12-18:/data-science-at-citybase.html</id><summary type="html"></summary><content type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;Data Science at Citybase&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;Data Science at Citybase&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2018–12–18&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;p&gt;I recently wrote about data science at CityBase on the CityBase blog: &lt;sup id="c26e94c1b0b80ac3545371089d4f9936"&gt;&lt;a href="#WillardProgrammingIntelligentCity2018a"&gt;Programming an Intelligent City: The Role of Data Science&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;section id="bibliography" class="level1"&gt;
&lt;h1&gt;Bibliography&lt;/h1&gt;
&lt;p&gt;&lt;a id="WillardProgrammingIntelligentCity2018a"&gt;&lt;/a&gt;[WillardProgrammingIntelligentCity2018a] Willard, Programming an Intelligent City: The Role of Data Science, &lt;i&gt;CityBase&lt;/i&gt;, (2018). &lt;a href="https://thecitybase.com/programming-an-intelligent-city-the-role-of-data-science/"&gt;link&lt;/a&gt;. &lt;a href="#c26e94c1b0b80ac3545371089d4f9936"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</content><category term="statistics"></category><category term="symbolic computation"></category><category term="citybase"></category></entry><entry><title>More Proximal Estimation</title><link href="https://brandonwillard.github.io/more-proximal-estimation.html" rel="alternate"></link><published>2017-03-06T00:00:00-06:00</published><updated>2017-03-06T00:00:00-06:00</updated><author><name>Brandon T. Willard</name></author><id>tag:brandonwillard.github.io,2017-03-06:/more-proximal-estimation.html</id><summary type="html"></summary><content type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;More Proximal Estimation&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  &lt;/style&gt;
  &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;More Proximal Estimation&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–03–06&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The focal point of this short exposition will be an elaboration of the basic &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; penalization problem discussed in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;, &lt;span class="math display"&gt;\[\begin{equation}
\operatorname*{argmin}_{\beta} \left\{
  \frac{1}{2} \|y - X \beta\|^2_2
    + \lambda \|\beta\|_1
  \right\}
  \;.
  \label{eq:lasso}
\end{equation}\]&lt;/span&gt; We continue our discussion on topics concerning automation and symbolic computation in Theano &lt;span class="citation" data-cites="bergstra_theano_2010"&gt;(Bergstra et al. 2010)&lt;/span&gt;, as well as the mathematical methodology we believe is suitable for such implementations. Again, our framing of the problem is in terms of “proximal methods” &lt;span class="citation" data-cites="parikh_proximal_2014 combettes_proximal_2011"&gt;(Parikh and Boyd 2014; Combettes and Pesquet 2011)&lt;/span&gt;. Along the way we propose one simple means of placing the well-known technique of coordinate descent within the scope of proximal methods via a general property of proximal operators. These efforts are a continued outgrowth of our work in &lt;span class="citation" data-cites="polson_proximal_2015"&gt;Polson, Scott, and Willard (2015)&lt;/span&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="proximal-and-computational-components" class="level1"&gt;
&lt;h1&gt;Proximal and Computational Components&lt;/h1&gt;
&lt;p&gt;First, we [re]-introduce the workhorse of proximal methods: the &lt;em&gt;proximal operator&lt;/em&gt;.&lt;/p&gt;
&lt;div class="definition" data-markdown="" data-env-number="1" data-title-name="[Proximal Operator]"&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{prox}_{\phi}(x) =
    \operatorname*{argmin}_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspired by Equation &lt;span class="math inline"&gt;\(\eqref{eq:lasso}\)&lt;/span&gt;, we produce a toy dataset as follows:&lt;/p&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb1-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-3" data-line-number="3"&gt;M &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;50&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-4" data-line-number="4"&gt;M_nonzero &lt;span class="op"&gt;=&lt;/span&gt; M &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="op"&gt;//&lt;/span&gt; &lt;span class="dv"&gt;10&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-5" data-line-number="5"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-6" data-line-number="6"&gt;beta_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-7" data-line-number="7"&gt;beta_true[:M_nonzero] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(M_nonzero)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-8" data-line-number="8"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-9" data-line-number="9"&gt;N &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(beta_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.4&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-10" data-line-number="10"&gt;X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(N, M)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-11" data-line-number="11"&gt;mu_true &lt;span class="op"&gt;=&lt;/span&gt; X.dot(beta_true)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-12" data-line-number="12"&gt;y &lt;span class="op"&gt;=&lt;/span&gt; mu_true &lt;span class="op"&gt;+&lt;/span&gt; sc.stats.norm.rvs(np.zeros(N), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-14" data-line-number="14"&gt;X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-15" data-line-number="15"&gt;y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-16" data-line-number="16"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-17" data-line-number="17"&gt;&lt;span class="co"&gt;# Estimation starting parameters...&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-18" data-line-number="18"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;]).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-19" data-line-number="19"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-20" data-line-number="20"&gt;&lt;span class="co"&gt;# Gradient [starting] step size&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-21" data-line-number="21"&gt;alpha_0 &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1.&lt;/span&gt; &lt;span class="op"&gt;/&lt;/span&gt; np.linalg.norm(X, &lt;span class="dv"&gt;2&lt;/span&gt;)&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-22" data-line-number="22"&gt;&lt;span class="co"&gt;# np.linalg.matrix_rank(X)&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-23" data-line-number="23"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-24" data-line-number="24"&gt;&lt;span class="co"&gt;# Regularization value heuristic&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-25" data-line-number="25"&gt;&lt;span class="co"&gt;# beta_ols = np.linalg.lstsq(X, y)[0]&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-26" data-line-number="26"&gt;&lt;span class="co"&gt;# lambda_max = 0.1 * np.linalg.norm(beta_ols, np.inf)&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-27" data-line-number="27"&gt;lambda_max &lt;span class="op"&gt;=&lt;/span&gt; np.linalg.norm(X.T.dot(y), np.inf)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;, we can start with a model defined within a system like PyMC3 &lt;span class="citation" data-cites="salvatier_probabilistic_2016"&gt;(Salvatier, Wiecki, and Fonnesbeck 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode" id="cb2"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb2-1" data-line-number="1"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-2" data-line-number="2"&gt;    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-3" data-line-number="3"&gt;                         shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-4" data-line-number="4"&gt;    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-5" data-line-number="5"&gt;                     shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this setting one might then arrive at the necessary steps toward estimation automatically (i.e. identify the underlying &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; estimation problem). We discuss this more in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For simplicity, we’ll just assume that all components of the estimation problem are know–i.e. loss and penalty functions. The proximal operator that arises in this standard example is the &lt;em&gt;soft thresholding&lt;/em&gt; operator. In Theano, it can be implemented with the following:&lt;/p&gt;
&lt;div class="sourceCode" id="cb3"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb3-1" data-line-number="1"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; tt_soft_threshold(beta_, lambda_):&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-2" data-line-number="2"&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="remark" data-markdown="" data-env-number="1" data-title-name=""&gt;
&lt;p&gt;This operator can take other forms, and the one used here is likely not the best. The &lt;code&gt;maximum&lt;/code&gt; can be replaced by other conditional-like statements–such as &lt;span class="math display"&gt;\[\begin{equation*}
\operatorname{S}(z, \lambda) =
    \begin{cases}
     {\mathop{\mathrm{sgn}}}(\beta) (\beta - \lambda) &amp;amp; \beta &amp;gt; \lambda
     \\
     0 &amp;amp; \text{otherwise}
    \end{cases}
    \;.
\end{equation*}\]&lt;/span&gt; If we were to–say–multiply the output of this operator with another, more difficult to compute result, then we might also wish to extend this multiplication into the definition of the operator and avoid its computation in the &lt;span class="math inline"&gt;\(\beta \leq \lambda\)&lt;/span&gt; case.&lt;/p&gt;
&lt;p&gt;Barring any reuses of this quantity, or a need to preserve undefined results produced by an expensive product with zero, we would ideally like a “compiler” to make such an optimization itself. It isn’t clear how a standard compiler–or interpreter/hybrid–could safely make this optimization, whereas it does seem more reasonable as a symbolic/Theano optimization.&lt;/p&gt;
&lt;p&gt;Optimizations like this are–I think–a necessary step to enable expressive, generalized methods, truly rapid prototyping at the math level.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now, assuming that we’ve obtained the relevant loss and penalty functions–for example, in PyMC3–then we can proceed to setting up the exact context of our proximal problem.&lt;/p&gt;
&lt;div class="sourceCode" id="cb4"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb4-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-3" data-line-number="3"&gt;&lt;span class="co"&gt;# Clone the negative log-likelihood of our observation model.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-4" data-line-number="4"&gt;nlogl_rv &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-5" data-line-number="5"&gt;nlogl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(nlogl_rv)&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-6" data-line-number="6"&gt;nlogl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;-logl&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-7" data-line-number="7"&gt;beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_inputs([nlogl])[&lt;span class="dv"&gt;4&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="proximal-gradient" class="level1"&gt;
&lt;h1&gt;Proximal Gradient&lt;/h1&gt;
&lt;p&gt;In what follows it will be convenient to generalize a bit and work in terms of arbitrary loss and penalty functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, respectively, which in our case corresponds to &lt;span class="math display"&gt;\[\begin{equation*}
\begin{gathered}
  l(\beta) = \frac12 \|y - X \beta\|^2_2, \quad
  \text{and}\;
  \phi(\beta) = \|\beta\|_1
  \;.\end{gathered}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The proximal gradient &lt;span class="citation" data-cites="combettes_proximal_2011"&gt;(Combettes and Pesquet 2011)&lt;/span&gt; algorithm is a staple of the proximal framework that provides solutions to problems of the form &lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{argmin}_\beta \left\{
    l(\beta) + \lambda \phi(\beta)
  \right\}
  \;,
\end{equation*}\]&lt;/span&gt; when both &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are lower semi-continuous convex functions, and &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; is differentiable with Lipschitz gradient.&lt;/p&gt;
&lt;p&gt;The solution is given as the following fixed-point: &lt;span class="math display"&gt;\[\begin{equation}
\beta = \operatorname*{prox}_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}\]&lt;/span&gt; The constant step size &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; is related to the Lipschitz constant of &lt;span class="math inline"&gt;\(\nabla l\)&lt;/span&gt;, but can also be a sequence obeying certain constraints. Since our &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; under consideration is &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt;, we have the incredibly standard &lt;span class="math inline"&gt;\(\nabla l(\beta) = X^\top (X \beta - y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;section id="implementation" class="level2"&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;As in &lt;span class="citation" data-cites="willard_role_2017"&gt;Willard (2017)&lt;/span&gt;, we provide an implementation of a proximal gradient step.&lt;/p&gt;
&lt;div class="sourceCode" id="cb5"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb5-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; function &lt;span class="im"&gt;as&lt;/span&gt; tt_function&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-2" data-line-number="2"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano.&lt;span class="bu"&gt;compile&lt;/span&gt;.nanguardmode &lt;span class="im"&gt;import&lt;/span&gt; NanGuardMode&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-4" data-line-number="4"&gt;tt_func_mode &lt;span class="op"&gt;=&lt;/span&gt; NanGuardMode(nan_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-5" data-line-number="5"&gt;                            inf_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-6" data-line-number="6"&gt;                            big_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-7" data-line-number="7"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-8" data-line-number="8"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-9" data-line-number="9"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; prox_gradient_step(loss, beta_tt, prox_func,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-10" data-line-number="10"&gt;                       alpha_tt&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;, lambda_tt&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-11" data-line-number="11"&gt;                       return_loss_grad&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-12" data-line-number="12"&gt;                       tt_func_kwargs&lt;span class="op"&gt;=&lt;/span&gt;{&lt;span class="st"&gt;&amp;#39;mode&amp;#39;&lt;/span&gt;: tt_func_mode}&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-13" data-line-number="13"&gt;                       ):&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-14" data-line-number="14"&gt;    &lt;span class="co"&gt;r&amp;quot;&amp;quot;&amp;quot; Creates a function that produces a proximal gradient step.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-15" data-line-number="15"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-16" data-line-number="16"&gt;&lt;span class="co"&gt;    Arguments&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-17" data-line-number="17"&gt;&lt;span class="co"&gt;    =========&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-18" data-line-number="18"&gt;&lt;span class="co"&gt;    loss: TensorVariable&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-19" data-line-number="19"&gt;&lt;span class="co"&gt;        Continuously differentiable &amp;quot;loss&amp;quot; function in the objective&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-20" data-line-number="20"&gt;&lt;span class="co"&gt;        function.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-21" data-line-number="21"&gt;&lt;span class="co"&gt;    beta_tt: TensorVariable&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-22" data-line-number="22"&gt;&lt;span class="co"&gt;        Variable argument of the loss function.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-23" data-line-number="23"&gt;&lt;span class="co"&gt;    prox_fn: function&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-24" data-line-number="24"&gt;&lt;span class="co"&gt;        Function that computes the proximal operator for the &amp;quot;penalty&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-25" data-line-number="25"&gt;&lt;span class="co"&gt;        function.  Must take two parameters: the first a&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-26" data-line-number="26"&gt;&lt;span class="co"&gt;TensorVariable&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-27" data-line-number="27"&gt;&lt;span class="co"&gt;        of the gradient step, the second a float or Scalar value.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-28" data-line-number="28"&gt;&lt;span class="co"&gt;    alpha_tt: float, Scalar (optional)&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-29" data-line-number="29"&gt;&lt;span class="co"&gt;        Gradient step size.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-30" data-line-number="30"&gt;&lt;span class="co"&gt;    lambda_tt: float, Scalar (optional)&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-31" data-line-number="31"&gt;&lt;span class="co"&gt;        Additional scalar value passed to `prox_fn`.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-32" data-line-number="32"&gt;&lt;span class="co"&gt;        &lt;/span&gt;&lt;span class="al"&gt;TODO&lt;/span&gt;&lt;span class="co"&gt;: Not sure if this should be here; is redundant.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-33" data-line-number="33"&gt;&lt;span class="co"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-34" data-line-number="34"&gt;    loss_grad &lt;span class="op"&gt;=&lt;/span&gt; tt.grad(loss, wrt&lt;span class="op"&gt;=&lt;/span&gt;beta_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-35" data-line-number="35"&gt;    loss_grad.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;loss_grad&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-36" data-line-number="36"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-37" data-line-number="37"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; alpha_tt &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-38" data-line-number="38"&gt;        alpha_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;alpha&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-39" data-line-number="39"&gt;        alpha_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-40" data-line-number="40"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; lambda_tt &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-41" data-line-number="41"&gt;        lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-42" data-line-number="42"&gt;        lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-43" data-line-number="43"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-44" data-line-number="44"&gt;    beta_grad_step &lt;span class="op"&gt;=&lt;/span&gt; beta_tt &lt;span class="op"&gt;-&lt;/span&gt; alpha_tt &lt;span class="op"&gt;*&lt;/span&gt; loss_grad&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-45" data-line-number="45"&gt;    beta_grad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_grad_step&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-46" data-line-number="46"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-47" data-line-number="47"&gt;    prox_grad_step &lt;span class="op"&gt;=&lt;/span&gt; prox_func(beta_grad_step, lambda_tt &lt;span class="op"&gt;*&lt;/span&gt; alpha_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-48" data-line-number="48"&gt;    prox_grad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;prox_grad_step&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-49" data-line-number="49"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-50" data-line-number="50"&gt;    inputs &lt;span class="op"&gt;=&lt;/span&gt; []&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-51" data-line-number="51"&gt;    updates &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-52" data-line-number="52"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(beta_tt, tt.sharedvar.SharedVariable):&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-53" data-line-number="53"&gt;        updates &lt;span class="op"&gt;=&lt;/span&gt; [(beta_tt, prox_grad_step)]&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-54" data-line-number="54"&gt;    &lt;span class="cf"&gt;else&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-55" data-line-number="55"&gt;        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [beta_tt]&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-56" data-line-number="56"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="kw"&gt;not&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(alpha_tt, tt.sharedvar.SharedVariable):&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-57" data-line-number="57"&gt;        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [alpha_tt]&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-58" data-line-number="58"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="kw"&gt;not&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(lambda_tt, tt.sharedvar.SharedVariable):&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-59" data-line-number="59"&gt;        inputs &lt;span class="op"&gt;+=&lt;/span&gt; [lambda_tt]&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-60" data-line-number="60"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-61" data-line-number="61"&gt;    prox_grad_step_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function(inputs,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-62" data-line-number="62"&gt;                                    prox_grad_step,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-63" data-line-number="63"&gt;                                    updates&lt;span class="op"&gt;=&lt;/span&gt;updates,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-64" data-line-number="64"&gt;                                    &lt;span class="op"&gt;**&lt;/span&gt;tt_func_kwargs)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-65" data-line-number="65"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-66" data-line-number="66"&gt;    res &lt;span class="op"&gt;=&lt;/span&gt; (prox_grad_step_fn,)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-67" data-line-number="67"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; return_loss_grad:&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-68" data-line-number="68"&gt;        res &lt;span class="op"&gt;+=&lt;/span&gt; (loss_grad,)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-69" data-line-number="69"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-70" data-line-number="70"&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; res&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="step-sizes" class="level2"&gt;
&lt;h2&gt;Step Sizes&lt;/h2&gt;
&lt;p&gt;A critical aspect of the proximal gradient approach–and most optimization–involves the use of appropriate step sizes, &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt;. They needn’t always be fixed values, and, because of this, we can search for a suitable value during estimation. Furthermore, in some cases, step sizes can be sequences amenable to acceleration techniques &lt;span class="citation" data-cites="beck_fast_2014"&gt;(Beck and Teboulle 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These values have obvious connections to the performance of an optimization method–beyond basic guarantees of convergence, so the power of any implementation will depend on how much support it has for various types of step size sequences.&lt;/p&gt;
&lt;p&gt;Often acceptable ranges of step size values are derived from Lipschitz and related properties of the functions involved–and/or their gradients. Similar considerations underlie the classical line-search methods in optimization, and give meaning to what some call “tuning parameters”. These connections between function-analytic properties and “tuning parameters” themselves highlight the need for more mathematical coverage within implementations–by which we imply their place in a fully computational, symbolic setting.&lt;/p&gt;
&lt;p&gt;In this spirit, one particularly relevant direction of work can be found in Theano’s experimental matrix “Hints”. The ideas behind &lt;code&gt;theano.sandbox.linalg.ops.{psd, spectral_radius_bound}&lt;/code&gt; examples of the machinery needed to automatically determine applicable and efficient &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; constants and sequences.&lt;/p&gt;
&lt;p&gt;In our example, we use the standard backtracking line-search.&lt;/p&gt;
&lt;div class="sourceCode" id="cb6"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb6-1" data-line-number="1"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; backtracking_search(beta_, alpha_,&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-2" data-line-number="2"&gt;                        prox_fn, loss_fn, loss_grad_fn,&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-3" data-line-number="3"&gt;                        lambda_&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, bt_rate&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt;, obj_tol&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e-5&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-4" data-line-number="4"&gt;    &lt;span class="co"&gt;# alpha_start = alpha_&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-5" data-line-number="5"&gt;    z &lt;span class="op"&gt;=&lt;/span&gt; beta_&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-6" data-line-number="6"&gt;    beta_start_ &lt;span class="op"&gt;=&lt;/span&gt; beta_&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-7" data-line-number="7"&gt;    loss_start_ &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(beta_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-8" data-line-number="8"&gt;    loss_grad_start_ &lt;span class="op"&gt;=&lt;/span&gt; loss_grad_fn(beta_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-9" data-line-number="9"&gt;    &lt;span class="cf"&gt;while&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-11" data-line-number="11"&gt;        beta_ &lt;span class="op"&gt;=&lt;/span&gt; beta_start_ &lt;span class="op"&gt;-&lt;/span&gt; alpha_ &lt;span class="op"&gt;*&lt;/span&gt; loss_grad_start_&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-12" data-line-number="12"&gt;        z &lt;span class="op"&gt;=&lt;/span&gt; prox_fn(beta_, alpha_ &lt;span class="op"&gt;*&lt;/span&gt; lambda_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-14" data-line-number="14"&gt;        loss_z &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(z)&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-15" data-line-number="15"&gt;        step_diff &lt;span class="op"&gt;=&lt;/span&gt; z &lt;span class="op"&gt;-&lt;/span&gt; beta_start_&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-16" data-line-number="16"&gt;        loss_diff &lt;span class="op"&gt;=&lt;/span&gt; loss_z &lt;span class="op"&gt;-&lt;/span&gt; loss_start_&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-17" data-line-number="17"&gt;        line_diff &lt;span class="op"&gt;=&lt;/span&gt; alpha_ &lt;span class="op"&gt;*&lt;/span&gt; (loss_diff &lt;span class="op"&gt;-&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-18" data-line-number="18"&gt;loss_grad_start_.T.dot(step_diff))&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-19" data-line-number="19"&gt;        line_diff &lt;span class="op"&gt;-=&lt;/span&gt; step_diff.T.dot(step_diff) &lt;span class="op"&gt;/&lt;/span&gt; &lt;span class="fl"&gt;2.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-20" data-line-number="20"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-21" data-line-number="21"&gt;        &lt;span class="cf"&gt;if&lt;/span&gt; line_diff &lt;span class="op"&gt;&amp;lt;=&lt;/span&gt; obj_tol:&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-22" data-line-number="22"&gt;            &lt;span class="cf"&gt;return&lt;/span&gt; z, alpha_, loss_z&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-23" data-line-number="23"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-24" data-line-number="24"&gt;        alpha_ &lt;span class="op"&gt;*=&lt;/span&gt; bt_rate&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-25" data-line-number="25"&gt;        &lt;span class="cf"&gt;assert&lt;/span&gt; alpha_ &lt;span class="op"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;invalid step size: &lt;/span&gt;&lt;span class="sc"&gt;{}&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;&lt;/span&gt;.&lt;span class="bu"&gt;format&lt;/span&gt;(alpha_)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="remark" data-markdown="" data-env-number="2" data-title-name=""&gt;
&lt;p&gt;Routines like this that make use of the gradient and other quantities might also be good candidates for execution in Theano, if only because of the graph optimizations that are able to remedy obviously redundant computations.&lt;/p&gt;
&lt;p&gt;In this vein, we could consider performing the line-search, and/or the entire optimization loop, within a Theano &lt;code&gt;scan&lt;/code&gt; operation. We could also create &lt;code&gt;Op&lt;/code&gt;s that represents gradient and line-search step. These might make graph construction much simpler, and be more suited for the current optimization framework.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;scan&lt;/code&gt; and tighter Theano integration may not on average produce better results than our current use of its compiled functions, we still wish to emphasize the possibilities.&lt;/p&gt;
&lt;p&gt;Likewise, an &lt;code&gt;Op&lt;/code&gt; for the proximal operator might also be necessary for solving proximal operators automatically in closed-form (when possible) within a graph. This is based on the standard use of lookup tables combined with sets of algebraic relationships and identities used in symbolic algebra libraries for automatic differentiation and integration. The same can be done to extend the coverage of known closed-form solutions to proximal operators in an automated setting.&lt;/p&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="examples" class="level1"&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;First, we need to set up the basic functions, which–in this case–are constructed from the Theano graphs.&lt;/p&gt;
&lt;div class="sourceCode" id="cb7"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb7-1" data-line-number="1"&gt;lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-2" data-line-number="2"&gt;lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-4" data-line-number="4"&gt;prox_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt, lambda_tt],&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-5" data-line-number="5"&gt;                      tt_soft_threshold(beta_tt, lambda_tt))&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-6" data-line-number="6"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-7" data-line-number="7"&gt;prox_grad_step_fn, loss_grad &lt;span class="op"&gt;=&lt;/span&gt; prox_gradient_step(&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-8" data-line-number="8"&gt;    nlogl, beta_tt, tt_soft_threshold,&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-9" data-line-number="9"&gt;    return_loss_grad&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-11" data-line-number="11"&gt;loss_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt], nlogl)&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-12" data-line-number="12"&gt;loss_grad_fn &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_tt], loss_grad)&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-14" data-line-number="14"&gt;cols_fns &lt;span class="op"&gt;=&lt;/span&gt; [&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-15" data-line-number="15"&gt;    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: i, &lt;span class="vs"&gt;r&amp;#39;$i$&amp;#39;&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-16" data-line-number="16"&gt;    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: np.asscalar(loss_fn(b)),&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-17" data-line-number="17"&gt;        &lt;span class="vs"&gt;r&amp;#39;$l(\beta^{(i)})$&amp;#39;&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-18" data-line-number="18"&gt;    (&lt;span class="kw"&gt;lambda&lt;/span&gt; i, b: np.linalg.norm(b &lt;span class="op"&gt;-&lt;/span&gt; beta_true, &lt;span class="dv"&gt;2&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-19" data-line-number="19"&gt;        &lt;span class="vs"&gt;r&amp;#39;$\|\beta^{(i)} - \beta^*\|^2_2$&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-20" data-line-number="20"&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a baseline comparison–and sanity check–we’ll use the &lt;code&gt;cvxpy&lt;/code&gt; library &lt;span class="citation" data-cites="diamond_cvxpy:_2016"&gt;(Diamond and Boyd 2016)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode" id="cb8"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb8-1" data-line-number="1"&gt;&lt;span class="im"&gt;import&lt;/span&gt; cvxpy &lt;span class="im"&gt;as&lt;/span&gt; cvx&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-3" data-line-number="3"&gt;beta_var_cvx &lt;span class="op"&gt;=&lt;/span&gt; cvx.Variable(M, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-4" data-line-number="4"&gt;lambda_cvx &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max &lt;span class="op"&gt;*&lt;/span&gt; N&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-5" data-line-number="5"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-6" data-line-number="6"&gt;cvx_obj &lt;span class="op"&gt;=&lt;/span&gt; cvx.Minimize(&lt;span class="fl"&gt;0.5&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; cvx.sum_squares(y &lt;span class="op"&gt;-&lt;/span&gt; X &lt;span class="op"&gt;*&lt;/span&gt; beta_var_cvx)&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-7" data-line-number="7"&gt;                       &lt;span class="op"&gt;+&lt;/span&gt; lambda_cvx &lt;span class="op"&gt;*&lt;/span&gt; cvx.norm(beta_var_cvx, &lt;span class="dv"&gt;1&lt;/span&gt;) )&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-8" data-line-number="8"&gt;cvx_prob &lt;span class="op"&gt;=&lt;/span&gt; cvx.Problem(cvx_obj)&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-9" data-line-number="9"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-10" data-line-number="10"&gt;_ &lt;span class="op"&gt;=&lt;/span&gt; cvx_prob.solve(solver&lt;span class="op"&gt;=&lt;/span&gt;cvx.CVXOPT, verbose&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-11" data-line-number="11"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-12" data-line-number="12"&gt;beta_cvx &lt;span class="op"&gt;=&lt;/span&gt; np.asarray(beta_var_cvx.value).squeeze()&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-13" data-line-number="13"&gt;loss_cvx &lt;span class="op"&gt;=&lt;/span&gt; loss_fn(beta_cvx)&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-14" data-line-number="14"&gt;beta_cvx_err &lt;span class="op"&gt;=&lt;/span&gt; np.linalg.norm(beta_cvx &lt;span class="op"&gt;-&lt;/span&gt; beta_true, &lt;span class="dv"&gt;2&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We now have the necessary pieces to perform an example estimation. We’ll start with an exceedingly large step size and let backtracking line-search find a good value.&lt;/p&gt;
&lt;div class="sourceCode" id="cb9"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb9-1" data-line-number="1"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; ProxGradient(&lt;span class="bu"&gt;object&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-3" data-line-number="3"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; &lt;span class="fu"&gt;__init__&lt;/span&gt;(&lt;span class="va"&gt;self&lt;/span&gt;, y, X, beta_0,&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-4" data-line-number="4"&gt;                 prox_fn_, loss_fn_, loss_grad_fn_,&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-5" data-line-number="5"&gt;                 alpha_0):&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-6" data-line-number="6"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-7" data-line-number="7"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;=&lt;/span&gt; y&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-8" data-line-number="8"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.X &lt;span class="op"&gt;=&lt;/span&gt; X&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-9" data-line-number="9"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val &lt;span class="op"&gt;=&lt;/span&gt; alpha_0&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-10" data-line-number="10"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.beta_0 &lt;span class="op"&gt;=&lt;/span&gt; beta_0&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-11" data-line-number="11"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.N, &lt;span class="va"&gt;self&lt;/span&gt;.M &lt;span class="op"&gt;=&lt;/span&gt; X.shape&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-12" data-line-number="12"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_ &lt;span class="op"&gt;=&lt;/span&gt; prox_fn_&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-13" data-line-number="13"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.loss_fn_ &lt;span class="op"&gt;=&lt;/span&gt; loss_fn_&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-14" data-line-number="14"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.loss_grad_fn_ &lt;span class="op"&gt;=&lt;/span&gt; loss_grad_fn_&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-15" data-line-number="15"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-16" data-line-number="16"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; step(&lt;span class="va"&gt;self&lt;/span&gt;, beta):&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-17" data-line-number="17"&gt;        beta_val &lt;span class="op"&gt;=&lt;/span&gt; np.copy(beta)&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-18" data-line-number="18"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-19" data-line-number="19"&gt;        beta_val, &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val, _ &lt;span class="op"&gt;=&lt;/span&gt; backtracking_search(&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-20" data-line-number="20"&gt;            beta_val, &lt;span class="va"&gt;self&lt;/span&gt;.alpha_val,&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-21" data-line-number="21"&gt;            &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_, &lt;span class="va"&gt;self&lt;/span&gt;.loss_fn_, &lt;span class="va"&gt;self&lt;/span&gt;.loss_grad_fn_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-22" data-line-number="22"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-23" data-line-number="23"&gt;        &lt;span class="cf"&gt;return&lt;/span&gt; beta_val&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb10"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb10-1" data-line-number="1"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-2" data-line-number="2"&gt;lambda_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-3" data-line-number="3"&gt;pg_step &lt;span class="op"&gt;=&lt;/span&gt; ProxGradient(y, X, beta_0,&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-4" data-line-number="4"&gt;                       &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_val &lt;span class="op"&gt;*&lt;/span&gt; a),&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-5" data-line-number="5"&gt;                       loss_fn, loss_grad_fn, &lt;span class="dv"&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-6" data-line-number="6"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-7" data-line-number="7"&gt;pg_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns &lt;span class="op"&gt;+&lt;/span&gt; [(&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: pg_step.alpha_val,&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-8" data-line-number="8"&gt;&lt;span class="vs"&gt;r&amp;#39;$\alpha$&amp;#39;&lt;/span&gt;)]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-9" data-line-number="9"&gt;pg_est_data, _ &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(pg_step, loss_fn, pg_cols_fns)&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-10" data-line-number="10"&gt;pg_ls_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(pg_est_data)&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-11" data-line-number="11"&gt;&lt;span class="co"&gt;# pg_ls_data = pg_ls_data.append(pg_est_data, ignore_index=True)&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_ls_plot"&gt;&lt;span id="fig:pg_ls_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{1}\label{fig:pg_ls_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_ls_plot_1.png" title="fig:" alt="Minimization by proximal gradient with backtracking line-search." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;span class="math inline"&gt;\(\ref{fig:pg_ls_plot}\)&lt;/span&gt; shows a couple convergence measures for proximal gradient steps alongside the step size changes due to backtracking line-search. Regarding the latter, in our example a sufficient step size is found within the first few iterations, so the overall result isn’t too interesting. Fortunately, this sort of behaviour isn’t uncommon, which makes line-search quite effective in practice.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="coordinate-wise-estimation" class="level1"&gt;
&lt;h1&gt;Coordinate-wise Estimation&lt;/h1&gt;
&lt;p&gt;Given that our loss is a composition of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and a linear operator of finite dimension (i.e. &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;), we can conveniently exploit conditional separability and obtain simple estimation steps in each coordinate. This is, effectively, what characterizes coordinate–or cyclic–descent. Since it is a common technique in the estimation of &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; models &lt;span class="citation" data-cites="friedman_pathwise_2007 mazumder_regularization_2009 scikit-learn_sklearn.linear_model.elasticnet_2017"&gt;(Friedman et al. 2007; Mazumder, Hastie, and Tibshirani 2009; scikit-learn 2017)&lt;/span&gt;, it’s worthwhile to consider how it can viewed in terms of proximal operators.&lt;/p&gt;
&lt;p&gt;From a statistical perspective, the basics of coordinate-wise methods begin with the “partial residuals”, &lt;span class="math inline"&gt;\(r_{-m} \in {{\mathbb{R}}}^{N}\)&lt;/span&gt; discussed in &lt;span class="citation" data-cites="friedman_pathwise_2007"&gt;Friedman et al. (2007)&lt;/span&gt;, and implicitly defined by &lt;span class="math display"&gt;\[\begin{equation}
\begin{aligned}
    \beta^*
    &amp;amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|
    y - X(\beta - e_m \beta_m)
        - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
      \right\}
    \\
    &amp;amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|r_{-m} - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \dots
    \right\}
  \;.
  \end{aligned}
  \label{eq:partial_resid}
\end{equation}\]&lt;/span&gt; The last expression hints at the most basic idea behind the coordinate-wise approach: conditional minimization in each &lt;span class="math inline"&gt;\(m\)&lt;/span&gt;. Its exact solution in each coordinate is given by the aforementioned soft thresholding function, which–as we’ve already stated–is a proximal operator. In symbols, &lt;span class="math inline"&gt;\(\operatorname*{prox}_{\lambda \left|\cdot\right|}(x) = \operatorname{S}_\lambda(x)\)&lt;/span&gt;, where the latter is the soft thresholding operator.&lt;/p&gt;
&lt;p&gt;Now, if we wanted to relate Equation &lt;span class="math inline"&gt;\(\eqref{eq:partial_resid}\)&lt;/span&gt; a proximal method via the statement of a proximal gradient fixed-point solution–i.e. Equation &lt;span class="math inline"&gt;\(\eqref{eq:forward-backward}\)&lt;/span&gt;–we might use the following property of proximal operators:&lt;/p&gt;
&lt;div id="lem:prox_ortho_basis" class="lemma" data-markdown="" data-env-number="1" data-title-name=""&gt;
&lt;p&gt;&lt;span id="lem:prox_ortho_basis_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{1}\label{lem:prox_ortho_basis}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{prox}_{\lambda \phi \circ e^\top_m}(z) =
    \sum^M_m \operatorname*{prox}_{\lambda \phi}\left(e^\top_m z\right) e_m
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class="proof" data-markdown="" data-env-number="1" data-title-name=""&gt;
&lt;p&gt;See &lt;span class="citation" data-cites="chaux_variational_2007"&gt;Chaux et al. (2007)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The next result yields our desired connection.&lt;/p&gt;
&lt;div id="eq:prox_grad_descent" class="proposition" data-markdown="" data-env-number="1" data-title-name=""&gt;
&lt;p&gt;&lt;span id="eq:prox_grad_descent_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{1}\label{eq:prox_grad_descent}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; such that &lt;span class="math inline"&gt;\({{\bf 1}}^\top X e_m = 0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(e^\top_m X^\top X e_m = 1\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(m \in \{1, \dots, M\}\)&lt;/span&gt;, the coordinate-wise step of the Lasso in &lt;span class="citation" data-cites="friedman_pathwise_2007"&gt;Friedman et al. (2007 Equation (9))&lt;/span&gt;, &lt;span class="math display"&gt;\[\begin{equation*}
\beta_m = \operatorname{S}_{\lambda}\left[
      \sum_{n}^N X_{n,m} \left(
      y_n - \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime}
      \right)
    \right]
    \;,
\end{equation*}\]&lt;/span&gt; has a proximal gradient fixed-point solution under a Euclidean basis decomposition with the form &lt;span class="math display"&gt;\[\begin{equation*}
\beta =
    \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
      e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
    \right] e_m
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class="proof" data-markdown="" data-env-number="2" data-title-name=""&gt;
&lt;p&gt;We start with an expansion of the terms in &lt;span class="math inline"&gt;\(\operatorname*{prox}_{\lambda \phi} \equiv \operatorname{S}_\lambda\)&lt;/span&gt;. After simplifying the notation with &lt;span class="math display"&gt;\[\begin{equation*}
\begin{gathered}
    \sum^N_{n} X_{n,m} z_n = e^\top_m X^\top z, \quad \text{and} \quad
    \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime} =
    X \left(\beta - \beta_m e_m \right)
    \;,
  \end{gathered}
\end{equation*}\]&lt;/span&gt; the expanded argument of &lt;span class="math inline"&gt;\(\operatorname{S}\)&lt;/span&gt; reduces to &lt;span class="math display"&gt;\[\begin{equation*}
\begin{aligned}
      e^\top_m X^\top \left(y - X\left( \beta - e_m \beta_m\right)\right)
      &amp;amp;= e^\top_m X^\top X e_m \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;amp;= \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;amp;= e^\top_m \left(\beta + X^\top \left(y - X \beta\right)\right)
    \end{aligned}
\end{equation*}\]&lt;/span&gt; where the last step follows from &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; standardization. This establishes the relationship with Equation &lt;span class="math inline"&gt;\(\eqref{eq:forward-backward}\)&lt;/span&gt; only component-wise. Using Lemma &lt;span class="math inline"&gt;\(\eqref{lem:prox_ortho_basis}\)&lt;/span&gt; together with &lt;span class="math inline"&gt;\(z = \beta - \alpha \nabla  l(\beta)\)&lt;/span&gt; yields the proximal gradient fixed-point statement, i.e. &lt;span class="math display"&gt;\[\begin{equation*}
\begin{aligned}
      \beta
      &amp;amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
    e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
      \right] e_m
      \\
      &amp;amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left(
      \beta_m + \alpha e_m^\top X^\top \left(y - X \beta \right)
      \right) e_m
      \;.
    \end{aligned}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="rem:bases" class="remark" data-markdown="" data-env-number="3" data-title-name=""&gt;
&lt;p&gt;&lt;span id="rem:bases_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{3}\label{rem:bases}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The property in Lemma &lt;span class="math inline"&gt;\(\eqref{lem:prox_ortho_basis}\)&lt;/span&gt; can used with other orthonormal bases–providing yet another connection between proximal methods and established dimensionality reduction and sparse estimation techniques &lt;span class="citation" data-cites="chaux_variational_2007"&gt;(Chaux et al. 2007)&lt;/span&gt;. Also, this property provides a neat way to think about &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;-based orthogonalizations in estimations for regression and grouped-penalization problems.&lt;/p&gt;
&lt;/div&gt;
&lt;section id="implementation-1" class="level2"&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;The following performs a standard form of coordinate descent:&lt;/p&gt;
&lt;div class="sourceCode" id="cb11"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb11-1" data-line-number="1"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; CoordDescent(&lt;span class="bu"&gt;object&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-3" data-line-number="3"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; &lt;span class="fu"&gt;__init__&lt;/span&gt;(&lt;span class="va"&gt;self&lt;/span&gt;, y, X, beta_0, prox_fn_, col_seq&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-4" data-line-number="4"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-5" data-line-number="5"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;=&lt;/span&gt; y&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-6" data-line-number="6"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.X &lt;span class="op"&gt;=&lt;/span&gt; X&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-7" data-line-number="7"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.beta_0 &lt;span class="op"&gt;=&lt;/span&gt; beta_0&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-8" data-line-number="8"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.N, &lt;span class="va"&gt;self&lt;/span&gt;.M &lt;span class="op"&gt;=&lt;/span&gt; X.shape&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-9" data-line-number="9"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;=&lt;/span&gt; np.dot(&lt;span class="va"&gt;self&lt;/span&gt;.X, &lt;span class="va"&gt;self&lt;/span&gt;.beta_0)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-10" data-line-number="10"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_ &lt;span class="op"&gt;=&lt;/span&gt; prox_fn_&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-11" data-line-number="11"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-12" data-line-number="12"&gt;        &lt;span class="co"&gt;# (Inverse) 2-norm of each column/feature, i.e.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-13" data-line-number="13"&gt;        &lt;span class="co"&gt;#   np.reciprocal(np.diag(np.dot(X.T, X)))&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-14" data-line-number="14"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.alpha_vals &lt;span class="op"&gt;=&lt;/span&gt; np.reciprocal((&lt;span class="va"&gt;self&lt;/span&gt;.X&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;).&lt;span class="bu"&gt;sum&lt;/span&gt;(axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;))&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-15" data-line-number="15"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-16" data-line-number="16"&gt;        &lt;span class="cf"&gt;if&lt;/span&gt; col_seq &lt;span class="kw"&gt;is&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-17" data-line-number="17"&gt;            &lt;span class="va"&gt;self&lt;/span&gt;.col_seq &lt;span class="op"&gt;=&lt;/span&gt; np.arange(&lt;span class="va"&gt;self&lt;/span&gt;.M)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-18" data-line-number="18"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-19" data-line-number="19"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; reset(&lt;span class="va"&gt;self&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-20" data-line-number="20"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;=&lt;/span&gt; np.dot(&lt;span class="va"&gt;self&lt;/span&gt;.X, &lt;span class="va"&gt;self&lt;/span&gt;.beta_0)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-21" data-line-number="21"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-22" data-line-number="22"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; step(&lt;span class="va"&gt;self&lt;/span&gt;, beta):&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-23" data-line-number="23"&gt;        beta_val &lt;span class="op"&gt;=&lt;/span&gt; np.copy(beta)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-24" data-line-number="24"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-25" data-line-number="25"&gt;        &lt;span class="cf"&gt;for&lt;/span&gt; j &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.col_seq:&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-26" data-line-number="26"&gt;            X_j &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.X[:, j]&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-27" data-line-number="27"&gt;            alpha_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.alpha_vals[j]&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-28" data-line-number="28"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-29" data-line-number="29"&gt;            &lt;span class="co"&gt;# A little cheaper to just subtract the column&amp;#39;s&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-30" data-line-number="30"&gt;contribution...&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-31" data-line-number="31"&gt;            &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;-=&lt;/span&gt; X_j &lt;span class="op"&gt;*&lt;/span&gt; beta_val[j]&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-32" data-line-number="32"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-33" data-line-number="33"&gt;            Xt_r &lt;span class="op"&gt;=&lt;/span&gt; np.dot(X_j.T, &lt;span class="va"&gt;self&lt;/span&gt;.y &lt;span class="op"&gt;-&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.Xb) &lt;span class="op"&gt;*&lt;/span&gt; alpha_val&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-34" data-line-number="34"&gt;            beta_val[j] &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="va"&gt;self&lt;/span&gt;.prox_fn_(np.atleast_1d(Xt_r),&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-35" data-line-number="35"&gt;alpha_val)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-36" data-line-number="36"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-37" data-line-number="37"&gt;            &lt;span class="co"&gt;# ...and add the updated column back.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-38" data-line-number="38"&gt;            &lt;span class="va"&gt;self&lt;/span&gt;.Xb &lt;span class="op"&gt;+=&lt;/span&gt; X_j &lt;span class="op"&gt;*&lt;/span&gt; beta_val[j]&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-39" data-line-number="39"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-40" data-line-number="40"&gt;        &lt;span class="va"&gt;self&lt;/span&gt;.beta_last &lt;span class="op"&gt;=&lt;/span&gt; beta_val&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-41" data-line-number="41"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-42" data-line-number="42"&gt;        &lt;span class="cf"&gt;return&lt;/span&gt; beta_val&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our example randomizes the order of coordinates to loosely demonstrate the range of efficiency possible in coordinate descent.&lt;/p&gt;
&lt;div class="sourceCode" id="cb12"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb12-1" data-line-number="1"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-2" data-line-number="2"&gt;lambda_val &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e-2&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt; lambda_max&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-3" data-line-number="3"&gt;cd_step &lt;span class="op"&gt;=&lt;/span&gt; CoordDescent(y, X, beta_0,&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-4" data-line-number="4"&gt;                       &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_val &lt;span class="op"&gt;*&lt;/span&gt; a))&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-5" data-line-number="5"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-6" data-line-number="6"&gt;cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns &lt;span class="op"&gt;+&lt;/span&gt; [(&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: j, &lt;span class="st"&gt;&amp;quot;replication&amp;quot;&lt;/span&gt;)]&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-7" data-line-number="7"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-8" data-line-number="8"&gt;pg_coord_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame()&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-9" data-line-number="9"&gt;&lt;span class="cf"&gt;for&lt;/span&gt; j &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;range&lt;/span&gt;(&lt;span class="dv"&gt;15&lt;/span&gt;):&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-10" data-line-number="10"&gt;    est_data, _ &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(cd_step, loss_fn, cd_cols_fns)&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-11" data-line-number="11"&gt;    pg_coord_data &lt;span class="op"&gt;=&lt;/span&gt; pg_coord_data.append(est_data,&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-12" data-line-number="12"&gt;                                         ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-13" data-line-number="13"&gt;    &lt;span class="co"&gt;# Reset internal state of our step method, since we&amp;#39;re&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-14" data-line-number="14"&gt;    &lt;span class="co"&gt;# running multiple replications.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-15" data-line-number="15"&gt;    cd_step.reset()&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-16" data-line-number="16"&gt;    np.random.shuffle(cd_step.col_seq)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_coord_plot"&gt;&lt;span id="fig:pg_coord_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{2}\label{fig:pg_coord_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_coord_plot_1.png" title="fig:" alt="Minimization by coordinate descent." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;span class="math inline"&gt;\(\ref{fig:pg_coord_plot}\)&lt;/span&gt; shows convergence measures for each randomized coordinate order. The [average] difference in the number of iterations required for coordinate descent and proximal gradient is fairly noticeable. Nonetheless, both reach effectively the same limits.&lt;/p&gt;
&lt;div class="remark" data-markdown="" data-env-number="4" data-title-name=""&gt;
&lt;p&gt;Similar ideas behind batched vs. non-batched steps and block sampling–found within the Gibbs sampling literature &lt;span class="citation" data-cites="roberts_updating_1997"&gt;(Roberts and Sahu 1997)&lt;/span&gt;–could explain the variation due to coordinate order and the relative efficiency of coordinate descent. There are also connections with our comments in Remark &lt;span class="math inline"&gt;\(\ref{rem:bases}\)&lt;/span&gt; and, to some extent, stochastic gradient descent (SGD) &lt;span class="citation" data-cites="bertsekas_incremental_2010"&gt;(Bertsekas 2010)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a woefully lacking over-generalization, let’s say that it comes down to the [spectral] properties of the composite operator(s) &lt;span class="math inline"&gt;\(l \circ X\)&lt;/span&gt; and/or &lt;span class="math inline"&gt;\(\nabla l \circ X\)&lt;/span&gt;. These determine the bounds of efficiency for steps in certain directions and how blocking or partitioning the dimensions of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; nears or distances from those bounds.&lt;/p&gt;
&lt;/div&gt;
&lt;section id="regularization-paths" class="level3"&gt;
&lt;h3&gt;Regularization Paths&lt;/h3&gt;
&lt;p&gt;Also, due to the relatively fast convergence of coordinate descent, the method is a little more suitable for the computation of regularization paths– i.e. varying &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; between iterations. There is much more to this topic, but for simplicity let’s just note that each &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; step has a “warm-start” from the previous descent iteration–which helps–and that we’re otherwise fine with the solution provided by this approach.&lt;/p&gt;
&lt;p&gt;Next, we make a small extension to demonstrate the computation of regularization paths–using &lt;code&gt;lasso_path&lt;/code&gt; for comparison.&lt;/p&gt;
&lt;div class="sourceCode" id="cb13"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb13-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; sklearn.linear_model &lt;span class="im"&gt;import&lt;/span&gt; lasso_path, enet_path&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-3" data-line-number="3"&gt;beta_0 &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(M).astype(&lt;span class="st"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-4" data-line-number="4"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-5" data-line-number="5"&gt;lambda_path, beta_path, _ &lt;span class="op"&gt;=&lt;/span&gt; lasso_path(X, y)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-6" data-line-number="6"&gt;path_len &lt;span class="op"&gt;=&lt;/span&gt; np.alen(lambda_path)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-7" data-line-number="7"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-8" data-line-number="8"&gt;beta_last &lt;span class="op"&gt;=&lt;/span&gt; beta_0&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-9" data-line-number="9"&gt;pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame()&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-10" data-line-number="10"&gt;&lt;span class="cf"&gt;for&lt;/span&gt; i, lambda_ &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;enumerate&lt;/span&gt;(lambda_path):&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-11" data-line-number="11"&gt;    cd_path_step &lt;span class="op"&gt;=&lt;/span&gt; CoordDescent(y, X, beta_last,&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-12" data-line-number="12"&gt;                        &lt;span class="kw"&gt;lambda&lt;/span&gt; x, a: prox_fn(x, N &lt;span class="op"&gt;*&lt;/span&gt; lambda_ &lt;span class="op"&gt;*&lt;/span&gt; a))&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-14" data-line-number="14"&gt;    cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns[&lt;span class="dv"&gt;1&lt;/span&gt;:] &lt;span class="op"&gt;+&lt;/span&gt; [&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-15" data-line-number="15"&gt;        (&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: lambda_, &lt;span class="vs"&gt;r&amp;#39;$\lambda$&amp;#39;&lt;/span&gt;)]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-16" data-line-number="16"&gt;    est_data, beta_last &lt;span class="op"&gt;=&lt;/span&gt; iterative_run(cd_path_step, loss_fn,&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-17" data-line-number="17"&gt;                                        cd_cols_fns,&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-18" data-line-number="18"&gt;                                        stop_tol&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e-4&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-19" data-line-number="19"&gt;                                        stop_loss&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-20" data-line-number="20"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-21" data-line-number="21"&gt;    pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.append(est_data.iloc[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, :],&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-22" data-line-number="22"&gt;                                       ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb14"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb14-1" data-line-number="1"&gt;cd_cols_fns &lt;span class="op"&gt;=&lt;/span&gt; cols_fns[&lt;span class="dv"&gt;1&lt;/span&gt;:] &lt;span class="op"&gt;+&lt;/span&gt; [&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-2" data-line-number="2"&gt;    (&lt;span class="kw"&gt;lambda&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;args, &lt;span class="op"&gt;**&lt;/span&gt;kwargs: lambda_path[args[&lt;span class="dv"&gt;0&lt;/span&gt;]], &lt;span class="vs"&gt;r&amp;#39;$\lambda$&amp;#39;&lt;/span&gt;)]&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-4" data-line-number="4"&gt;iter_values &lt;span class="op"&gt;=&lt;/span&gt; []&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-5" data-line-number="5"&gt;&lt;span class="cf"&gt;for&lt;/span&gt; i, beta_ &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;enumerate&lt;/span&gt;(beta_path.T):&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-6" data-line-number="6"&gt;    iter_values.append([col_fn(i, beta_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-7" data-line-number="7"&gt;                        &lt;span class="cf"&gt;for&lt;/span&gt; col_fn, _ &lt;span class="kw"&gt;in&lt;/span&gt; cd_cols_fns])&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-8" data-line-number="8"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-9" data-line-number="9"&gt;sklearn_path_data &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(iter_values,&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-10" data-line-number="10"&gt;                                 columns&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="bu"&gt;zip&lt;/span&gt;(&lt;span class="op"&gt;*&lt;/span&gt;cd_cols_fns)[&lt;span class="dv"&gt;1&lt;/span&gt;])&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-11" data-line-number="11"&gt;sklearn_path_data &lt;span class="op"&gt;=&lt;/span&gt; sklearn_path_data.assign(&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-12" data-line-number="12"&gt;    replication&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;, &lt;span class="bu"&gt;type&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;sklearn&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-14" data-line-number="14"&gt;pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.assign(&lt;span class="bu"&gt;type&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;pg&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-15" data-line-number="15"&gt;pg_path_data &lt;span class="op"&gt;=&lt;/span&gt; pg_path_data.append(sklearn_path_data,&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-16" data-line-number="16"&gt;                                   ignore_index&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span id="fig:pg_path_plot"&gt;&lt;span id="fig:pg_path_plot_span" style="display:none;visibility:hidden"&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{3}\label{fig:pg_path_plot}\end{equation}\]&lt;/span&gt;&lt;/span&gt;&lt;img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_path_plot_1.png" title="fig:" alt="Regularization paths via coordinate descent." /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Among the changes discussed earlier regarding Theano &lt;code&gt;Op&lt;/code&gt;s for the proximal objects used here, we would also like to motivate much larger changes to the applied mathematician/statistician’s standard tools by demonstrating the relevance of less common–yet increasingly useful–abstractions. For instance, the proximal methods are neatly framed within operator theory and set-valued analysis, where concepts like the resolvent, sub-differential/gradient and others are common. Abstractions like these provide a compact means of extending familiar ideas into new contexts–such as non-differentiable functions.&lt;/p&gt;
&lt;p&gt;Unfortunately, our numerical libraries do not provide much in the way utilizing these abstractions. Most are strictly founded in the representation of point-valued mappings, which can require significant work-arounds to handle even the most common non-differentiable functions (e.g. the absolute value within our example problem). Our use of the proximal framework is, in part, motivated by its near seamless use &lt;em&gt;and&lt;/em&gt; simultaneous bypassing of set-valued maps–in implementation, at least.&lt;/p&gt;
&lt;p&gt;There is no fundamental restriction blocking support for set-valued maps, however–aside from the necessary labor and community interest. Even minimal support could provide a context that makes frameworks like ours merely minor abstractions. A similar idea can be found in the symbolic calculation of limits via filters &lt;span class="citation" data-cites="beeson_meaning_2005"&gt;(Beeson and Wiedijk 2005)&lt;/span&gt;. Perhaps we can liken these changes to the modern evolution of linear algebra libraries to tensor libraries.&lt;/p&gt;
&lt;p&gt;We would also like to stress that the value provided by the symbolic tools discussed here (Theano, really) are not &lt;em&gt;just&lt;/em&gt; in their ability to act as compilers at a “math level”, but more for their ability to concretely encode mathematical characterizations of optimization problems and methods. Work in this direction is not new by any means; however, the combination of open-source tools and industry interest in algorithms that fall under the broad class of proximal methods (e.g. gradient descent, ADMM, EM, etc.) provides a more immediate reason to pursue these abstractions in code and automate their use.&lt;/p&gt;
&lt;p&gt;Regarding the proximal methods, we can consider Theano optimizations that make direct use of the orthonormal basis property in Lemma &lt;span class="math inline"&gt;\(\eqref{lem:prox_ortho_basis}\)&lt;/span&gt;, or the Moreau-Fenchel theorem, and automate consideration for various estimation methods via splitting (e.g. ADMM, Douglas-Rachford, etc.)–perhaps by making decisions based on inferred or specified tensor, function, and operator properties. In future installments we’ll delve into the details of these ideas.&lt;/p&gt;
&lt;p&gt;&lt;span class="citation" data-cites="wytock_new_2016"&gt;(Wytock et al. 2016)&lt;/span&gt; also discuss similar ideas in an optimization setting, such as the use of symbolic graphs and a close coupling with useful mathematical abstractions–including proximal operators. Additionally, there are many other good examples &lt;span class="citation" data-cites="diamond_cvxpy:_2016"&gt;(Diamond and Boyd 2016)&lt;/span&gt; of constructive mathematical abstractions applied in code.&lt;/p&gt;
&lt;p&gt;In most cases, libraries providing optimization tools and supporting model estimation do not attempt to root their implementations within an independently developed symbolic framework and then realize their relevant methodologies in that context. Too often the mathematical abstractions–or the resulting methods alone–are directly implemented at the highest levels of abstraction possible. This is what we see as the result of popular libraries like &lt;code&gt;scikit-learn&lt;/code&gt; and the body of &lt;code&gt;R&lt;/code&gt; packages. One can also find the same efforts for proximal methods themselves–e.g. in &lt;span class="citation" data-cites="svaiter_pyprox_2017"&gt;(svaiter 2017)&lt;/span&gt;, where individual functions for ADMM, forward-backward/proximal gradient and Douglas-Rachford are the end result. This is the most common approach and it makes sense in terms of simplicity, but offers very little of the extensibility, generalization, or efficiencies provided by shared efforts across related projects and fields.&lt;/p&gt;
&lt;p&gt;In the context of Theano, implementations immediately benefit from its code conversion, parallelization and relevant improvements to its basic graph optimizations. The latter covers both low-level computational efficiency–such as relevant application of BLAS functions–and high-level tensor algebra simplifications.&lt;/p&gt;
&lt;p&gt;In a development community that builds on these tools, related efficiency and performance gains can occur much more often, without necessarily sacrificing the specificity inherent to certain areas of application. For example, we can safely use the Rao-Blackwell theorem as the basis of a graph optimization in PyMC3, so it could be included among that project’s default offerings; however, it would be far too cumbersome to use productively in a less specific context.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-beck_fast_2014"&gt;
&lt;p&gt;Beck, Amir, and Marc Teboulle. 2014. “A Fast Dual Proximal Gradient Algorithm for Convex Minimization and Applications.” &lt;em&gt;Operations Research Letters&lt;/em&gt; 42 (1): 1–6. &lt;a href="http://www.sciencedirect.com/science/article/pii/S0167637713001454" class="uri"&gt;http://www.sciencedirect.com/science/article/pii/S0167637713001454&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-beeson_meaning_2005"&gt;
&lt;p&gt;Beeson, Michael, and Freek Wiedijk. 2005. “The Meaning of Infinity in Calculus and Computer Algebra Systems.” &lt;em&gt;Journal of Symbolic Computation&lt;/em&gt;, Automated reasoning and computer algebra systems (ar-ca)AR-ca, 39 (5): 523–38. &lt;a href="https://www.sciencedirect.com/science/article/pii/S074771710500026X" class="uri"&gt;https://www.sciencedirect.com/science/article/pii/S074771710500026X&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-bergstra_theano_2010"&gt;
&lt;p&gt;Bergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. “Theano: A CPU and GPU Math Expression Compiler.” In &lt;em&gt;Proceedings of the Python for Scientific Computing Conference (SciPy)&lt;/em&gt;. Austin, TX.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-bertsekas_incremental_2010"&gt;
&lt;p&gt;Bertsekas, Dimitri P. 2010. “Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey.” &lt;a href="http://web.mit.edu/dimitrib/www/Incremental_Survey_LIDS.pdf" class="uri"&gt;http://web.mit.edu/dimitrib/www/Incremental_Survey_LIDS.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-chaux_variational_2007"&gt;
&lt;p&gt;Chaux, Caroline, Patrick L Combettes, Jean-Christophe Pesquet, and Valérie R Wajs. 2007. “A Variational Formulation for Frame-Based Inverse Problems.” &lt;em&gt;Inverse Problems&lt;/em&gt; 23 (4): 1495.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-combettes_proximal_2011"&gt;
&lt;p&gt;Combettes, Patrick L, and Jean-Christophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” &lt;em&gt;Fixed-Point Algorithms for Inverse Problems in Science and Engineering&lt;/em&gt;, 185–212.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-diamond_cvxpy:_2016"&gt;
&lt;p&gt;Diamond, Steven, and Stephen Boyd. 2016. “CVXPY: A Python-Embedded Modeling Language for Convex Optimization.” &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 17 (83): 1–5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-friedman_pathwise_2007"&gt;
&lt;p&gt;Friedman, Jerome, Trevor Hastie, Holger Höfling, Robert Tibshirani, and others. 2007. “Pathwise Coordinate Optimization.” &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 1 (2): 302–32. &lt;a href="http://projecteuclid.org/euclid.aoas/1196438020" class="uri"&gt;http://projecteuclid.org/euclid.aoas/1196438020&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mazumder_regularization_2009"&gt;
&lt;p&gt;Mazumder, Rahul, Trevor Hastie, and Rob Tibshirani. 2009. “Regularization Methods for Learning Incomplete Matrices.” &lt;em&gt;arXiv Preprint arXiv:0906.2034&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/0906.2034" class="uri"&gt;https://arxiv.org/abs/0906.2034&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-parikh_proximal_2014"&gt;
&lt;p&gt;Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” &lt;em&gt;Foundations and Trends in Optimization&lt;/em&gt; 1 (3): 123–231. &lt;a href="https://doi.org/10.1561/2400000003" class="uri"&gt;https://doi.org/10.1561/2400000003&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. &lt;a href="http://projecteuclid.org/euclid.ss/1449670858" class="uri"&gt;http://projecteuclid.org/euclid.ss/1449670858&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-roberts_updating_1997"&gt;
&lt;p&gt;Roberts, Gareth O., and Sujit K. Sahu. 1997. “Updating Schemes, Correlation Structure, Blocking and Parameterization for the Gibbs Sampler.” &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 59 (2): 291–317. &lt;a href="http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00070/abstract" class="uri"&gt;http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00070/abstract&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-salvatier_probabilistic_2016"&gt;
&lt;p&gt;Salvatier, John, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using PyMC3.” &lt;em&gt;PeerJ Computer Science&lt;/em&gt; 2 (April): e55. &lt;a href="https://peerj.com/articles/cs-55" class="uri"&gt;https://peerj.com/articles/cs-55&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-scikit-learn_sklearn.linear_model.elasticnet_2017"&gt;
&lt;p&gt;scikit-learn. 2017. “Sklearn.Linear_model.ElasticNet Scikit-Learn 0.19.Dev0 Documentation.” &lt;a href="http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet" class="uri"&gt;http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-svaiter_pyprox_2017"&gt;
&lt;p&gt;svaiter. 2017. “Pyprox.” &lt;a href="https://github.com/svaiter/pyprox" class="uri"&gt;https://github.com/svaiter/pyprox&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-willard_role_2017"&gt;
&lt;p&gt;Willard, Brandon T. 2017. “A Role for Symbolic Computation in the General Estimation of Statistical Models.” &lt;a href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html" class="uri"&gt;https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-wytock_new_2016"&gt;
&lt;p&gt;Wytock, Matt, Steven Diamond, Felix Heide, and Stephen Boyd. 2016. “A New Architecture for Optimization Modeling Frameworks.” &lt;em&gt;arXiv Preprint arXiv:1609.03488&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/1609.03488" class="uri"&gt;https://arxiv.org/abs/1609.03488&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</content></entry><entry><title>A Role for Symbolic Computation in the General Estimation of Statistical Models</title><link href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html" rel="alternate"></link><published>2017-01-18T00:00:00-06:00</published><updated>2017-01-18T00:00:00-06:00</updated><author><name>Brandon T. Willard</name></author><id>tag:brandonwillard.github.io,2017-01-18:/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</id><summary type="html"></summary><content type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  &lt;/style&gt;
  &lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–01–18&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this document we describe how symbolic computation can be used to provide generalizable statistical estimation through a combination of existing open source frameworks. Specifically, we will show how symbolic tools can be used to address the estimation of non-smooth functions that appear in models with parameter regularization, shrinkage and sparsity. We employ a mathematical framework that makes extensive use of &lt;em&gt;proximal operators&lt;/em&gt; &lt;span class="citation" data-cites="parikh_proximal_2014 combettes_proximal_2011"&gt;(Parikh and Boyd 2014; Combettes and Pesquet 2011)&lt;/span&gt; and their properties for maximum a posteriori (MAP) estimation: i.e. the &lt;em&gt;proximal framework&lt;/em&gt;. This framework produces what we’ll call &lt;em&gt;proximal methods&lt;/em&gt; and their implementations as &lt;em&gt;proximal algorithms&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;span class="citation" data-cites="polson_proximal_2015"&gt;Polson, Scott, and Willard (2015)&lt;/span&gt; we outlined a set of seemingly disparate optimization techniques within the fields of statistics, computer vision, and machine learning (e.g. gradient descent, ADMM, EM, Douglas-Rachford) that are unified by their various applications of proximal methods. These methods–and the concepts behind them–have found much success in recent times and admit quite a few interesting paths for research. In other words, there are many reasons to alone discuss the implementation of proximal methods.&lt;/p&gt;
&lt;p&gt;Proximal operators also enjoy a breadth of closed-form solutions and useful properties that are amenable to symbolic computation. In more than a few cases, the work required to produce a proximal algorithm overlaps with well-established features of computer algebra systems and symbolic mathematics, such as symbolic differentiation and algebraic equation solving.&lt;/p&gt;
&lt;p&gt;Symbolic integration provides an excellent example of how proximal operators could be implemented in a symbolic system. In these systems, mappings between functions (as canonicalized graphs) and their generalized hypergeometric equivalents are used to exploit the latter’s relevant convolution identities. In the same vein, it is possible to use tables of closed-form proximal operators and their properties to produce a wide array of estimation algorithms for many non-smooth functions. We outline how this might be done in the following sections.&lt;/p&gt;
&lt;p&gt;Otherwise, the ideas discussed here are part of a never-ending attempt to answer a question that arises naturally in both mathematics and programming–at all levels: &lt;em&gt;How does one provide a means of generating robust solutions to as many problems as possible?&lt;/em&gt; Instead of the common efforts to independently implement each model, method and/or combination of the two–followed by their placement in an API or library of functions–implementations can be encoded in and organized by the very mathematics from which they were derived. This close coupling between mathematical principles and their implementations might be the only reasonable way to remove barriers between theory, research and practice.&lt;/p&gt;
&lt;section id="a-context" class="level2"&gt;
&lt;h2&gt;A Context&lt;/h2&gt;
&lt;p&gt;Much recent work in statistical modeling and estimation has had the goal of producing sparse results and/or efficient, near automatic model selection. This objective is shared with other related practices–such as Deep Learning and Compressed Sensing. In the former case, we can point to Dropout &lt;span class="citation" data-cites="srivastava_dropout_2014"&gt;(Srivastava et al. 2014)&lt;/span&gt; and–in the latter–&lt;span class="math inline"&gt;\(\ell_p\)&lt;/span&gt; regularization &lt;span class="citation" data-cites="donoho_compressed_2006"&gt;(Donoho 2006)&lt;/span&gt; as basic examples.&lt;/p&gt;
&lt;p&gt;Here we’ll simply assume that a practitioner intends to produce sparse estimates using the well-known LASSO–or &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; penalty.&lt;/p&gt;
&lt;p&gt;In PyMC3 &lt;span class="citation" data-cites="salvatier_probabilistic_2016"&gt;(Salvatier, Wiecki, and Fonnesbeck 2016)&lt;/span&gt;, the Bayes version of LASSO &lt;span class="citation" data-cites="park_bayesian_2008"&gt;(Park and Casella 2008)&lt;/span&gt; is easily specified.&lt;/p&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb1-1" data-line-number="1"&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-2" data-line-number="2"&gt;&lt;span class="im"&gt;import&lt;/span&gt; scipy &lt;span class="im"&gt;as&lt;/span&gt; sc&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-4" data-line-number="4"&gt;&lt;span class="im"&gt;import&lt;/span&gt; pymc3 &lt;span class="im"&gt;as&lt;/span&gt; pm&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-5" data-line-number="5"&gt;&lt;span class="im"&gt;import&lt;/span&gt; theano&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-6" data-line-number="6"&gt;&lt;span class="im"&gt;import&lt;/span&gt; theano.tensor &lt;span class="im"&gt;as&lt;/span&gt; tt&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-7" data-line-number="7"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-8" data-line-number="8"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-9" data-line-number="9"&gt;theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;FAST_COMPILE&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-11" data-line-number="11"&gt;mu_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(&lt;span class="dv"&gt;100&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-12" data-line-number="12"&gt;mu_true[:&lt;span class="dv"&gt;20&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(&lt;span class="dv"&gt;20&lt;/span&gt;)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-14" data-line-number="14"&gt;X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(&lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(mu_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.7&lt;/span&gt;), np.alen(mu_true))&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-15" data-line-number="15"&gt;y &lt;span class="op"&gt;=&lt;/span&gt; sc.stats.norm.rvs(loc&lt;span class="op"&gt;=&lt;/span&gt;X.dot(mu_true), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-16" data-line-number="16"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-17" data-line-number="17"&gt;X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-18" data-line-number="18"&gt;y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-19" data-line-number="19"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-20" data-line-number="20"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-21" data-line-number="21"&gt;    &lt;span class="co"&gt;# Would be nice if we could pass the symbolic y_tt.shape, so&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-22" data-line-number="22"&gt;    &lt;span class="co"&gt;# that our model would automatically conform to changes in&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-23" data-line-number="23"&gt;    &lt;span class="co"&gt;# the shared variables X_tt.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-24" data-line-number="24"&gt;    &lt;span class="co"&gt;# See https://github.com/pymc-devs/pymc3/pull/1125&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-25" data-line-number="25"&gt;    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-26" data-line-number="26"&gt;    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb1-27" data-line-number="27"&gt;                     shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, the negative total log likelihood in our example has a non-smooth &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; term. Keeping this in mind, let’s say we wanted to produce a MAP estimate using PyMC3. A function is already provided for this task: &lt;code&gt;find_MAP&lt;/code&gt;.&lt;/p&gt;
&lt;div class="sourceCode" id="cb2"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb2-1" data-line-number="1"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; lasso_model:&lt;/a&gt;
&lt;a class="sourceLine" id="cb2-2" data-line-number="2"&gt;    params_0 &lt;span class="op"&gt;=&lt;/span&gt; pm.find_MAP(&lt;span class="bu"&gt;vars&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;[beta_rv])&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In our run of the above, an exception was thrown due to &lt;code&gt;nan&lt;/code&gt; values within the gradient evaluation. We can inspect the gradient at &lt;span class="math inline"&gt;\(\beta = 0, 1\)&lt;/span&gt; and reproduce the result.&lt;/p&gt;
&lt;div class="sourceCode" id="cb3"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb3-1" data-line-number="1"&gt;start &lt;span class="op"&gt;=&lt;/span&gt; pm.Point({&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;: np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])}, model&lt;span class="op"&gt;=&lt;/span&gt;lasso_model)&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-2" data-line-number="2"&gt;bij &lt;span class="op"&gt;=&lt;/span&gt; pm.DictToArrayBijection(pm.ArrayOrdering(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;),&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-3" data-line-number="3"&gt;start)&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-4" data-line-number="4"&gt;logp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastlogp)&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-5" data-line-number="5"&gt;dlogp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastdlogp(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;))&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-6" data-line-number="6"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-7" data-line-number="7"&gt;&lt;span class="co"&gt;# Could also inspect the log likelihood of the prior:&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-8" data-line-number="8"&gt;&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-9" data-line-number="9"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-10" data-line-number="10"&gt;grad_at_0 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.zeros_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))&lt;/a&gt;
&lt;a class="sourceLine" id="cb3-11" data-line-number="11"&gt;grad_at_1 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.ones_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb4"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb4-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_0)))&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-2" data-line-number="2"&gt;&lt;span class="dv"&gt;100&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-3" data-line-number="3"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_1)))&lt;/a&gt;
&lt;a class="sourceLine" id="cb4-4" data-line-number="4"&gt;&lt;span class="dv"&gt;0&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The s are not due to any short-coming of PyMC3; they only demonstrate a suitable place for our ideas and improvements. Additionally, by working within PyMC3, we can readily apply certain mathematical results. For instance, theorems that apply only to distributions. This idea is more relevant to the graph optimizations we consider later, but is still very important.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="the-proximal-context" class="level1"&gt;
&lt;h1&gt;The Proximal Context&lt;/h1&gt;
&lt;p&gt;We start with the essential ingredient: the proximal operator.&lt;/p&gt;
&lt;div class="Def" data-markdown="" data-env-number="1" data-title-name="[Proximal Operator]"&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\operatorname*{prox}_{\phi}(x) =
    \operatorname*{argmin}_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As we mentioned earlier, the proximal operator is the main tool of proximal algorithms. Exact solutions to proximal operators exist for many &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, and, since they’re often quite simple in form, their computation is relatively cheap: a property that the proximal methods themselves can inherit.&lt;/p&gt;
&lt;p&gt;Consider the MAP estimation of a penalized likelihood, i.e. &lt;span class="math display"&gt;\[\begin{equation}
\beta^* = \operatorname*{argmin}_\beta \left\{ l(\beta) + \gamma \phi(\beta) \right\}
  \;,
  \label{eq:prox_problem}
\end{equation}\]&lt;/span&gt; where functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are commonly referred to as likelihood and prior terms (or loss and penalty), respectively. The proximal framework usually assumes &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are at least lower semi-continuous and convex–although quite a few useful results still hold for non-convex functions.&lt;/p&gt;
&lt;p&gt;Notice that Equation &lt;span class="math inline"&gt;\(\eqref{eq:prox_problem}\)&lt;/span&gt; takes the form of a proximal operator when &lt;span class="math inline"&gt;\(l(\beta) = \frac{1}{2} (y - \beta)^2\)&lt;/span&gt;. Otherwise, in regression problems, we have &lt;span class="math inline"&gt;\(l(\beta) = \frac{1}{2} \|y - X \beta\|^2\)&lt;/span&gt;. In this case, properties of the proximal operator can be used to produce independent proximal operators in each dimension of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;. Since more than one property of the proximal operator can accomplish this–and result in distinct approaches–one might begin to see here a reason for the breadth of proximal methods.&lt;/p&gt;
&lt;p&gt;The proximal operator relevant to our example, &lt;span class="math inline"&gt;\(\operatorname*{prox}_{|\cdot|}\)&lt;/span&gt;, is equivalent to the soft-thresholding operator. Its implementation in Theano is somewhat trivial, but–for the sake of exposition–we provide an example.&lt;/p&gt;
&lt;div class="sourceCode" id="cb5"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb5-1" data-line-number="1"&gt;beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-2" data-line-number="2"&gt;beta_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.r_[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;, &lt;span class="dv"&gt;-1&lt;/span&gt;, &lt;span class="fl"&gt;-0.2&lt;/span&gt;, &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;1&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-3" data-line-number="3"&gt;&lt;span class="dv"&gt;10&lt;/span&gt;].astype(tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-4" data-line-number="4"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-5" data-line-number="5"&gt;lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-6" data-line-number="6"&gt;lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array(&lt;span class="fl"&gt;0.5&lt;/span&gt;).astype(tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-7" data-line-number="7"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-8" data-line-number="8"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; soft_threshold(beta_, lambda_):&lt;/a&gt;
&lt;a class="sourceLine" id="cb5-9" data-line-number="9"&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb6"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb6-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(soft_threshold(beta_tt, lambda_tt).tag.test_value)&lt;/a&gt;
&lt;a class="sourceLine" id="cb6-2" data-line-number="2"&gt;[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;9.5&lt;/span&gt; &lt;span class="fl"&gt;-0.5&lt;/span&gt; &lt;span class="fl"&gt;-0.&lt;/span&gt;   &lt;span class="fl"&gt;0.&lt;/span&gt;   &lt;span class="fl"&gt;0.&lt;/span&gt;   &lt;span class="fl"&gt;0.5&lt;/span&gt;  &lt;span class="fl"&gt;9.5&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Proximal operators can be composed with a gradient step to produce the &lt;em&gt;proximal gradient&lt;/em&gt; algorithm: &lt;span class="math display"&gt;\[\begin{equation}
\beta = \operatorname*{prox}_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Besides the proximal operator for &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, steps in the proximal gradient algorithm are very straightforward and require only the gradient of &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt;. This is where a tangible benefit of symbolic computation becomes apparent: &lt;span class="math inline"&gt;\(\nabla l(\beta)\)&lt;/span&gt; can be computed automatically and efficiently. With [backtracking] line search to handle unknown step sizes, &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt;, the proximal gradient algorithm provides a surprisingly general means of sparse estimation.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-symbolic-operations" class="level1"&gt;
&lt;h1&gt;The Symbolic Operations&lt;/h1&gt;
&lt;p&gt;In order to identify a relevant, non-smooth problem, check that a given proximal method’s conditions are satisfied (e.g. convexity), and potentially solve the resulting proximal operators in closed-form, we need to obtain expressions for &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, we’re able to tease apart &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; using only the interface provided by PyMC3. Specifically, the &lt;em&gt;observed&lt;/em&gt; and &lt;em&gt;unobserved&lt;/em&gt; random variable fields in PyMC3 models.&lt;/p&gt;
&lt;div class="sourceCode" id="cb7"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb7-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-2" data-line-number="2"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-3" data-line-number="3"&gt;logl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt,&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-4" data-line-number="4"&gt;                {beta_rv: beta_tt})&lt;/a&gt;
&lt;a class="sourceLine" id="cb7-5" data-line-number="5"&gt;logl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl&amp;quot;&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, let’s assume we’re extending &lt;code&gt;find_MAP&lt;/code&gt; with even more generality, so that we can’t determine &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; in this way. This situation can occur when a user specifies custom distributions or potential functions. Regardless, we need to operate at a more symbolic level.&lt;/p&gt;
&lt;div class="remark" data-markdown="" data-env-number="1" data-title-name=""&gt;
&lt;p&gt;At this point, it is extremely worthwhile to browse the &lt;a href="http://deeplearning.net/software/theano/extending/graphstructures.html"&gt;Theano documentation&lt;/a&gt; regarding graphs and their constituent objects.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The total log-likelihood is a good place to start. Let’s look at the symbolic graph for the log-likelihood of our model.&lt;/p&gt;
&lt;div class="sourceCode" id="cb8"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb8-1" data-line-number="1"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pp &lt;span class="im"&gt;as&lt;/span&gt; tt_pp&lt;/a&gt;
&lt;a class="sourceLine" id="cb8-2" data-line-number="2"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pprint &lt;span class="im"&gt;as&lt;/span&gt; tt_pprint&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb9"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb9-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(tt_pp(lasso_model.logpt))&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-2" data-line-number="2"&gt;(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(((&lt;span class="op"&gt;-&lt;/span&gt;log(TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-3" data-line-number="3"&gt;&lt;span class="op"&gt;-&lt;/span&gt; (&lt;span class="op"&gt;|&lt;/span&gt;(&lt;span class="op"&gt;\&lt;/span&gt;beta &lt;span class="op"&gt;-&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;})&lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;})))) &lt;span class="op"&gt;+&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-4" data-line-number="4"&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(switch(TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;},&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-5" data-line-number="5"&gt;(((TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} &lt;span class="op"&gt;*&lt;/span&gt; ((y &lt;span class="op"&gt;-&lt;/span&gt; (X &lt;span class="op"&gt;\&lt;/span&gt;dot &lt;span class="op"&gt;\&lt;/span&gt;beta)) &lt;span class="op"&gt;**&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-6" data-line-number="6"&gt;&lt;span class="op"&gt;+&lt;/span&gt; log(TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;})) &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;}),&lt;/a&gt;
&lt;a class="sourceLine" id="cb9-7" data-line-number="7"&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf}))))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#pretty-printing"&gt;pretty printed&lt;/a&gt; Theano graph tells us–among other things–that we indeed have a sum of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; terms, although they are found among other confusing results (such as a &lt;code&gt;switch&lt;/code&gt; statement).&lt;/p&gt;
&lt;p&gt;As with most graphs produced by symbolic algebra systems, we need to understand how operations and objects are expressed in a graph and exactly which ones are relevant to us. After doing so, we can develop a means of finding what we want. The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#debug-print"&gt;debug printout&lt;/a&gt; is often a better visual summary of graphs, since it expresses branches clearly.&lt;/p&gt;
&lt;div class="sourceCode" id="cb10"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb10-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(lasso_model.logpt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-2" data-line-number="2"&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-3" data-line-number="3"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-4" data-line-number="4"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-5" data-line-number="5"&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-6" data-line-number="6"&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-7" data-line-number="7"&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-8" data-line-number="8"&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-9" data-line-number="9"&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-10" data-line-number="10"&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-11" data-line-number="11"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-12" data-line-number="12"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-13" data-line-number="13"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-14" data-line-number="14"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-15" data-line-number="15"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; N]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-16" data-line-number="16"&gt; &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-17" data-line-number="17"&gt; &lt;span class="op"&gt;|&lt;/span&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-18" data-line-number="18"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-19" data-line-number="19"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; R] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-20" data-line-number="20"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-21" data-line-number="21"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-22" data-line-number="22"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-23" data-line-number="23"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; U] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-24" data-line-number="24"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-25" data-line-number="25"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; W] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-26" data-line-number="26"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-27" data-line-number="27"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-28" data-line-number="28"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; Z] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-29" data-line-number="29"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BA] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-30" data-line-number="30"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; BB]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-31" data-line-number="31"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; BC] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-32" data-line-number="32"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; BD]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-33" data-line-number="33"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-34" data-line-number="34"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BE] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-35" data-line-number="35"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-36" data-line-number="36"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BF] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-37" data-line-number="37"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BG] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-38" data-line-number="38"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BH]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-39" data-line-number="39"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BI] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-40" data-line-number="40"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BJ]&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-41" data-line-number="41"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BK] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb10-42" data-line-number="42"&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; BL]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that the top-most operator is an &lt;code&gt;Elemwise&lt;/code&gt; that applies the scalar &lt;code&gt;add&lt;/code&gt; operation. This is the “&lt;span class="math inline"&gt;\(+\)&lt;/span&gt;” in &lt;span class="math inline"&gt;\(l + \phi\)&lt;/span&gt;. If we were to consider the inputs of this operator as candidates for &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, then we could do the following:&lt;/p&gt;
&lt;div class="sourceCode" id="cb11"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb11-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(lasso_model.logpt.owner.inputs)&lt;/a&gt;
&lt;a class="sourceLine" id="cb11-2" data-line-number="2"&gt;[Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;, Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Starting from the sub-graphs of each term, we could then search for any non-smooth functions that have known closed-form proximal operators. In our case, we only consider the absolute value function.&lt;/p&gt;
&lt;div class="sourceCode" id="cb12"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb12-1" data-line-number="1"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; get_abs_between(input_node):&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-2" data-line-number="2"&gt;    &lt;span class="co"&gt;&amp;quot;&amp;quot;&amp;quot; Search for `abs` in the operations between our input and the&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-3" data-line-number="3"&gt;&lt;span class="co"&gt;    log-likelihood output node.&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-4" data-line-number="4"&gt;&lt;span class="co"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-5" data-line-number="5"&gt;    term_ops &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(tt.gof.graph.ops([input_node],&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-6" data-line-number="6"&gt;[lasso_model.logpt]))&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-7" data-line-number="7"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-8" data-line-number="8"&gt;    &lt;span class="co"&gt;# Is there an absolute value in there?&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-9" data-line-number="9"&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;filter&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.op &lt;span class="kw"&gt;is&lt;/span&gt; tt.abs_, term_ops)&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-11" data-line-number="11"&gt;abs_res &lt;span class="op"&gt;=&lt;/span&gt; [(get_abs_between(in_), in_)&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-12" data-line-number="12"&gt;           &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt; lasso_model.logpt.owner.inputs]&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-14" data-line-number="14"&gt;&lt;span class="cf"&gt;for&lt;/span&gt; r_ &lt;span class="kw"&gt;in&lt;/span&gt; abs_res:&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-15" data-line-number="15"&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(r_[&lt;span class="dv"&gt;0&lt;/span&gt;]) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-16" data-line-number="16"&gt;        phi &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-17" data-line-number="17"&gt;    &lt;span class="cf"&gt;else&lt;/span&gt;:&lt;/a&gt;
&lt;a class="sourceLine" id="cb12-18" data-line-number="18"&gt;        logp &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb13"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb13-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(logp)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-2" data-line-number="2"&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-3" data-line-number="3"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-4" data-line-number="4"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-5" data-line-number="5"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-6" data-line-number="6"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; E]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-7" data-line-number="7"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-8" data-line-number="8"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-9" data-line-number="9"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-10" data-line-number="10"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-11" data-line-number="11"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-12" data-line-number="12"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-13" data-line-number="13"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-14" data-line-number="14"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; M]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-15" data-line-number="15"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-16" data-line-number="16"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; O]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-17" data-line-number="17"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; P]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-18" data-line-number="18"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-19" data-line-number="19"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; R]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-20" data-line-number="20"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-21" data-line-number="21"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-22" data-line-number="22"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; U]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-23" data-line-number="23"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-24" data-line-number="24"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; W]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-25" data-line-number="25"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-26" data-line-number="26"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-27" data-line-number="27"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(phi)&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-28" data-line-number="28"&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-29" data-line-number="29"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-30" data-line-number="30"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-31" data-line-number="31"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-32" data-line-number="32"&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-33" data-line-number="33"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-34" data-line-number="34"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; G]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-35" data-line-number="35"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-36" data-line-number="36"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-37" data-line-number="37"&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-38" data-line-number="38"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; K]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-39" data-line-number="39"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-40" data-line-number="40"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; M]&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-41" data-line-number="41"&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb13-42" data-line-number="42"&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; O]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above approach is still too limiting; we need something more robust. For instance, our logic could fail on graphs that are expressed as &lt;span class="math inline"&gt;\(\eta (l + \phi) + 1\)&lt;/span&gt;–although a graph for the equivalent expression &lt;span class="math inline"&gt;\(\eta l + \eta \phi + \eta\)&lt;/span&gt; might succeed. These are types of weaknesses inherent to naive approaches like ours. Furthermore, sufficient logic that uses a similar approach is likely to result in complicated and less approachable code.&lt;/p&gt;
&lt;p&gt;The appropriate computational tools are found in the subjects of graph unification and term rewriting, as well as the areas of functional and logic programming. Luckily, Theano provides some basic unification capabilities through its &lt;code&gt;PatternSub&lt;/code&gt; class.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PatternSub&lt;/code&gt; works within the context of Theano &lt;a href="http://deeplearning.net/software/theano/optimizations.html"&gt;graph optimization&lt;/a&gt;. Graph optimizations perform the common symbolic operations of reduction/simplification and rewriting. Consider the &lt;code&gt;phi&lt;/code&gt; variable; the print-outs show an unnecessary subtraction with &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;. Clearly this step is unnecessary, so–in a basic way–we can see that the graph hasn’t been simplified, yet.&lt;/p&gt;
&lt;p&gt;Many standard algebraic simplifications are already present in Theano, and, by creating our own graph optimizations, we can provide the advanced functionality we’ve been alluding to.&lt;/p&gt;
&lt;div class="example" data-markdown="" data-env-number="1" data-title-name="[Algebraic Graph Optimization]"&gt;
&lt;p&gt;As a quick demonstration, we’ll make replacement patterns for multiplicative distribution across two forms of addition: &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt;.&lt;/p&gt;
&lt;div class="sourceCode" id="cb14"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb14-1" data-line-number="1"&gt;test_a_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;5&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-2" data-line-number="2"&gt;test_b_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;2&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-3" data-line-number="3"&gt;test_c_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(np.r_[&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="dv"&gt;2&lt;/span&gt;], name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-4" data-line-number="4"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-5" data-line-number="5"&gt;test_exprs_tt &lt;span class="op"&gt;=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; test_b_tt,)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-6" data-line-number="6"&gt;test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_b_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-7" data-line-number="7"&gt;test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-8" data-line-number="8"&gt;test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_c_tt),)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-9" data-line-number="9"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-10" data-line-number="10"&gt;mul_dist_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-11" data-line-number="11"&gt;    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-12" data-line-number="12"&gt;    (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-13" data-line-number="13"&gt;),)&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-14" data-line-number="14"&gt;mul_dist_pat_tt &lt;span class="op"&gt;+=&lt;/span&gt; (tt.gof.opt.PatternSub(&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-15" data-line-number="15"&gt;    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.add, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-16" data-line-number="16"&gt;    (tt.add, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))&lt;/a&gt;
&lt;a class="sourceLine" id="cb14-17" data-line-number="17"&gt;),)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Substitutions can be applied to an objective function until it is in a fully-reduced form: &lt;code&gt;EquilibriumOptimizer&lt;/code&gt; provides this functionality.&lt;/p&gt;
&lt;div class="sourceCode" id="cb15"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb15-1" data-line-number="1"&gt;test_sub_eqz_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(&lt;/a&gt;
&lt;a class="sourceLine" id="cb15-2" data-line-number="2"&gt;    mul_dist_pat_tt, max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb15-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb15-4" data-line-number="4"&gt;test_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph(&lt;/a&gt;
&lt;a class="sourceLine" id="cb15-5" data-line-number="5"&gt;    tt.gof.graph.inputs(test_exprs_tt), test_exprs_tt)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb16"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb16-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(test_fgraph_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-2" data-line-number="2"&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-3" data-line-number="3"&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-4" data-line-number="4"&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-5" data-line-number="5"&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-6" data-line-number="6"&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-7" data-line-number="7"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-8" data-line-number="8"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-9" data-line-number="9"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-10" data-line-number="10"&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-11" data-line-number="11"&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-12" data-line-number="12"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-13" data-line-number="13"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-14" data-line-number="14"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-15" data-line-number="15"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-16" data-line-number="16"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-17" data-line-number="17"&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-18" data-line-number="18"&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-19" data-line-number="19"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-20" data-line-number="20"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-21" data-line-number="21"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/a&gt;
&lt;a class="sourceLine" id="cb16-22" data-line-number="22"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, when we apply the optimization, the &lt;code&gt;FunctionGraph&lt;/code&gt; should contain the replacements.&lt;/p&gt;
&lt;div class="sourceCode" id="cb17"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb17-1" data-line-number="1"&gt;test_fgraph_opt &lt;span class="op"&gt;=&lt;/span&gt; test_sub_eqz_opt_tt.optimize(test_fgraph_tt)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb18"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb18-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(test_fgraph_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-2" data-line-number="2"&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-3" data-line-number="3"&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-4" data-line-number="4"&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-5" data-line-number="5"&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;10&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-6" data-line-number="6"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-7" data-line-number="7"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-8" data-line-number="8"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-9" data-line-number="9"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-10" data-line-number="10"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-11" data-line-number="11"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-12" data-line-number="12"&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;12&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-13" data-line-number="13"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-14" data-line-number="14"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-15" data-line-number="15"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-16" data-line-number="16"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-17" data-line-number="17"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-18" data-line-number="18"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-19" data-line-number="19"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-20" data-line-number="20"&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-21" data-line-number="21"&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;11&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-22" data-line-number="22"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-23" data-line-number="23"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-24" data-line-number="24"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-25" data-line-number="25"&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-26" data-line-number="26"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; P] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-27" data-line-number="27"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb18-28" data-line-number="28"&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Even more symbolic capabilities might be needed to [efficiently] achieve the functionality we desire. Standalone libraries like SymPy and &lt;a href="https://github.com/logpy/logpy/"&gt;LogPy&lt;/a&gt; can be adapted to Theano graphs and provide these capabilities–although direct implementation in Theano may be better.&lt;/p&gt;
&lt;p&gt;Finally, let’s briefly imagine how convexity could be determined symbolically. For differentiable terms, we could start with a simple second derivative test. Within Theano, a “second derivative” can be obtained using the &lt;code&gt;hessian&lt;/code&gt; function, and within &lt;code&gt;theano.sandbox.linalg&lt;/code&gt; are &lt;code&gt;Optimizer&lt;/code&gt; hints for matrix positivity and other properties relevant to determining convexity.&lt;/p&gt;
&lt;div class="remark" data-markdown="" data-env-number="2" data-title-name=""&gt;
&lt;p&gt;Other great examples of linear algebra themed optimizations are in &lt;code&gt;theano.sandbox.linalg&lt;/code&gt;: for instance, &lt;code&gt;no_transpose_symmetric&lt;/code&gt;. Some of these demonstrate exactly how straight-forward adding algebraic features can be.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Although our convexity testing idea is far too simple for some functions, the point is that the basic tools necessary for work in this direction are already in place. With the logic programming and symbolic libraries mentioned earlier, a robust implementation of the convex function calculus could be very much in reach.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We’ve sketched out some ideas and tools with which one could develop a robust estimation platform guided by the more abstract mathematical frameworks from which new and efficient methods are produced.&lt;/p&gt;
&lt;p&gt;Some key steps may require the integration of a fully featured symbolic algebra system. Along these lines, connections between Theano, SymPy and LogPy have been explored in &lt;span class="citation" data-cites="rocklin_mathematically_2013"&gt;Rocklin (2013)&lt;/span&gt;–as well as many other important aspects of the topics discussed here.&lt;/p&gt;
&lt;p&gt;Besides the automation of proximal algorithms themselves, there are areas of application involving very large and complex models–perhaps the ones arising in Deep Learning. How might we consider the operator splitting of ADMM within deeply layered or hierarchical models &lt;span class="citation" data-cites="polson_statistical_2015"&gt;(Polson, Willard, and Heidari 2015)&lt;/span&gt;? At which levels and on which terms should the splitting be performed? Beyond trying to solve the potentially unwieldy mathematics arising from such questions, by imbuing these symbolic tools with more mathematical awareness, we can at least experiment in these directions and quickly offer numerical solutions. This is–in part–the edge from which statistics hasn’t been benefiting and modern machine learning has.&lt;/p&gt;
&lt;p&gt;Before closing, a very related–and interesting–set of ideas is worth mentioning: the possibility of encoding more symbolic knowledge into probabilistic programming platforms like PyMC3. Using the same optimization mechanisms as the examples here, simple distributional relationships can be encoded. For instance, the convolution of normally distributed random variables:&lt;/p&gt;
&lt;div class="sourceCode" id="cb19"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb19-1" data-line-number="1"&gt;mu_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_X&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-2" data-line-number="2"&gt;mu_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;1.&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-3" data-line-number="3"&gt;sd_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_X&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-4" data-line-number="4"&gt;sd_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;2.&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-5" data-line-number="5"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-6" data-line-number="6"&gt;mu_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_Y&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-7" data-line-number="7"&gt;mu_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;1.&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-8" data-line-number="8"&gt;sd_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_Y&amp;#39;&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-9" data-line-number="9"&gt;sd_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;0.5&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-10" data-line-number="10"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-11" data-line-number="11"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; conv_model:&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-12" data-line-number="12"&gt;    X_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, mu_X, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_X, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-13" data-line-number="13"&gt;    Y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu_Y, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_Y, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))&lt;/a&gt;
&lt;a class="sourceLine" id="cb19-14" data-line-number="14"&gt;    Z_rv &lt;span class="op"&gt;=&lt;/span&gt; X_rv &lt;span class="op"&gt;+&lt;/span&gt; Y_rv&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We create a Theano &lt;code&gt;Op&lt;/code&gt; to handle the convolution.&lt;/p&gt;
&lt;div class="sourceCode" id="cb20"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb20-1" data-line-number="1"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; NormConvOp(tt.Op):&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-2" data-line-number="2"&gt;    __props__ &lt;span class="op"&gt;=&lt;/span&gt; ()&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-4" data-line-number="4"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; make_node(&lt;span class="va"&gt;self&lt;/span&gt;, &lt;span class="op"&gt;*&lt;/span&gt;inputs):&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-5" data-line-number="5"&gt;        name_new &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;str&lt;/span&gt;.join(&lt;span class="st"&gt;&amp;#39;+&amp;#39;&lt;/span&gt;, [&lt;span class="bu"&gt;getattr&lt;/span&gt;(in_, &lt;span class="st"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;) &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-6" data-line-number="6"&gt;inputs])&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-7" data-line-number="7"&gt;        mu_new &lt;span class="op"&gt;=&lt;/span&gt; tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.mu &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt; inputs])&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-8" data-line-number="8"&gt;        sd_new &lt;span class="op"&gt;=&lt;/span&gt; tt.sqrt(tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.sd&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="kw"&gt;in&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-9" data-line-number="9"&gt;inputs]))&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-10" data-line-number="10"&gt;        conv_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(name_new, mu&lt;span class="op"&gt;=&lt;/span&gt;mu_new, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_new,&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-11" data-line-number="11"&gt;                            &lt;span class="co"&gt;# Is this another place where&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-12" data-line-number="12"&gt;automatically&lt;span class="op"&gt;/&lt;/span&gt;Theano managed&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-13" data-line-number="13"&gt;                            &lt;span class="co"&gt;# shapes are really needed.  For now, we&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-14" data-line-number="14"&gt;hack it.&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-15" data-line-number="15"&gt;                            shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-16" data-line-number="16"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-17" data-line-number="17"&gt;        &lt;span class="cf"&gt;return&lt;/span&gt; tt.Apply(&lt;span class="va"&gt;self&lt;/span&gt;, inputs, [conv_rv])&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-18" data-line-number="18"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-19" data-line-number="19"&gt;    &lt;span class="kw"&gt;def&lt;/span&gt; perform(&lt;span class="va"&gt;self&lt;/span&gt;, node, inputs, output_storage):&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-20" data-line-number="20"&gt;        z &lt;span class="op"&gt;=&lt;/span&gt; output_storage[&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/a&gt;
&lt;a class="sourceLine" id="cb20-21" data-line-number="21"&gt;        z[&lt;span class="dv"&gt;0&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.add(&lt;span class="op"&gt;*&lt;/span&gt;inputs)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all that’s needed is a &lt;code&gt;PatternSub&lt;/code&gt; like before.&lt;/p&gt;
&lt;div class="sourceCode" id="cb21"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb21-1" data-line-number="1"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; is_normal_dist(x):&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-2" data-line-number="2"&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;hasattr&lt;/span&gt;(x, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;) &lt;span class="kw"&gt;and&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(x.distribution,&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-3" data-line-number="3"&gt;pm.Normal)&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-4" data-line-number="4"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-5" data-line-number="5"&gt;norm_conv_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-6" data-line-number="6"&gt;    (tt.add,&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-7" data-line-number="7"&gt;     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-8" data-line-number="8"&gt;      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)},&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-9" data-line-number="9"&gt;     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;,&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-10" data-line-number="10"&gt;      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)}&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-11" data-line-number="11"&gt;     ),&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-12" data-line-number="12"&gt;    (NormConvOp(), &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;)),)&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-13" data-line-number="13"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-14" data-line-number="14"&gt;norm_conv_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(norm_conv_pat_tt,&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-15" data-line-number="15"&gt;                                                   max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-16" data-line-number="16"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-17" data-line-number="17"&gt;Z_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph([X_rv, Y_rv], [Z_rv])&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-18" data-line-number="18"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-19" data-line-number="19"&gt;&lt;span class="co"&gt;# We lose the `FreeRV.distribution` attribute when cloning the graph&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-20" data-line-number="20"&gt;&lt;span class="co"&gt;# with `theano.gof.graph.clone_get_equiv` in `FunctionGraph`, so this&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-21" data-line-number="21"&gt;&lt;span class="co"&gt;# hackishly reattaches that information:&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-22" data-line-number="22"&gt;_ &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="bu"&gt;setattr&lt;/span&gt;(g_in, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;, s_in.distribution)&lt;/a&gt;
&lt;a class="sourceLine" id="cb21-23" data-line-number="23"&gt;     &lt;span class="cf"&gt;for&lt;/span&gt; s_in, g_in &lt;span class="kw"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;zip&lt;/span&gt;([X_rv, Y_rv], Z_fgraph_tt.inputs)]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode" id="cb22"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb22-1" data-line-number="1"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; conv_model:&lt;/a&gt;
&lt;a class="sourceLine" id="cb22-2" data-line-number="2"&gt;    _ &lt;span class="op"&gt;=&lt;/span&gt; norm_conv_opt_tt.optimize(Z_fgraph_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb22-3" data-line-number="3"&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb22-4" data-line-number="4"&gt;norm_conv_var_dist &lt;span class="op"&gt;=&lt;/span&gt; Z_fgraph_tt.outputs[&lt;span class="dv"&gt;0&lt;/span&gt;].distribution&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The resulting graph:&lt;/p&gt;
&lt;div class="sourceCode" id="cb23"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb23-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; tt.printing.debugprint(Z_fgraph_tt)&lt;/a&gt;
&lt;a class="sourceLine" id="cb23-2" data-line-number="2"&gt;NormConvOp [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;X+Y&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/a&gt;
&lt;a class="sourceLine" id="cb23-3" data-line-number="3"&gt; &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; B]&lt;/a&gt;
&lt;a class="sourceLine" id="cb23-4" data-line-number="4"&gt; &lt;span class="op"&gt;|&lt;/span&gt;Y [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and the convolution’s parameters (for the test values):&lt;/p&gt;
&lt;div class="sourceCode" id="cb24"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;a class="sourceLine" id="cb24-1" data-line-number="1"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.mu.tag.test_value)&lt;/a&gt;
&lt;a class="sourceLine" id="cb24-2" data-line-number="2"&gt;[ &lt;span class="fl"&gt;2.&lt;/span&gt;]&lt;/a&gt;
&lt;a class="sourceLine" id="cb24-3" data-line-number="3"&gt;&lt;span class="op"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.sd.tag.test_value)&lt;/a&gt;
&lt;a class="sourceLine" id="cb24-4" data-line-number="4"&gt;[ &lt;span class="fl"&gt;2.06155281&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;More sophisticated routines–like the example above–could implement parameter expansions, efficient re-parameterizations and equivalent scale mixture forms in an effort to optimize a graph for sampling or point evaluation. Objectives for these optimizations could be straightforward and computationally based (e.g. reducing the number of operations in computations of the log likelihood and other quantities) or more statistically focused (e.g. highly efficient sampling, improve mixing). These ideas are most definitely not new–one example is given by &lt;span class="citation" data-cites="mohasel_afshar_probabilistic_2016"&gt;Mohasel Afshar (2016)&lt;/span&gt; for symbolic Gibbs sampling, but we hope the examples given here make the point that the tools are readily available and quite accessible.&lt;/p&gt;
&lt;p&gt;We’ll end on a much more spacey consideration. Namely, that this is a context in which we can start experimenting rapidly with objectives over the space of estimation routines. This space is generated by–but not limited to–the variety of symbolic representations, re-parameterizations, etc., mentioned above. It does not necessarily require the complete estimation of a model at each step, nor even the numeric value of quantities like the gradient or Hessian. It may involve them, but not their evaluation; perhaps, instead, symbolic comparisons of competing gradients and Hessians arising from different representations. What we’re describing lies somewhere between the completely numeric assessments common today, and the entirely symbolic work found within the theorems and manipulations of the mathematics we use to derive methods.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-combettes_proximal_2011"&gt;
&lt;p&gt;Combettes, Patrick L, and Jean-Christophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” &lt;em&gt;Fixed-Point Algorithms for Inverse Problems in Science and Engineering&lt;/em&gt;, 185–212.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-donoho_compressed_2006"&gt;
&lt;p&gt;Donoho, David L. 2006. “Compressed Sensing.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 52 (4): 1289–1306. &lt;a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066" class="uri"&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mohasel_afshar_probabilistic_2016"&gt;
&lt;p&gt;Mohasel Afshar, Hadi. 2016. “Probabilistic Inference in Piecewise Graphical Models.” &lt;a href="https://digitalcollections.anu.edu.au/handle/1885/107386" class="uri"&gt;https://digitalcollections.anu.edu.au/handle/1885/107386&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-parikh_proximal_2014"&gt;
&lt;p&gt;Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” &lt;em&gt;Foundations and Trends in Optimization&lt;/em&gt; 1 (3): 123–231. &lt;a href="https://doi.org/10.1561/2400000003" class="uri"&gt;https://doi.org/10.1561/2400000003&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-park_bayesian_2008"&gt;
&lt;p&gt;Park, Trevor, and George Casella. 2008. “The Bayesian Lasso.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 103 (482): 681–86. &lt;a href="http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337" class="uri"&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. &lt;a href="http://projecteuclid.org/euclid.ss/1449670858" class="uri"&gt;http://projecteuclid.org/euclid.ss/1449670858&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_statistical_2015"&gt;
&lt;p&gt;Polson, Nicholas G., Brandon T. Willard, and Massoud Heidari. 2015. “A Statistical Theory of Deep Learning via Proximal Splitting.” &lt;em&gt;arXiv Preprint arXiv:1509.06061&lt;/em&gt;. &lt;a href="http://arxiv.org/abs/1509.06061" class="uri"&gt;http://arxiv.org/abs/1509.06061&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rocklin_mathematically_2013"&gt;
&lt;p&gt;Rocklin, Matthew. 2013. “Mathematically Informed Linear Algebra Codes Through Term Rewriting.” PhD thesis, PhD Thesis, August. &lt;a href="http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf" class="uri"&gt;http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-salvatier_probabilistic_2016"&gt;
&lt;p&gt;Salvatier, John, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using PyMC3.” &lt;em&gt;PeerJ Computer Science&lt;/em&gt; 2 (April): e55. &lt;a href="https://peerj.com/articles/cs-55" class="uri"&gt;https://peerj.com/articles/cs-55&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-srivastava_dropout_2014"&gt;
&lt;p&gt;Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” &lt;em&gt;The Journal of Machine Learning Research&lt;/em&gt; 15 (1): 1929–58. &lt;a href="http://dl.acm.org/citation.cfm?id=2670313" class="uri"&gt;http://dl.acm.org/citation.cfm?id=2670313&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</content></entry></feed>