<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Brandon T. Willard</title><link href="https://brandonwillard.github.io/" rel="alternate"></link><link href="https://brandonwillard.github.io/feeds/all-en.atom.xml" rel="self"></link><id>https://brandonwillard.github.io/</id><updated>2017-01-18T00:00:00-06:00</updated><entry><title>A Role for Symbolic Computation in the General Estimation of Statistical Models</title><link href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html" rel="alternate"></link><published>2017-01-18T00:00:00-06:00</published><updated>2017-01-18T00:00:00-06:00</updated><author><name>Brandon T. Willard</name></author><id>tag:brandonwillard.github.io,2017-01-18:a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</id><summary type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon T. Willard" /&gt;
  &lt;title&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #303030; color: #cccccc; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #cccccc; background-color: #303030; }
code &gt; span.kw { color: #f0dfaf; } /* Keyword */
code &gt; span.dt { color: #dfdfbf; } /* DataType */
code &gt; span.dv { color: #dcdccc; } /* DecVal */
code &gt; span.bn { color: #dca3a3; } /* BaseN */
code &gt; span.fl { color: #c0bed1; } /* Float */
code &gt; span.ch { color: #dca3a3; } /* Char */
code &gt; span.st { color: #cc9393; } /* String */
code &gt; span.co { color: #7f9f7f; } /* Comment */
code &gt; span.ot { color: #efef8f; } /* Other */
code &gt; span.al { color: #ffcfaf; } /* Alert */
code &gt; span.fu { color: #efef8f; } /* Function */
code &gt; span.er { color: #c3bf9f; } /* Error */
code &gt; span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
code &gt; span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code &gt; span.sc { color: #dca3a3; } /* SpecialChar */
code &gt; span.vs { color: #cc9393; } /* VerbatimString */
code &gt; span.ss { color: #cc9393; } /* SpecialString */
code &gt; span.im { } /* Import */
code &gt; span.va { } /* Variable */
code &gt; span.cf { color: #f0dfaf; } /* ControlFlow */
code &gt; span.op { color: #f0efd0; } /* Operator */
code &gt; span.bu { } /* BuiltIn */
code &gt; span.ex { } /* Extension */
code &gt; span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code &gt; span.at { } /* Attribute */
code &gt; span.do { color: #7f9f7f; } /* Documentation */
code &gt; span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code &gt; span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code &gt; span.in { color: #7f9f7f; font-weight: bold; } /* Information */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;A Role for Symbolic Computation in the General Estimation of Statistical Models&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon T. Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2017–01–18&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this document we describe how symbolic computation can be used to provide generalizable statistical estimation through a combination of existing open source frameworks.&lt;/p&gt;
&lt;p&gt;Specifically, we consider the optimization problems resulting from models with non-smooth objective functions. These problems arise in the context of regularization and shrinkage, and here we’ll address their automated application within the &lt;em&gt;proximal framework&lt;/em&gt; &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt;. In &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt; we outlined a set of seemingly disparate optimization techniques within the fields of statistics, computer vision, and machine learning, that are unified by the types of functional bounds employed, the tools of convex analysis and the language of operator theory. These methods–and the concepts behind them–have found much success in recent times and admit quite a few interesting paths for research.&lt;/p&gt;
&lt;p&gt;In at least a few cases, the work required to produce a proximal algorithm overlaps with some highly functional areas in computer algebra and symbolic mathematics. For instance, some proximal operators–the main ingredient within proximal algorithms–can be solved exactly with symbolic differentiation and algebraic equation solvers. With perhaps only the addition of sophisticated variable assumptions and an ability to manipulate inequalities, even larger &lt;em&gt;classes&lt;/em&gt; of proximal operators could be solved. We refer the reader to the table in &lt;span class="citation" data-cites="polson_proximal_2015"&gt;(Polson, Scott, and Willard 2015)&lt;/span&gt; for examples of operators that are currently solved–by hand–using roughly the same means.&lt;/p&gt;
&lt;p&gt;The kind of automation proposed here only begins to answer a problem that arises somewhat naturally in these areas: how does one provide access to methods that produce–or apply to–numerous distinct problems and solutions. Instead of the common attempt to implement each model or method separately, and then combine them under a loosely organized API or function interface, the symbolic approach offers wider applicability and concise implementations in terms of code that maps directly to the mathematics. Also, it is one of the few practical means of including the higher and lower level considerations made by professionals at all stages of statistical modeling: mathematical, numerical, computational (e.g. distributed environments, concurrency, etc.) Some steps toward these broader goals are within reach and worth taking now.&lt;/p&gt;
&lt;p&gt;That said, statistical modeling and estimation as a whole should seriously consider aligning more with the current tools and offerings of symbolic mathematics. Relative to the subject matter here, symbolic integration provides an excellent example. In computer algebra systems, mappings between basic functional forms and their generalized hypergeometric equivalents are used to exploit convenient convolution identities. In the same vein, it might be possible to use analogous tables for solutions to a wide variety of non-smooth models. We outline how this might be done in the following sections.&lt;/p&gt;
&lt;section id="a-context" class="level2"&gt;
&lt;h2&gt;A Context&lt;/h2&gt;
&lt;p&gt;Much recent work in statistical modeling and estimation has revolved around the desire for sparsity, regularization and efficient [automatic] model selection. This is, in some sense, an objective shared with the more specialized areas of Deep Learning and Compressed Sensing. In the latter case, we can point to Dropout &lt;span class="citation" data-cites="srivastava_dropout_2014"&gt;(Srivastava et al. 2014)&lt;/span&gt; and, in the former, &lt;span class="math inline"&gt;\(\ell_p\)&lt;/span&gt; regularization &lt;span class="citation" data-cites="donoho_compressed_2006"&gt;(Donoho 2006)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without delving into those topics here, we’ll simply assume that a practitioner intends to produce a sparse estimate for a model that results in LASSO. First, some setup:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np
&lt;span class="im"&gt;import&lt;/span&gt; scipy &lt;span class="im"&gt;as&lt;/span&gt; sc
&lt;span class="im"&gt;import&lt;/span&gt; pandas &lt;span class="im"&gt;as&lt;/span&gt; pd

&lt;span class="im"&gt;import&lt;/span&gt; pymc3 &lt;span class="im"&gt;as&lt;/span&gt; pm
&lt;span class="im"&gt;import&lt;/span&gt; theano
&lt;span class="im"&gt;import&lt;/span&gt; theano.tensor &lt;span class="im"&gt;as&lt;/span&gt; tt

&lt;span class="im"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class="im"&gt;as&lt;/span&gt; plt
&lt;span class="im"&gt;import&lt;/span&gt; seaborn &lt;span class="im"&gt;as&lt;/span&gt; sb

&lt;span class="co"&gt;# plt.style.use(&amp;#39;ggplot&amp;#39;)&lt;/span&gt;
plt.rc(&lt;span class="st"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;, usetex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;FAST_COMPILE&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using PyMC3, the Bayes version of the LASSO &lt;span class="citation" data-cites="park_bayesian_2008"&gt;(Park and Casella 2008)&lt;/span&gt; model is easily specified.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; shared &lt;span class="im"&gt;as&lt;/span&gt; tt_shared

mu_true &lt;span class="op"&gt;=&lt;/span&gt; np.zeros(&lt;span class="dv"&gt;100&lt;/span&gt;)
mu_true[:&lt;span class="dv"&gt;20&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.exp(&lt;span class="op"&gt;-&lt;/span&gt;np.arange(&lt;span class="dv"&gt;20&lt;/span&gt;)) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="dv"&gt;100&lt;/span&gt;

X &lt;span class="op"&gt;=&lt;/span&gt; np.random.randn(&lt;span class="bu"&gt;int&lt;/span&gt;(np.alen(mu_true) &lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="fl"&gt;0.7&lt;/span&gt;), np.alen(mu_true))
y &lt;span class="op"&gt;=&lt;/span&gt; sc.stats.norm.rvs(loc&lt;span class="op"&gt;=&lt;/span&gt;X.dot(mu_true), scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

X_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(X, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
y_tt &lt;span class="op"&gt;=&lt;/span&gt; tt_shared(y, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, borrow&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; lasso_model:
    &lt;span class="co"&gt;# Would be nice if we could pass the symbolic y_tt.shape, so&lt;/span&gt;
    &lt;span class="co"&gt;# that our model would automatically conform to changes in&lt;/span&gt;
    &lt;span class="co"&gt;# the shared variables X_tt.&lt;/span&gt;
    &lt;span class="co"&gt;# See https://github.com/pymc-devs/pymc3/pull/1125&lt;/span&gt;
    beta_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Laplace(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, b&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, shape&lt;span class="op"&gt;=&lt;/span&gt;X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])
    y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X_tt.dot(beta_rv), sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;,
shape&lt;span class="op"&gt;=&lt;/span&gt;y.shape[&lt;span class="dv"&gt;0&lt;/span&gt;], observed&lt;span class="op"&gt;=&lt;/span&gt;y_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The negative total log likelihood in our example problem has a non-smooth &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; term. The standard means of estimating a [MAP] solution to this problem usually involves the soft-thresholding operator, which is a type of proximal operator. This operator is cheap to compute, so that–among other things–makes the proximal approaches that use it quite appealing.&lt;/p&gt;
&lt;p&gt;Moving on, let’s say we wanted to produce a MAP estimate in this PyMC3 context. A function is already provided for this generic task: &lt;code&gt;find_MAP&lt;/code&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; lasso_model:
    params_0 &lt;span class="op"&gt;=&lt;/span&gt; pm.find_MAP(&lt;span class="bu"&gt;vars&lt;/span&gt;&lt;span class="op"&gt;=&lt;/span&gt;[beta_rv])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In our run of the above, an exception is thrown due to the &lt;code&gt;nan&lt;/code&gt; values that arise within the gradient evaluation.&lt;/p&gt;
&lt;p&gt;More directly, we can inspect the gradient at &lt;span class="math inline"&gt;\(\beta = 0, 1\)&lt;/span&gt; to demonstrate the same.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;start &lt;span class="op"&gt;=&lt;/span&gt; pm.Point({&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;: np.zeros(X.shape[&lt;span class="dv"&gt;1&lt;/span&gt;])}, model&lt;span class="op"&gt;=&lt;/span&gt;lasso_model)
bij &lt;span class="op"&gt;=&lt;/span&gt; pm.DictToArrayBijection(pm.ArrayOrdering(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;),
start)
logp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastlogp)
dlogp &lt;span class="op"&gt;=&lt;/span&gt; bij.mapf(lasso_model.fastdlogp(lasso_model.&lt;span class="bu"&gt;vars&lt;/span&gt;))

&lt;span class="co"&gt;# Could also inspect the log likelihood of the prior:&lt;/span&gt;
&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;
&lt;span class="co"&gt;# beta_rv.dlogp().f(np.zeros_like(start[&amp;#39;beta&amp;#39;]))&lt;/span&gt;

grad_at_0 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.zeros_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))
grad_at_1 &lt;span class="op"&gt;=&lt;/span&gt; dlogp(np.ones_like(start[&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;]))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;1&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_0)))
Out[&lt;span class="dv"&gt;1&lt;/span&gt;]: &lt;span class="dv"&gt;100&lt;/span&gt;
In [&lt;span class="dv"&gt;2&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(np.&lt;span class="bu"&gt;sum&lt;/span&gt;(np.isnan(grad_at_1)))
Out[&lt;span class="dv"&gt;2&lt;/span&gt;]: &lt;span class="dv"&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="the-proximal-context" class="level1"&gt;
&lt;h1&gt;The Proximal Context&lt;/h1&gt;
&lt;p&gt;The general form of what we’re calling a &lt;em&gt;proximal problem&lt;/em&gt; mirrors a penalized likelihood, i.e. &lt;span class="math display"&gt;\[\begin{equation}
\beta^* = \operatorname*{argmin}_\beta \left\{ l(\beta) + \gamma \phi(\beta) \right\}
  \;,
  \label{eq:prox_problem}
\end{equation}\]&lt;/span&gt; where the entire sum is commonly associated with the negative log likelihood and the functions &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; with observation and prior terms, or loss and penalty terms, respectively. Within the proximal framework, &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; are usually convex and lower semi-continuous–although quite a few properties and results can still hold for non-convex functions.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;proximal operator&lt;/em&gt; is an explicit form of &lt;span class="math inline"&gt;\(\ref{eq:prox_problem}\)&lt;/span&gt; that has &lt;span class="math inline"&gt;\(l(\beta) = \frac{1}{2} (\beta - z)^2\)&lt;/span&gt;. It is a defining part of the intermediate steps within most proximal algorithms. Exact solutions to proximal operators exist for many &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;. These are the elements that could exist in a table, many entries of which could be generated automatically, in analogy to symbolic integration.&lt;/p&gt;
&lt;p&gt;The proximal operator relevant to our example, the soft-threshold operator, is implement in Theano with something like the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;beta_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
beta_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.r_[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="fl"&gt;0.2&lt;/span&gt;, &lt;span class="dv"&gt;1&lt;/span&gt;,
&lt;span class="dv"&gt;10&lt;/span&gt;].astype(tt.config.floatX)

lambda_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.scalar(&lt;span class="st"&gt;&amp;#39;lambda&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
lambda_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array(&lt;span class="fl"&gt;0.5&lt;/span&gt;).astype(tt.config.floatX)

&lt;span class="kw"&gt;def&lt;/span&gt; soft_threshold(beta_, lambda_):
    &lt;span class="cf"&gt;return&lt;/span&gt; tt.sgn(beta_) &lt;span class="op"&gt;*&lt;/span&gt; tt.maximum(tt.abs_(beta_) &lt;span class="op"&gt;-&lt;/span&gt; lambda_, &lt;span class="dv"&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;3&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(soft_threshold(beta_tt, lambda_tt).tag.test_value)
Out[&lt;span class="dv"&gt;3&lt;/span&gt;]: [&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;9.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt; &lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="dv"&gt;0&lt;/span&gt;.   &lt;span class="fl"&gt;0.5&lt;/span&gt;  &lt;span class="fl"&gt;9.5&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Operators like these can be composed with a gradient step to produce a &lt;em&gt;proximal gradient&lt;/em&gt; algorithm.&lt;/p&gt;
&lt;p&gt;Besides the proximal operator for &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, the steps in a proximal gradient algorithm are very straightforward and rely primarily on the gradient of &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt;. When considering this quantity, a tangible benefit of symbolic computation becomes apparent; complicated gradients can be computed automatically and efficiently. With [backtracking] line search to handle unknown step sizes, the proximal gradient alone provides a surprisingly general means of sparse estimation.&lt;/p&gt;
&lt;p&gt;Here is an implementation of a proximal gradient step:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; function &lt;span class="im"&gt;as&lt;/span&gt; tt_function

&lt;span class="kw"&gt;def&lt;/span&gt; prox_grad_step(logl, beta_tt, lambda_1_tt, gamma_prox_tt,
                   prox_func&lt;span class="op"&gt;=&lt;/span&gt;soft_threshold):
    &lt;span class="co"&gt;# Negative log-likelihood without non-smooth (\ell_1) term:&lt;/span&gt;
    logl_grad &lt;span class="op"&gt;=&lt;/span&gt; tt.grad(logl, wrt&lt;span class="op"&gt;=&lt;/span&gt;beta_tt)
    logl_grad.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl_grad&amp;quot;&lt;/span&gt;

    &lt;span class="im"&gt;from&lt;/span&gt; theano.&lt;span class="bu"&gt;compile&lt;/span&gt;.nanguardmode &lt;span class="im"&gt;import&lt;/span&gt; NanGuardMode
    tt_func_mode &lt;span class="op"&gt;=&lt;/span&gt; NanGuardMode(nan_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;,
                                inf_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                                big_is_error&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)

    beta_var_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, dtype&lt;span class="op"&gt;=&lt;/span&gt;beta_tt.dtype)
    beta_var_tt.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; beta_tt.get_value()
    grad_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([beta_var_tt], logl_grad,
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode,
                            givens&lt;span class="op"&gt;=&lt;/span&gt;{beta_tt: beta_var_tt})

    beta_quad_step &lt;span class="op"&gt;=&lt;/span&gt; beta_tt &lt;span class="op"&gt;-&lt;/span&gt; lambda_1_tt &lt;span class="op"&gt;*&lt;/span&gt; logl_grad
    beta_quad_step.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_quad_step&amp;quot;&lt;/span&gt;

    beta_prox &lt;span class="op"&gt;=&lt;/span&gt; prox_func(beta_quad_step, gamma_prox_tt &lt;span class="op"&gt;*&lt;/span&gt; lambda_1_tt)
    beta_prox.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;beta_prox&amp;quot;&lt;/span&gt;

    r_tt &lt;span class="op"&gt;=&lt;/span&gt; y_tt &lt;span class="op"&gt;-&lt;/span&gt; X_tt.dot(beta_tt)

    prox_step &lt;span class="op"&gt;=&lt;/span&gt; tt_function([],
                            [beta_prox, logl, logl_grad,
tt.mean(r_tt&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;)],
                            updates&lt;span class="op"&gt;=&lt;/span&gt;[(beta_tt, beta_prox)],
                            mode&lt;span class="op"&gt;=&lt;/span&gt;tt_func_mode)

    &lt;span class="cf"&gt;return&lt;/span&gt; (prox_step, grad_step)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/section&gt;
&lt;section id="the-symbolic-operations" class="level1"&gt;
&lt;h1&gt;The Symbolic Operations&lt;/h1&gt;
&lt;p&gt;In order to employ a lookup table, or to even identify a proximal problem and check that its conditions (e.g. convexity) are satisfied, we need to obtain the exact forms of each component: &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;. We start with the determination of &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, we’re able to tease apart our &lt;span class="math inline"&gt;\(l(\beta)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi(\beta)\)&lt;/span&gt; using the symbolic log likelihoods for the organizational designations of &lt;em&gt;observed&lt;/em&gt; and unobserved PyMC3 random variables.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; clone &lt;span class="im"&gt;as&lt;/span&gt; tt_clone

logl &lt;span class="op"&gt;=&lt;/span&gt; tt_clone(lasso_model.observed_RVs[&lt;span class="dv"&gt;0&lt;/span&gt;].logpt,
                {beta_rv: beta_tt})
logl.name &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;logl&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, let’s assume we’re extending &lt;code&gt;find_MAP&lt;/code&gt; with some generality, so that distinguishing &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; in &lt;span class="math inline"&gt;\(\ref{eq:prox_problem}\)&lt;/span&gt; using these designations isn’t reliable. This is necessary for cases in which a user specifies custom distributions or a potential function. In either case, to achieve our desired functionality we need to operate at a more symbolic level.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;At this point, it is extremely worthwhile to browse the &lt;a href="http://deeplearning.net/software/theano/extending/graphstructures.html"&gt;Theano documentation&lt;/a&gt; regarding graphs and their constituent objects.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The total log likelihood is a good place to start. Let’s look at symbolic graphs produced by ours.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pp &lt;span class="im"&gt;as&lt;/span&gt; tt_pp
&lt;span class="im"&gt;from&lt;/span&gt; theano &lt;span class="im"&gt;import&lt;/span&gt; pprint &lt;span class="im"&gt;as&lt;/span&gt; tt_pprint&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;4&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(tt_pp(lasso_model.logpt))
Out[&lt;span class="dv"&gt;4&lt;/span&gt;]:
(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(((&lt;span class="op"&gt;-&lt;/span&gt;log(TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;-&lt;/span&gt; (&lt;span class="op"&gt;|&lt;/span&gt;(&lt;span class="op"&gt;\&lt;/span&gt;beta &lt;span class="op"&gt;-&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;})&lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;})))) &lt;span class="op"&gt;+&lt;/span&gt;
Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}(switch(TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;},
(((TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} &lt;span class="op"&gt;*&lt;/span&gt; ((y &lt;span class="op"&gt;-&lt;/span&gt; (X &lt;span class="op"&gt;\&lt;/span&gt;dot &lt;span class="op"&gt;\&lt;/span&gt;beta)) &lt;span class="op"&gt;**&lt;/span&gt; TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;}))
&lt;span class="op"&gt;+&lt;/span&gt; log(TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;})) &lt;span class="op"&gt;/&lt;/span&gt; TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;}),
TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf}))))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#pretty-printing"&gt;pretty printed&lt;/a&gt; Theano graph tells us, among other things, that we have the anticipated sum of &lt;span class="math inline"&gt;\(\ell_2\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\ell_1\)&lt;/span&gt; terms.&lt;/p&gt;
&lt;p&gt;As with most graphs produced by symbolic algebra systems, we need to consider exactly how its operations are arranged, so that we can develop a means of matching general structures. The &lt;a href="http://deeplearning.net/software/theano/tutorial/printing_drawing.html#debug-print"&gt;debug printout&lt;/a&gt; is better for this.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;5&lt;/span&gt;]: tt.printing.debugprint(lasso_model.logpt)
Out[&lt;span class="dv"&gt;5&lt;/span&gt;]: Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
 &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; N]
 &lt;span class="op"&gt;|&lt;/span&gt;       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; R] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; P]
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; U] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; W] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; Z] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BA] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; BB]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; BC] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; BD]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; L]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BE] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; H]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BF] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; BG] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BH]
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BI] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; BJ]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; BK] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; BL]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that the top-most operator is an &lt;code&gt;Elemwise&lt;/code&gt; that applies the scalar &lt;code&gt;add&lt;/code&gt; operation. This is the &lt;span class="math inline"&gt;\(+\)&lt;/span&gt; in &lt;span class="math inline"&gt;\(l(\beta) + \phi(\beta)\)&lt;/span&gt;. If we were to consider the inputs to this operator as our candidates for &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, then we might find the pair with the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;6&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(lasso_model.logpt.owner.inputs)
Out[&lt;span class="dv"&gt;6&lt;/span&gt;]: [Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;, Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64}.&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using these two terms, we might simply search for an absolute value operator.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; get_abs_between(input_node):
    &lt;span class="co"&gt;# Get all the operations in the sub-tree between our input and the&lt;/span&gt;
    &lt;span class="co"&gt;# log likelihood output node.&lt;/span&gt;
    term_ops &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(tt.gof.graph.ops([input_node],
[lasso_model.logpt]))

    &lt;span class="co"&gt;# Is there an absolute value in there?&lt;/span&gt;
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;filter&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.op &lt;span class="op"&gt;is&lt;/span&gt; tt.abs_, term_ops)

abs_res &lt;span class="op"&gt;=&lt;/span&gt; [(get_abs_between(in_), in_)
           &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt; lasso_model.logpt.owner.inputs]

&lt;span class="cf"&gt;for&lt;/span&gt; r_ &lt;span class="op"&gt;in&lt;/span&gt; abs_res:
    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="bu"&gt;len&lt;/span&gt;(r_[&lt;span class="dv"&gt;0&lt;/span&gt;]) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;:
        phi &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]
    &lt;span class="cf"&gt;else&lt;/span&gt;:
        logp &lt;span class="op"&gt;=&lt;/span&gt; r_[&lt;span class="dv"&gt;1&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;7&lt;/span&gt;]: tt.printing.debugprint(logp)
Out[&lt;span class="dv"&gt;7&lt;/span&gt;]: Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{switch,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; E]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="fl"&gt;1.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{&lt;span class="bu"&gt;pow&lt;/span&gt;,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;y [&lt;span class="bu"&gt;id&lt;/span&gt; M]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;dot [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; O]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; P]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; Q] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; R]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; S] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; T] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;0.159154943092&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; U]
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; V] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="fl"&gt;2.0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; W]
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; X] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="op"&gt;-&lt;/span&gt;inf} [&lt;span class="bu"&gt;id&lt;/span&gt; Y]
In [&lt;span class="dv"&gt;8&lt;/span&gt;]: tt.printing.debugprint(phi)
Out[&lt;span class="dv"&gt;8&lt;/span&gt;]: Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Sum{acc_dtype&lt;span class="op"&gt;=&lt;/span&gt;float64} [&lt;span class="bu"&gt;id&lt;/span&gt; B] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; C] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{neg,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{log,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; G]
     &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{true_div,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{abs_,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{sub,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;beta [&lt;span class="bu"&gt;id&lt;/span&gt; K]
       &lt;span class="op"&gt;|&lt;/span&gt;   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
       &lt;span class="op"&gt;|&lt;/span&gt;     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;0&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; M]
       &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
         &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;1&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; O]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From these terms one can similarly attempt to determine multiplicative constants, or parts thereof (e.g. &lt;span class="math inline"&gt;\(\gamma\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The above approach is too limiting; we need something more robust. The above logic will fail on graphs that are constructed differently (e.g. producing equivalent, but different, representations via associativity) or when our naive use of &lt;code&gt;theano.gof.graph.ops&lt;/code&gt; is compromised by certain types of intermediate operations (e.g. non-distributed, non-affine). These are only a few of the many weaknesses inherent to the naive approach above. Furthermore, sufficient coverage of all the necessary conditions–using the same approach–is likely to result in complicated, less approachable code.&lt;/p&gt;
&lt;p&gt;What we need to identify the more general patterns suitable for our goal is mostly covered within the areas of graph unification and logic programming. Luckily, Theano has some basic unification capabilities that we’re able to deploy immediately.&lt;/p&gt;
&lt;p&gt;As an example, we’ll jump right to the creation of a &lt;a href="http://deeplearning.net/software/theano/optimizations.html"&gt;graph optimization&lt;/a&gt;. This is the context in which much of these symbolic operations are better suited; especially if we are required to alter the graph (or a copy thereof) during our search for terms. Consider the &lt;code&gt;phi&lt;/code&gt; variable; the printouts show an unnecessary subtraction (with &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;). Clearly this part–and the entire graph–hasn’t been simplified. Standard simplifications already exist for these sorts of things, and most are performed in tandem.&lt;/p&gt;
&lt;p&gt;Within the graph optimization framework we have the &lt;code&gt;PatternSub&lt;/code&gt; local optimization. It provides basic unification. To demonstrate the kind of operations that could be used to pre-condition a graph and robustly determine a set of supported &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt;, we’ll make replacement patterns for multiplicative distribution across two forms of addition: &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;add&lt;/code&gt;. These searches and replacements can be applied to an objective function until it is in a sufficiently reduced form.&lt;/p&gt;
&lt;p&gt;In the following, we’ll simply demonstrate the steps involved in programming the necessary logic.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_a_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;5&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;)
test_b_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(&lt;span class="dv"&gt;2&lt;/span&gt;, name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;)
test_c_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.as_tensor_variable(np.r_[&lt;span class="dv"&gt;1&lt;/span&gt;, &lt;span class="dv"&gt;2&lt;/span&gt;], name&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;)

test_exprs_tt &lt;span class="op"&gt;=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; test_b_tt,)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_b_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_a_tt),)
test_exprs_tt &lt;span class="op"&gt;+=&lt;/span&gt; (test_a_tt &lt;span class="op"&gt;*&lt;/span&gt; (test_c_tt &lt;span class="op"&gt;+&lt;/span&gt; test_c_tt),)

mul_dist_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.&lt;span class="bu"&gt;sum&lt;/span&gt;, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)
mul_dist_pat_tt &lt;span class="op"&gt;+=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, (tt.add, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;)),
    (tt.add, (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;), (tt.mul, &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;))
),)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next step involves the repeated application of these operations, so that a non-trivial graph can be completely transformed/reduced in some way. We achieve this with the &lt;code&gt;EquilibriumOptimizer&lt;/code&gt; class.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_sub_eqz_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(mul_dist_pat_tt,
max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

test_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph(
    tt.gof.graph.inputs(test_exprs_tt), test_exprs_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;9&lt;/span&gt;]: tt.printing.debugprint(test_fgraph_tt)
Out[&lt;span class="dv"&gt;9&lt;/span&gt;]: Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; J] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; I]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, when we apply the optimization, the &lt;code&gt;FunctionGraph&lt;/code&gt; should have applied the replacements:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;test_fgraph_opt &lt;span class="op"&gt;=&lt;/span&gt; test_sub_eqz_opt_tt.optimize(test_fgraph_tt)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;10&lt;/span&gt;]: tt.printing.debugprint(test_fgraph_tt)
Out[&lt;span class="dv"&gt;10&lt;/span&gt;]: Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;5&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; D] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;10&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; E] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;4&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;2&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; C]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; F] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;3&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; G] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;12&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; H] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;9&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; K] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;8&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; I] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;2&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; L] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;1&lt;/span&gt;
     &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
Elemwise{add,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; M] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;11&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; N] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;7&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{&lt;span class="dv"&gt;5&lt;/span&gt;} [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt; &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]
 &lt;span class="op"&gt;|&lt;/span&gt;Elemwise{mul,no_inplace} [&lt;span class="bu"&gt;id&lt;/span&gt; P] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;6&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;DimShuffle{x} [&lt;span class="bu"&gt;id&lt;/span&gt; O] &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
   &lt;span class="op"&gt;|&lt;/span&gt;TensorConstant{[&lt;span class="dv"&gt;1&lt;/span&gt; &lt;span class="dv"&gt;2&lt;/span&gt;]} [&lt;span class="bu"&gt;id&lt;/span&gt; J]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There is much more to consider in the examples above. Nonetheless, standalone libraries, like &lt;a href="https://github.com/logpy/logpy/"&gt;LogPy&lt;/a&gt; and SymPy, can be adapted to Theano graphs to provide the functionality necessary to cover most of these considerations.&lt;/p&gt;
&lt;p&gt;Finally, let’s briefly imagine how convexity could be determined symbolically. For differentiable terms, we could start with a simple second derivative test. Within Theano, a “second derivative” can be obtained using the &lt;code&gt;hessian&lt;/code&gt; function, and within &lt;code&gt;theano.sandbox.linalg&lt;/code&gt; are &lt;code&gt;Optimizer&lt;/code&gt; hints for matrix positivity and other properties relevant to determining convexity using this simple idea.&lt;/p&gt;
&lt;div class="remark" markdown="" env-number="2" title-name=""&gt;
&lt;p&gt;Other great examples of linear algebra themed optimizations are in &lt;code&gt;theano.sandbox.linalg&lt;/code&gt;: for example, &lt;code&gt;no_transpose_symmetric&lt;/code&gt;. Some of these demonstrate exactly how straight-forward it can be to add algebraic considerations.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Although our convexity testing idea is far too simple for many &lt;span class="math inline"&gt;\(l\)&lt;/span&gt;, the point we want to make is that the basic code necessary for simple tests like this may already be in place. However, with the logic programming mentioned earlier in this section, comes the possibility of implementing aspects of the convex function calculus, by which one can determine convexity for many more classes of functions.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="discussion" class="level1"&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;We’ve sketched out the concepts and mechanism with which one can develop a robust estimation platform that can be transparently guided by the more abstract mathematics frameworks from which new, efficient methods are produced.&lt;/p&gt;
&lt;p&gt;Some key steps in the process will most likely require the integration of a symbolic algebra system, so that a much wider array of algebraic machinery can be leveraged to assess–say–convexity of the terms or to solve the proximal operators themselves. Connections between Theano, SymPy and LogPy have already been explored in &lt;span class="citation" data-cites="rocklin_mathematically_2013"&gt;(Rocklin 2013)&lt;/span&gt;, as well as many other important aspects of the topics discussed here.&lt;/p&gt;
&lt;p&gt;Additionally, more advanced proximal algorithms exist to improve upon the convergence and stability of the most basic proximal gradient given here. These algorithms often involve operator splitting, which requires careful consideration regarding the exact type of splitting and on which terms it is performed. Within this area are the familiar convex-conjugate approaches; these too could be approached by symbolic solvers, or simply addressed by [partially] generated tables.&lt;/p&gt;
&lt;p&gt;Overall, there appear to be many avenues to explore just within the space of proximal algorithms and modern symbolic systems. Not all of this work necessitates the inclusion of fully featured symbolic algebra systems; much can be done with the symbolic tools of Theano alone. Furthermore, there are specialized, lightweight logic programming systems–like LogPy–that can serve as a step before full symbolic algebra integration.&lt;/p&gt;
&lt;p&gt;Besides the automation of proximal algorithms themselves, there are areas of application involving very large and complicated models or graphs–such as the ones arising in Deep Learning. How might we consider the operator splitting of ADMM within deeply layered or hierarchical models &lt;span class="citation" data-cites="polson_statistical_2015"&gt;(Polson, Willard, and Heidari 2015)&lt;/span&gt;? At which levels and on which terms should the splitting be performed? Beyond simply trying to solve the potentially intractable mathematics arising from related questions, with the symbolic capabilities described here, we can at least begin to experiment with the questions.&lt;/p&gt;
&lt;p&gt;Before closing, a very related–and at least as interesting–set of ideas is worth mentioning: the possibility of encoding more symbolic knowledge into probabilistic programming platforms like PyMC3. Using the same optimization mechanisms as the examples here, simple distributional relationships can be encoded. For instance, the convolution of normally distributed random variables:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;mu_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_X&amp;#39;&lt;/span&gt;)
mu_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_X &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_X&amp;#39;&lt;/span&gt;)
sd_X.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;2&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

mu_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;mu_Y&amp;#39;&lt;/span&gt;)
mu_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="dv"&gt;1&lt;/span&gt;.], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)
sd_Y &lt;span class="op"&gt;=&lt;/span&gt; tt.vector(&lt;span class="st"&gt;&amp;#39;sd_Y&amp;#39;&lt;/span&gt;)
sd_Y.tag.test_value &lt;span class="op"&gt;=&lt;/span&gt; np.array([&lt;span class="fl"&gt;0.5&lt;/span&gt;], dtype&lt;span class="op"&gt;=&lt;/span&gt;tt.config.floatX)

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; conv_model:
    X_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;, mu_X, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_X, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Y_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu_Y, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_Y, shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))
    Z_rv &lt;span class="op"&gt;=&lt;/span&gt; X_rv &lt;span class="op"&gt;+&lt;/span&gt; Y_rv&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We create a Theano &lt;code&gt;Op&lt;/code&gt; to handle the convolution.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;class&lt;/span&gt; NormConvOp(tt.Op):
    __props__ &lt;span class="op"&gt;=&lt;/span&gt; ()

    &lt;span class="kw"&gt;def&lt;/span&gt; make_node(&lt;span class="va"&gt;self&lt;/span&gt;, &lt;span class="op"&gt;*&lt;/span&gt;inputs):
        name_new &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;str&lt;/span&gt;.join(&lt;span class="st"&gt;&amp;#39;+&amp;#39;&lt;/span&gt;, [&lt;span class="bu"&gt;getattr&lt;/span&gt;(in_, &lt;span class="st"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;&amp;#39;&lt;/span&gt;) &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt;
inputs])
        mu_new &lt;span class="op"&gt;=&lt;/span&gt; tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.mu &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt; inputs])
        sd_new &lt;span class="op"&gt;=&lt;/span&gt; tt.sqrt(tt.add(&lt;span class="op"&gt;*&lt;/span&gt;[in_.distribution.sd&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; in_ &lt;span class="op"&gt;in&lt;/span&gt;
inputs]))
        conv_rv &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(name_new, mu&lt;span class="op"&gt;=&lt;/span&gt;mu_new, sd&lt;span class="op"&gt;=&lt;/span&gt;sd_new,
                            &lt;span class="co"&gt;# Is this another place where&lt;/span&gt;
automatically&lt;span class="op"&gt;/&lt;/span&gt;Theano managed
                            &lt;span class="co"&gt;# shapes are really needed.  For now, we&lt;/span&gt;
hack it.
                            shape&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="dv"&gt;1&lt;/span&gt;,))

        &lt;span class="cf"&gt;return&lt;/span&gt; tt.Apply(&lt;span class="va"&gt;self&lt;/span&gt;, inputs, [conv_rv])

    &lt;span class="kw"&gt;def&lt;/span&gt; perform(&lt;span class="va"&gt;self&lt;/span&gt;, node, inputs, output_storage):
        z &lt;span class="op"&gt;=&lt;/span&gt; output_storage[&lt;span class="dv"&gt;0&lt;/span&gt;]
        z[&lt;span class="dv"&gt;0&lt;/span&gt;] &lt;span class="op"&gt;=&lt;/span&gt; np.add(&lt;span class="op"&gt;*&lt;/span&gt;inputs)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all that’s needed is a &lt;code&gt;PatternSub&lt;/code&gt; like before.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="kw"&gt;def&lt;/span&gt; is_normal_dist(x):
    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="bu"&gt;hasattr&lt;/span&gt;(x, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;) &lt;span class="op"&gt;and&lt;/span&gt; &lt;span class="bu"&gt;isinstance&lt;/span&gt;(x.distribution,
pm.Normal)

norm_conv_pat_tt &lt;span class="op"&gt;=&lt;/span&gt; (tt.gof.opt.PatternSub(
    (tt.add,
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;,
      &lt;span class="st"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)},
     {&lt;span class="st"&gt;&amp;#39;pattern&amp;#39;&lt;/span&gt;: &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;,
      &lt;span class="co"&gt;&amp;#39;constraint&amp;#39;&lt;/span&gt;: &lt;span class="kw"&gt;lambda&lt;/span&gt; x: is_normal_dist(x)}
     ),
    (NormConvOp(), &lt;span class="st"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;)),)

norm_conv_opt_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.opt.EquilibriumOptimizer(norm_conv_pat_tt,
                                                   max_use_ratio&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;10&lt;/span&gt;)

Z_fgraph_tt &lt;span class="op"&gt;=&lt;/span&gt; tt.gof.fg.FunctionGraph([X_rv, Y_rv], [Z_rv])

&lt;span class="co"&gt;# We lose the `FreeRV.distribution` attribute when cloning the graph&lt;/span&gt;
&lt;span class="co"&gt;# with `theano.gof.graph.clone_get_equiv` in `FunctionGraph`, so this&lt;/span&gt;
&lt;span class="co"&gt;# hackishly reattaches that information:&lt;/span&gt;
_ &lt;span class="op"&gt;=&lt;/span&gt; [&lt;span class="bu"&gt;setattr&lt;/span&gt;(g_in, &lt;span class="st"&gt;&amp;#39;distribution&amp;#39;&lt;/span&gt;, s_in.distribution)
     &lt;span class="cf"&gt;for&lt;/span&gt; s_in, g_in &lt;span class="op"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;zip&lt;/span&gt;([X_rv, Y_rv], Z_fgraph_tt.inputs)]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="cf"&gt;with&lt;/span&gt; conv_model:
    _ &lt;span class="op"&gt;=&lt;/span&gt; norm_conv_opt_tt.optimize(Z_fgraph_tt)

norm_conv_var_dist &lt;span class="op"&gt;=&lt;/span&gt; Z_fgraph_tt.outputs[&lt;span class="dv"&gt;0&lt;/span&gt;].distribution&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The resulting graph:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;11&lt;/span&gt;]: tt.printing.debugprint(Z_fgraph_tt)
Out[&lt;span class="dv"&gt;11&lt;/span&gt;]: NormConvOp [&lt;span class="bu"&gt;id&lt;/span&gt; A] &lt;span class="st"&gt;&amp;#39;X+Y&amp;#39;&lt;/span&gt;   &lt;span class="dv"&gt;0&lt;/span&gt;
 &lt;span class="op"&gt;|&lt;/span&gt;X [&lt;span class="bu"&gt;id&lt;/span&gt; B]
 &lt;span class="op"&gt;|&lt;/span&gt;Y [&lt;span class="bu"&gt;id&lt;/span&gt; C]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and the convolution’s parameters (for the test values):&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;12&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.mu.tag.test_value)
Out[&lt;span class="dv"&gt;12&lt;/span&gt;]: [ &lt;span class="dv"&gt;2&lt;/span&gt;.]
In [&lt;span class="dv"&gt;13&lt;/span&gt;]: &lt;span class="bu"&gt;print&lt;/span&gt;(norm_conv_var_dist.sd.tag.test_value)
Out[&lt;span class="dv"&gt;13&lt;/span&gt;]: [ &lt;span class="fl"&gt;2.06155281&lt;/span&gt;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;More sophisticated routines–like the example above–could implement parameter expansions, efficient re-parameterizations and equivalent scale mixture forms in an effort to optimize a graph for sampling or point evaluation. Objectives for these optimizations could be straightforward and computationally based (e.g. reducing the number of operations in computations of the log likelihood and other quantities) or more statistically focused (e.g. highly efficient sampling, improve mixing). These ideas are most definitely not new–one example is given by &lt;span class="citation" data-cites="mohasel_afshar_probabilistic_2016"&gt;(Mohasel Afshar 2016)&lt;/span&gt; for symbolic Gibbs sampling, but we hope the examples given here make the point that the tools are readily available and quite accessible.&lt;/p&gt;
&lt;p&gt;We’ll end on a much more spacey consideration. Namely, that this is a context in which we can start experimenting rapidly with objectives over the space of estimation routines. This space is generated by–but not limited to–the variety of symbolic representations, re-parameterizations, etc., mentioned above. It does not necessarily require the complete estimation of a model at each step, nor even the numeric value of quantities like the gradient or Hessian. It may involve them, but not their evaluation; perhaps, instead, symbolic comparisons of competing gradients and Hessians arising from different representations. What we’re describing lies somewhere between the completely numeric assessments common today, and the entirely symbolic work found within the theorems and manipulations of the mathematics we use to derive methods.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;
&lt;div id="ref-donoho_compressed_2006"&gt;
&lt;p&gt;Donoho, David L. 2006. “Compressed Sensing.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 52 (4): 1289–1306. &lt;a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066" class="uri"&gt;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1614066&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-mohasel_afshar_probabilistic_2016"&gt;
&lt;p&gt;Mohasel Afshar, Hadi. 2016. “Probabilistic Inference in Piecewise Graphical Models.” &lt;a href="https://digitalcollections.anu.edu.au/handle/1885/107386" class="uri"&gt;https://digitalcollections.anu.edu.au/handle/1885/107386&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-park_bayesian_2008"&gt;
&lt;p&gt;Park, Trevor, and George Casella. 2008. “The Bayesian Lasso.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 103 (482): 681–86. &lt;a href="http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337" class="uri"&gt;http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_proximal_2015"&gt;
&lt;p&gt;Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” &lt;em&gt;Statistical Science&lt;/em&gt; 30 (4): 559–81. doi:&lt;a href="https://doi.org/10.1214/15-STS530"&gt;10.1214/15-STS530&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-polson_statistical_2015"&gt;
&lt;p&gt;Polson, Nicholas G., Brandon T. Willard, and Massoud Heidari. 2015. “A Statistical Theory of Deep Learning via Proximal Splitting.” &lt;em&gt;ArXiv Preprint ArXiv:1509.06061&lt;/em&gt;. &lt;a href="http://arxiv.org/abs/1509.06061" class="uri"&gt;http://arxiv.org/abs/1509.06061&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-rocklin_mathematically_2013"&gt;
&lt;p&gt;Rocklin, Matthew. 2013. “Mathematically Informed Linear Algebra Codes Through Term Rewriting.” PhD thesis, PhD Thesis, August. &lt;a href="http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf" class="uri"&gt;http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-srivastava_dropout_2014"&gt;
&lt;p&gt;Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” &lt;em&gt;The Journal of Machine Learning Research&lt;/em&gt; 15 (1): 1929–58. &lt;a href="http://dl.acm.org/citation.cfm?id=2670313" class="uri"&gt;http://dl.acm.org/citation.cfm?id=2670313&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</summary></entry><entry><title>Regarding Statistical Model Specification and Sample Results</title><link href="https://brandonwillard.github.io/regarding-statistical-model-specification-and-sample-results.html" rel="alternate"></link><published>2016-11-01T00:00:00-05:00</published><updated>2016-11-01T00:00:00-05:00</updated><author><name>Brandon Willard</name></author><id>tag:brandonwillard.github.io,2016-11-01:regarding-statistical-model-specification-and-sample-results.html</id><summary type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon Willard" /&gt;
  &lt;title&gt;Regarding Statistical Model Specification and Sample Results&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #303030; color: #cccccc; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #cccccc; background-color: #303030; }
code &gt; span.kw { color: #f0dfaf; } /* Keyword */
code &gt; span.dt { color: #dfdfbf; } /* DataType */
code &gt; span.dv { color: #dcdccc; } /* DecVal */
code &gt; span.bn { color: #dca3a3; } /* BaseN */
code &gt; span.fl { color: #c0bed1; } /* Float */
code &gt; span.ch { color: #dca3a3; } /* Char */
code &gt; span.st { color: #cc9393; } /* String */
code &gt; span.co { color: #7f9f7f; } /* Comment */
code &gt; span.ot { color: #efef8f; } /* Other */
code &gt; span.al { color: #ffcfaf; } /* Alert */
code &gt; span.fu { color: #efef8f; } /* Function */
code &gt; span.er { color: #c3bf9f; } /* Error */
code &gt; span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
code &gt; span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code &gt; span.sc { color: #dca3a3; } /* SpecialChar */
code &gt; span.vs { color: #cc9393; } /* VerbatimString */
code &gt; span.ss { color: #cc9393; } /* SpecialString */
code &gt; span.im { } /* Import */
code &gt; span.va { } /* Variable */
code &gt; span.cf { color: #f0dfaf; } /* ControlFlow */
code &gt; span.op { color: #f0efd0; } /* Operator */
code &gt; span.bu { } /* BuiltIn */
code &gt; span.ex { } /* Extension */
code &gt; span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code &gt; span.at { } /* Attribute */
code &gt; span.do { color: #7f9f7f; } /* Documentation */
code &gt; span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code &gt; span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code &gt; span.in { color: #7f9f7f; font-weight: bold; } /* Information */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;Regarding Statistical Model Specification and Sample Results&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2016–11–01&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;section id="introduction" class="level1"&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post I want to address some concepts regarding statistical model specification within the Bayesian paradigm, motivation for its use, and the utility of sample results (e.g. empirical posterior distributions). This write-up isn’t intended to be thorough or self-contained, especially since numerous quality introductions already exist for Bayesian modeling and MCMC (e.g. &lt;a href="http://www.stat.columbia.edu/~gelman/book/"&gt;Bayesian Data Analysis&lt;/a&gt;). Also, what’s advocated here is in large part just &lt;em&gt;statistical&lt;/em&gt; modeling and not exclusively &lt;em&gt;Bayesian&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The generality, applicability and relative simplicity of the core concepts within Bayesian modeling are sadly overlooked in practice. Bayes is too often conflated with MCMC and its associated computational costs, or be seen as needlessly “mathy” and technical. I argue that there is an oft unacknowledged trade-off in the efforts of mathematical modeling, and that Bayesian modeling helps navigate that complexity. In doing so, one can save on expended efforts in the long run.&lt;/p&gt;
&lt;p&gt;When a model is [fully] specified in a statistical or Bayesian way, the modeler has at their disposal distributions for the unknown quantities of interest; these distributions are often the primary interest. The desired estimates–and ones derived from them–are found “within” those distributions. For instance, as a distribution’s moments (e.g. mean, mode, etc.), which can correspond to estimation errors for parameters or functions thereof (e.g. rolling sums and averages).&lt;/p&gt;
&lt;p&gt;Normally, modeling objectives are specified in terms of &lt;em&gt;point-estimates&lt;/em&gt; instead of distributions: like the “best” parameters and their associated errors. This situation is also covered by the Bayesian paradigm, especially when the corresponding distributions have a closed-form and are fully specified by a finite number of parameters. However, when this isn’t the case, point-estimates provide only part of the picture. It’s usually these missing parts that make model assessment and prediction mostly separate and difficult endeavours.&lt;/p&gt;
&lt;p&gt;Even so, modeling and estimation often proceeds without much statistical consideration or context, making these distributions–and the results they can provide–more and more inaccessible. In a situation where modeling started with common machine learning/statistical software and resulted in non-statistical extensions thereto, the details needed for things like uncertainty quantification broadly equate to specifying the missing statistical context. Actually, “retro-fitting” is a better description in many cases. Considerations like this might be reason enough to–at least minimally–maintain clear statistical assumptions. The Bayesian approach fulfills this requirement and much more.&lt;/p&gt;
&lt;p&gt;As a starting point, one can find quite a few non-Bayes models with Bayesian interpretations. Otherwise, finding a Bayesian interpretation itself can advance an understanding of the statistical assumptions and properties of an existing non-Bayes model. Multiple examples arise from models defined by objective or loss functions with forms equivalent to the total log-likelihoods of Bayesian models. This, for instance, is one way that general point-wise estimates can be related to maximum a posteriori (MAP) estimates in the Bayesian context.&lt;/p&gt;
&lt;p&gt;Anyway, there’s too much to consider here, so I’m going to continue with some illustrations and details for just a few of these considerations.&lt;/p&gt;
&lt;section id="notation" class="level3"&gt;
&lt;h3&gt;Notation&lt;/h3&gt;
&lt;p&gt;Before getting into the details, let’s cover some preliminaries regarding notation.&lt;/p&gt;
&lt;p&gt;The symbol &lt;span class="math inline"&gt;\(\sim\)&lt;/span&gt; is overloaded to mean a couple things. First, a statement like &lt;span class="math inline"&gt;\(X \sim \operatorname{P}\)&lt;/span&gt; means “&lt;span class="math inline"&gt;\(X\)&lt;/span&gt; is distributed according to &lt;span class="math inline"&gt;\(\operatorname{P}\)&lt;/span&gt;”, when &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; is understood to be a random variable (generally denoted by capital letter variables). Second, for a non-random variable &lt;span class="math inline"&gt;\(x\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(x \sim \operatorname{P}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(x \sim X\)&lt;/span&gt; means “&lt;span class="math inline"&gt;\(x\)&lt;/span&gt; is a sample from distribution &lt;span class="math inline"&gt;\(\operatorname{P}\)&lt;/span&gt;”. When &lt;span class="math inline"&gt;\(\operatorname{P}\)&lt;/span&gt; is not meant to signify a distribution, but instead a generic function–like a probability density function &lt;span class="math inline"&gt;\(p(X=x) \equiv p(x)\)&lt;/span&gt;, then the distribution in question is [the] one arising from the function (interpreted as a probability density and/or measure)–when possible. See &lt;a href="https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics"&gt;here&lt;/a&gt; for a similar notation. Also, whenever indices are dropped, the resulting symbol is assumed to be a stacked matrix containing each entry, e.g. &lt;span class="math display"&gt;\[\begin{equation*}
X^\top = \begin{pmatrix} X_1 &amp;amp; \dots &amp;amp; X_N \end{pmatrix} \;.
\end{equation*}\]&lt;/span&gt; When the indexed symbol is a vector, then it is customary to denote the row stacked matrix of each vector with the symbol’s capital letter. E.g., for [column] vectors &lt;span class="math inline"&gt;\(z_i\)&lt;/span&gt; over &lt;span class="math inline"&gt;\(i \in \{1, \dots, N\}\)&lt;/span&gt;, &lt;span class="math display"&gt;\[\begin{equation*}
Z = \begin{pmatrix} z_1 \\ \vdots \\ z_N \end{pmatrix} \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="a-simple-model" class="level1"&gt;
&lt;h1&gt;A Simple Model&lt;/h1&gt;
&lt;p&gt;First, a simple normal-normal model &lt;span class="math display"&gt;\[\begin{equation}
Y_t \sim \operatorname{N}(x^\top_t \theta, \sigma^2), \quad
    \theta \sim \operatorname{N}(\mu, I \tau^2)
    \label{eq:normal-normal}
\end{equation}\]&lt;/span&gt; for an identity matrix &lt;span class="math inline"&gt;\(I\)&lt;/span&gt;, observed random variable &lt;span class="math inline"&gt;\(Y_t\)&lt;/span&gt; at time &lt;span class="math inline"&gt;\(t \in \{1, \dots, T\}\)&lt;/span&gt;, and known constant values (of matching dimensions) &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\sigma\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt;. The &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; play the role of predictors, or features, and we’ll assume that the time dependencies arise primarily through them.&lt;/p&gt;
&lt;p&gt;In Bayes parlance, the model in &lt;span class="math inline"&gt;\(\eqref{eq:normal-normal}\)&lt;/span&gt; gives &lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt; a normal prior distribution, and the primary goal involves estimating the “posterior” distribution &lt;span class="math inline"&gt;\(p(\theta \mid y)\)&lt;/span&gt;–for a vector of observations &lt;span class="math inline"&gt;\(y\)&lt;/span&gt; under the assumption &lt;span class="math inline"&gt;\(y \sim Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This simple example has the well known closed-form posterior solution for &lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math display"&gt;\[\begin{equation}
\left(\theta \mid y_t\right) \sim \operatorname{N}(m, C)
    \;.
    \label{eq:theta-posterior}
\end{equation}\]&lt;/span&gt; for &lt;span class="math display"&gt;\[\begin{equation*}
\begin{gathered}
  m = C \left(\mu \tau^{-2} + X^\top y\, \sigma^{-2}\right), \quad
  C = \left(\tau^{-2} + \operatorname{diag}(X^\top X) \sigma^{-2}\right)^{-1}
  \;.\end{gathered}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Results like this are easily obtained for the classical pairings of “conjugate” distributions. Detailed &lt;a href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions"&gt;tables&lt;/a&gt; and &lt;a href="https://goo.gl/UCL3pc"&gt;tutorials&lt;/a&gt; for conjugate distributions can be found online or in any standard text.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="estimation-via-mcmc" class="level1"&gt;
&lt;h1&gt;Estimation (via MCMC)&lt;/h1&gt;
&lt;p&gt;From here on let’s assume we do not have the closed-form result in &lt;span class="math inline"&gt;\(\eqref{eq:theta-posterior}\)&lt;/span&gt;. Instead, we’ll estimate the posterior numerically with &lt;a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"&gt;MCMC&lt;/a&gt;. Again, MCMC is covered to varying degrees of detail all over the place (e.g. &lt;a href="https://goo.gl/JNwfuo"&gt;here&lt;/a&gt;), so we’ll skip most of those details. Let’s say we’ve decided to use &lt;a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm"&gt;Metropolis-Hastings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For demonstration purposes, we produce a simulation of some data we might observe and for which we would consider applying the model in &lt;span class="math inline"&gt;\(\eqref{eq:normal-normal}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; numpy &lt;span class="im"&gt;as&lt;/span&gt; np
&lt;span class="im"&gt;import&lt;/span&gt; scipy.stats &lt;span class="im"&gt;as&lt;/span&gt; scs

&lt;span class="co"&gt;# Unknown parameter&lt;/span&gt;
mu_true &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1.5&lt;/span&gt;

&lt;span class="co"&gt;# [Assumed] known parameter&lt;/span&gt;
sigma2 &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;0.05&lt;/span&gt;

&lt;span class="co"&gt;# Prior parameters&lt;/span&gt;
tau2 &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="fl"&gt;1e2&lt;/span&gt;
mu &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;

&lt;span class="im"&gt;import&lt;/span&gt; pandas &lt;span class="im"&gt;as&lt;/span&gt; pd

start_datetime &lt;span class="op"&gt;=&lt;/span&gt; pd.tslib.Timestamp(pd.datetime.now())
sim_index &lt;span class="op"&gt;=&lt;/span&gt; pd.date_range(start&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;2016-01-01 12:00:00&amp;#39;&lt;/span&gt;,
                          end&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;2016-01-08 12:00:00&amp;#39;&lt;/span&gt;,
                          freq&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;H&amp;#39;&lt;/span&gt;)

&lt;span class="co"&gt;# Simulated observations&lt;/span&gt;
X &lt;span class="op"&gt;=&lt;/span&gt; np.sin(np.linspace(&lt;span class="dv"&gt;0&lt;/span&gt;, &lt;span class="dv"&gt;2&lt;/span&gt;&lt;span class="op"&gt;*&lt;/span&gt;np.pi, np.alen(sim_index)))
y_obs &lt;span class="op"&gt;=&lt;/span&gt; scs.norm.rvs(loc&lt;span class="op"&gt;=&lt;/span&gt;X &lt;span class="op"&gt;*&lt;/span&gt; mu_true, scale&lt;span class="op"&gt;=&lt;/span&gt;sigma2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A Metropolis-Hastings sampler would perform a simple loop that accepts or rejects samples from a proposal distribution, &lt;span class="math inline"&gt;\(\theta_i \sim p(\theta_i \mid \theta_{i-1})\)&lt;/span&gt;, according to the probability &lt;span class="math display"&gt;\[\begin{equation*}
\min\left\{1,
  \frac{p(Y = y \mid X, \theta_i)}{p(Y = y \mid X, \theta_{i-1})}
  \frac{p(\theta_i \mid \theta_{i-1})}{p(\theta_{i-1} \mid \theta_i)}
  \right\}
  \;.
\end{equation*}\]&lt;/span&gt; Let’s say our proposal is a normal distribution with a mean equal to the previous sample and a variance given by &lt;span class="math inline"&gt;\(\lambda^2\)&lt;/span&gt;. The resulting sampling scheme is a random walk Metropolis-Hastings sampler, and since the proposal is a symmetric distribution, &lt;span class="math inline"&gt;\(\frac{p(\theta_i \mid \theta_{i-1})}{p(\theta_{i-1} \mid \theta_i)} = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In code, this could look like&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; functools &lt;span class="im"&gt;import&lt;/span&gt; partial

&lt;span class="kw"&gt;def&lt;/span&gt; model_logpdf(theta_):
    res &lt;span class="op"&gt;=&lt;/span&gt; np.&lt;span class="bu"&gt;sum&lt;/span&gt;(scs.norm.logpdf(y_obs, loc&lt;span class="op"&gt;=&lt;/span&gt;X &lt;span class="op"&gt;*&lt;/span&gt; theta_,
                                 scale&lt;span class="op"&gt;=&lt;/span&gt;np.sqrt(sigma2)))
    res &lt;span class="op"&gt;+=&lt;/span&gt; scs.norm.logpdf(theta_, loc&lt;span class="op"&gt;=&lt;/span&gt;mu,
                           scale&lt;span class="op"&gt;=&lt;/span&gt;np.sqrt(tau2))
    &lt;span class="cf"&gt;return&lt;/span&gt; res

N_samples &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;2000&lt;/span&gt;
theta_samples &lt;span class="op"&gt;=&lt;/span&gt; []
lam &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;.
current_sample &lt;span class="op"&gt;=&lt;/span&gt; np.random.normal(loc&lt;span class="op"&gt;=&lt;/span&gt;mu, scale&lt;span class="op"&gt;=&lt;/span&gt;lam)
proposal_logpdf &lt;span class="op"&gt;=&lt;/span&gt; partial(scs.norm.logpdf, scale&lt;span class="op"&gt;=&lt;/span&gt;lam)

&lt;span class="cf"&gt;for&lt;/span&gt; i &lt;span class="op"&gt;in&lt;/span&gt; &lt;span class="bu"&gt;xrange&lt;/span&gt;(N_samples):
    proposal_sample &lt;span class="op"&gt;=&lt;/span&gt; np.random.normal(loc&lt;span class="op"&gt;=&lt;/span&gt;current_sample,
                                       scale&lt;span class="op"&gt;=&lt;/span&gt;lam&lt;span class="op"&gt;**&lt;/span&gt;&lt;span class="dv"&gt;2&lt;/span&gt;, size&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;)
    l_ratio &lt;span class="op"&gt;=&lt;/span&gt; np.&lt;span class="bu"&gt;sum&lt;/span&gt;(model_logpdf(proposal_sample))
    l_ratio &lt;span class="op"&gt;-=&lt;/span&gt; np.&lt;span class="bu"&gt;sum&lt;/span&gt;(model_logpdf(current_sample))

    p_ratio &lt;span class="op"&gt;=&lt;/span&gt; np.&lt;span class="bu"&gt;sum&lt;/span&gt;(proposal_logpdf(current_sample,
                                     loc&lt;span class="op"&gt;=&lt;/span&gt;proposal_sample))
    p_ratio &lt;span class="op"&gt;-=&lt;/span&gt; np.&lt;span class="bu"&gt;sum&lt;/span&gt;(proposal_logpdf(proposal_sample,
                                      loc&lt;span class="op"&gt;=&lt;/span&gt;current_sample))

    &lt;span class="cf"&gt;if&lt;/span&gt; np.log(np.random.uniform()) &lt;span class="op"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="bu"&gt;min&lt;/span&gt;(&lt;span class="dv"&gt;0&lt;/span&gt;, l_ratio &lt;span class="op"&gt;+&lt;/span&gt; p_ratio):
        current_sample &lt;span class="op"&gt;=&lt;/span&gt; proposal_sample

    theta_samples &lt;span class="op"&gt;+=&lt;/span&gt; [current_sample]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Metropolis-Hastings sampler does not rely on any prior information or Bayesian formulations. Although the prior is implicitly involved, via the total probability, the concepts behind the sampler itself are still valid without it. Basically, Metropolis-Hastings–like many other MCMC sampling routines–is not specifically Bayesian. It’s better to simply consider MCMC as just another estimation approach (or perhaps a type of stochastic optimization).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt; is arguably the other most ubiquitous MCMC technique. Since a model specified in a Bayesian way usually provides a clear joint distribution (or at least something proportional to it) and conditional probabilities, Gibbs sampling is well facilitated.&lt;/p&gt;
&lt;p&gt;The context of Bayesian modeling is, however, a good source of direction and motivation for improvements to a sampling procedure (and estimation in general). Under Bayesian assumptions, decompositions and reformulations for broad classes of distributions are often immediately available. Guiding theorems, like the &lt;a href="https://en.wikipedia.org/wiki/Rao%E2%80%93Blackwell_theorem"&gt;Rao-Blackwell&lt;/a&gt; theorem, are also applicable, and–more generally–the same principles, tools and results that guide the model creation and assessment process can also feed into the estimation process. Making these two processes less disjoint can arguably be an advantage.&lt;/p&gt;
&lt;section id="the-situation-on-implementation" class="level2"&gt;
&lt;h2&gt;The Situation on Implementation&lt;/h2&gt;
&lt;p&gt;MCMC sampling schemes like the above are fairly general and easily abstracted, giving rise to some generic frameworks that put more focus on model specification and attempt to automate the choice of estimation (or implement one robust technique). Some of the more common frameworks are Bayesian in nature: &lt;a href="http://www.openbugs.net/w/FrontPage"&gt;OpenBUGS&lt;/a&gt;, &lt;a href="http://mcmc-jags.sourceforge.net/"&gt;JAGS&lt;/a&gt;, &lt;a href="http://mc-stan.org/"&gt;Stan&lt;/a&gt;, and &lt;a href="https://pymc-devs.github.io/pymc/"&gt;PyMC2&lt;/a&gt; / &lt;a href="https://pymc-devs.github.io/pymc3/"&gt;PyMC3&lt;/a&gt;. These libraries provide a sort of meta-language that facilitates the specification of a Bayesian model and mirrors the mathematical language of probability. They also implicitly implement the &lt;a href="https://en.wikipedia.org/wiki/Algebra_of_random_variables"&gt;algebra of random variables&lt;/a&gt; and automatically handle the mechanics of variable transforms.&lt;/p&gt;
&lt;p&gt;Our model, estimated with a Metropolis-Hastings sampler, can be expressed in PyMC3 with the following code:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; pymc3 &lt;span class="im"&gt;as&lt;/span&gt; pm
&lt;span class="im"&gt;import&lt;/span&gt; theano
theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;FAST_COMPILE&amp;#39;&lt;/span&gt;

&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; model:
    &lt;span class="co"&gt;# Model definition&lt;/span&gt;
    theta &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;mu, tau&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;tau2)
    Y &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;X &lt;span class="op"&gt;*&lt;/span&gt; theta, tau&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;sigma2, observed&lt;span class="op"&gt;=&lt;/span&gt;y_obs)

    &lt;span class="co"&gt;# Posterior sampling&lt;/span&gt;
    sample_steps &lt;span class="op"&gt;=&lt;/span&gt; pm.Metropolis()
    sample_traces &lt;span class="op"&gt;=&lt;/span&gt; pm.sample(&lt;span class="dv"&gt;2000&lt;/span&gt;, sample_steps)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As per the basic examples in the &lt;a href="https://goo.gl/WW3TO8"&gt;PyMC3 notebooks&lt;/a&gt;, the posterior samples are plotted below using the following code:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class="im"&gt;as&lt;/span&gt; plt

plt.style.use(&lt;span class="st"&gt;&amp;#39;ggplot&amp;#39;&lt;/span&gt;)
plt.rc(&lt;span class="st"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;, usetex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

tp_axes &lt;span class="op"&gt;=&lt;/span&gt; pm.traceplot(sample_traces)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also superimpose the true posterior density given by &lt;span class="math inline"&gt;\(\eqref{eq:theta-posterior}\)&lt;/span&gt; with the following:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;_ &lt;span class="op"&gt;=&lt;/span&gt; [a_.set_title(&lt;span class="vs"&gt;r&amp;#39;Posterior $(\theta \mid y)$ Samples&amp;#39;&lt;/span&gt;) &lt;span class="cf"&gt;for&lt;/span&gt; a_ &lt;span class="op"&gt;in&lt;/span&gt;
tp_axes.ravel()]

freq_axis &lt;span class="op"&gt;=&lt;/span&gt; tp_axes[&lt;span class="dv"&gt;0&lt;/span&gt;][&lt;span class="dv"&gt;0&lt;/span&gt;]
freq_axis.set_xlabel(&lt;span class="vs"&gt;r&amp;#39;$\theta$&amp;#39;&lt;/span&gt;)

sample_axis &lt;span class="op"&gt;=&lt;/span&gt; tp_axes[&lt;span class="dv"&gt;0&lt;/span&gt;][&lt;span class="dv"&gt;1&lt;/span&gt;]
sample_axis.set_xlabel(&lt;span class="vs"&gt;r&amp;#39;$i$&amp;#39;&lt;/span&gt;)

rhs &lt;span class="op"&gt;=&lt;/span&gt; np.dot(&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;tau2, mu) &lt;span class="op"&gt;+&lt;/span&gt; np.dot(X.T &lt;span class="op"&gt;/&lt;/span&gt; sigma2, y_obs)
tau_post &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;tau2 &lt;span class="op"&gt;+&lt;/span&gt; np.dot(X.T &lt;span class="op"&gt;/&lt;/span&gt; sigma2, X)

post_mean &lt;span class="op"&gt;=&lt;/span&gt; rhs&lt;span class="op"&gt;/&lt;/span&gt;tau_post
post_var_inv &lt;span class="op"&gt;=&lt;/span&gt; tau_post

post_pdf &lt;span class="op"&gt;=&lt;/span&gt; partial(scs.norm.pdf,
                   loc&lt;span class="op"&gt;=&lt;/span&gt;post_mean,
                   scale&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;np.sqrt(post_var_inv))


&lt;span class="kw"&gt;def&lt;/span&gt; add_function_plot(func, ax, num&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e2&lt;/span&gt;, label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;None&lt;/span&gt;):
    post_range &lt;span class="op"&gt;=&lt;/span&gt; np.linspace(&lt;span class="op"&gt;*&lt;/span&gt;ax.get_xlim(),
                             num&lt;span class="op"&gt;=&lt;/span&gt;num, endpoint&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
    post_data &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;map&lt;/span&gt;(post_pdf, post_range)

    &lt;span class="cf"&gt;return&lt;/span&gt; ax.plot(post_range, post_data,
                   label&lt;span class="op"&gt;=&lt;/span&gt;label)

&lt;span class="co"&gt;# Add true posterior pdf to the plot&lt;/span&gt;
add_function_plot(post_pdf, freq_axis,
                  label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;Exact&amp;#39;&lt;/span&gt;)

&lt;span class="co"&gt;# Add manually produced MH samples to the plot&lt;/span&gt;
&lt;span class="im"&gt;import&lt;/span&gt; seaborn &lt;span class="im"&gt;as&lt;/span&gt; sns
sns.distplot(theta_samples[:&lt;span class="dv"&gt;2000&lt;/span&gt;], ax&lt;span class="op"&gt;=&lt;/span&gt;freq_axis, hist&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;,
                label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;Manual MH&amp;#39;&lt;/span&gt;)

sample_axis.plot(theta_samples[:&lt;span class="dv"&gt;2000&lt;/span&gt;],
                 label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;Manual MH&amp;#39;&lt;/span&gt;)


freq_axis.legend()
sample_axis.legend()
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_theta_post_plot_1.png" alt="Posterior samples" /&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="the-costs" class="level2"&gt;
&lt;h2&gt;The Costs&lt;/h2&gt;
&lt;p&gt;MCMC, and specifically the Metropolis-Hastings approach used above, can look very simple and universally applicable, but–of course–there’s a trade-off occurring somewhere. The trade-offs most often appear in relation to the complexity and cost of [intermediate] sampling steps and convergence rates. To over simplify, the standard &lt;span class="math inline"&gt;\(O(N^{-1/2})\)&lt;/span&gt; error rate–from the &lt;a href="https://en.wikipedia.org/wiki/Central_limit_theorem"&gt;Central Limit Theorem&lt;/a&gt;–is the MCMC baseline, which isn’t all that competitive with some of the standard deterministic optimization methods.&lt;/p&gt;
&lt;p&gt;Even for conceptually simple models, the proposal distribution (and its parameters) are not always easy to choose or cheap to tune. The upfront computational costs can be quite high for the more generic MCMC approaches, but there are almost always paths toward efficient samplers–in the context of a specific problem, at least.&lt;/p&gt;
&lt;p&gt;In practice, the generality and relative simplicity of the Bayes approach, combined with MCMC, can be somewhat misleading to newcomers. After some immediate success with simpler and/or scaled down problems, one is soon lead to believe that the cost of direct computations and the effort and skill required to derive efficient methods is not worth the potential parsimony and extra information provided by sample results.&lt;/p&gt;
&lt;p&gt;The unfortunate outcome of this situation is sometimes an effective rejection of Bayes and MCMC altogether. Although the point hasn’t been illustrated here, MCMC isn’t the only option. Bayesian models are just as amenable to deterministic estimation as non-Bayesian ones, and a wide array of efficient deterministic estimation techniques are available–albeit not so common in standard practice (e.g. &lt;a href="http://projecteuclid.org/euclid.ss/1449670858"&gt;proximal algorithms&lt;/a&gt;).&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="predictions" class="level1"&gt;
&lt;h1&gt;Predictions&lt;/h1&gt;
&lt;p&gt;The sampling situation offered by MCMC (and Bayes) puts one in a nice situation to make extensive use of predictions &lt;em&gt;and&lt;/em&gt; obtain uncertainty measures (e.g. variances, credible intervals, etc.).&lt;/p&gt;
&lt;p&gt;In general, posterior predictive samples are fairly easy to obtain. Once you have posterior samples of &lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;, say &lt;span class="math inline"&gt;\(\{\theta_i\}_{i=0}^M\)&lt;/span&gt;, simply plug those into the sampling/observation distribution and sample &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; values. Specifically, &lt;span class="math display"&gt;\[\begin{equation}
\{y_i \sim p(Y \mid X, \theta_i) : \theta_i \sim p(\theta_i \mid y)\}_{i=0}^M
  \label{eq:post_predict_samples}
\end{equation}\]&lt;/span&gt; is a posterior predictive sample from &lt;span class="math inline"&gt;\(p(Y \mid X, y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The procedural interpretation of &lt;span class="math inline"&gt;\(\eqref{eq:post_predict_samples}\)&lt;/span&gt; is:&lt;/p&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;p&gt;Sample &lt;span class="math inline"&gt;\(\theta_i \sim p(\theta_i \mid y)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sample &lt;span class="math inline"&gt;\(y_i \sim p(Y \mid X, \theta_i)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Assuming we’ve already produced a posterior sample, this is as simple as plugging those &lt;span class="math inline"&gt;\(\theta_i\)&lt;/span&gt; into the observation distribution &lt;span class="math inline"&gt;\(\eqref{eq:normal-normal}\)&lt;/span&gt; and sampling. The cumulative effect of this process is equivalent to producing an estimate of the marginal &lt;span class="math display"&gt;\[\begin{equation*}
\int p(Y_t \mid x_t, \theta) p(\theta \mid y) d\theta = p(Y_t \mid x_t, y)
  \;.
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The posterior predictive sample in &lt;span class="math inline"&gt;\(\eqref{eq:post_predict_samples}\)&lt;/span&gt; contains much of the information a modeler desires. Take the variance of this sample and one has a common measure of prediction error; produce quantiles of the sample and one has &lt;a href="https://en.wikipedia.org/wiki/Credible_interval"&gt;“credible”&lt;/a&gt; prediction intervals. The sample produced by mapping an arbitrary function to each posterior predictive sample is itself amenable to the aforementioned summaries, allowing one to easily produce errors for complicated uses of predicted quantities. We illustrate these use cases below.&lt;/p&gt;
&lt;p&gt;Using our previous simulation and PyMC3, the posterior predictive samples are obtained with&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;ppc_samples &lt;span class="op"&gt;=&lt;/span&gt; pm.sample_ppc(sample_traces, model&lt;span class="op"&gt;=&lt;/span&gt;model)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and plotted with&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;plt.clf()
ppc_hpd &lt;span class="op"&gt;=&lt;/span&gt; pm.hpd(ppc_samples[&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;], &lt;span class="fl"&gt;0.05&lt;/span&gt;)
plt.fill_between(np.arange(np.alen(y_obs)),
                 ppc_hpd[:, &lt;span class="dv"&gt;0&lt;/span&gt;],
                 ppc_hpd[:, &lt;span class="dv"&gt;1&lt;/span&gt;],
                 label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;$(Y \mid X, y)$ 95\&lt;/span&gt;&lt;span class="sc"&gt;% i&lt;/span&gt;&lt;span class="vs"&gt;nterval&amp;#39;&lt;/span&gt;,
                 alpha&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt;)
plt.plot(y_obs, label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;$y$&amp;#39;&lt;/span&gt;, color&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;)
plt.plot(ppc_samples[&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;].mean(axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;),
         label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;$E[Y \mid X, y]$&amp;#39;&lt;/span&gt;, alpha&lt;span class="op"&gt;=&lt;/span&gt;.&lt;span class="dv"&gt;7&lt;/span&gt;)
plt.legend()
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_hourly_ppc_plot_1.png" alt="Posterior predictive samples" /&gt;&lt;/p&gt;
&lt;div class="example" markdown="" env-number="1" title-name=""&gt;
&lt;p&gt;Let’s say we’re interested in daily, monthly, or yearly averages for &lt;span class="math inline"&gt;\(Y_t\)&lt;/span&gt; at a lower frequency–like minutes or hours. Similarly, we might want to consider functions of differences between the outputs of different models, &lt;span class="math inline"&gt;\(f(Y^{(j)} - Y^{(k)})\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(j, k \in \{1, 2\}\)&lt;/span&gt;, or more generally &lt;span class="math inline"&gt;\(f(Y^{(j)}, Y^{(k)})\)&lt;/span&gt;. These quantities derived from simple manipulations of &lt;code&gt;ppc_hpd&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Next, we produce predictions for daily averages–along with [credible] intervals.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;y_obs_h &lt;span class="op"&gt;=&lt;/span&gt; pd.Series(y_obs, index&lt;span class="op"&gt;=&lt;/span&gt;sim_index)

ppc_samples_h &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(ppc_samples[&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;].T, index&lt;span class="op"&gt;=&lt;/span&gt;sim_index)
ppc_samples_h &lt;span class="op"&gt;=&lt;/span&gt; ppc_samples_h.stack()
ppc_samples_h &lt;span class="op"&gt;=&lt;/span&gt; ppc_samples_h[:,&lt;span class="dv"&gt;0&lt;/span&gt;]

ppc_quantiles_d &lt;span class="op"&gt;=&lt;/span&gt; ppc_samples_h.resample(&lt;span class="st"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;).&lt;span class="bu"&gt;apply&lt;/span&gt;(
    &lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.quantile(q&lt;span class="op"&gt;=&lt;/span&gt;[&lt;span class="fl"&gt;0.05&lt;/span&gt;, &lt;span class="fl"&gt;0.5&lt;/span&gt;, &lt;span class="fl"&gt;0.95&lt;/span&gt;]))

ppc_quantiles_d &lt;span class="op"&gt;=&lt;/span&gt; ppc_quantiles_d.unstack()

y_obs_d &lt;span class="op"&gt;=&lt;/span&gt; y_obs_h.resample(&lt;span class="st"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;).mean()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;plt.clf()
y_obs_d.plot(label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;$f(y)$&amp;#39;&lt;/span&gt;, color&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;)
plt.fill_between(ppc_quantiles_d.index,
                 ppc_quantiles_d[&lt;span class="fl"&gt;0.05&lt;/span&gt;],
                 ppc_quantiles_d[&lt;span class="fl"&gt;0.95&lt;/span&gt;],
                 label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;$(f(Y) \mid X, y)$ 95\&lt;/span&gt;&lt;span class="sc"&gt;% i&lt;/span&gt;&lt;span class="vs"&gt;nterval&amp;#39;&lt;/span&gt;,
                 alpha&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt;)
ppc_quantiles_d[&lt;span class="fl"&gt;0.5&lt;/span&gt;].plot(label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;$E[f(Y) \mid X, y]$&amp;#39;&lt;/span&gt;, alpha&lt;span class="op"&gt;=&lt;/span&gt;.&lt;span class="dv"&gt;7&lt;/span&gt;)
plt.legend()
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_daily_ppc_plot_1.png" alt="Daily posterior predictive results from the hourly posterior." /&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="hierarchical-extensions" class="level1"&gt;
&lt;h1&gt;Hierarchical Extensions&lt;/h1&gt;
&lt;p&gt;Even though we only considered “in-sample” predictions in the previous section, out-of-sample and missing values are covered by exactly the same process (neatly simplified by PyMC3’s &lt;code&gt;sample_ppc&lt;/code&gt;). In our example we needed an exogenous variable &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; in order to sample a point from the observation model &lt;span class="math inline"&gt;\((Y_t \mid x_t)\)&lt;/span&gt;. When the values in &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; cannot be obtained–e.g. future values of a non-deterministic quantity–clever, context specific imputations are usually proposed.&lt;/p&gt;
&lt;p&gt;Nearly every instance of such imputations gives rise to an implicit model. Going back to our preference for transparent statistical specification, it behooves us to formally specify the model. If we do so in a well-defined Bayes way, then we’re immediately provided the exact same conveniences as above.&lt;/p&gt;
&lt;div id="ex:X_temp" class="example" markdown="" env-number="2" title-name=""&gt;
&lt;div id="ex:X_temp_math" style="display:none;visibility:hidden"&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{2}\label{ex:X_temp}\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If the &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; values in our sample now correspond to, say, temperature, and today is the last day in our time-indexed observations &lt;code&gt;y_obs&lt;/code&gt;, then predicting forward in time will require temperatures for the future.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;One answer to this situation is a model for &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt;. If we specify some &lt;span class="math inline"&gt;\(X_t \sim P\)&lt;/span&gt;, then we can apply the same principles above via the posterior predictive &lt;span class="math inline"&gt;\(p(X_t)\)&lt;/span&gt;. This posterior predictive will have no exogenous dependencies (unless we want it to), and its posterior can be estimated with our given &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; observations. All this occurs in exactly the same fashion as our model for &lt;span class="math inline"&gt;\(Y_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In practice, one often sees the use of summary statistics from previous &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; observations in intervals representative of the desired prediction period. For instance, in the context of Example &lt;span class="math inline"&gt;\(\ref{ex:X_temp}\)&lt;/span&gt;, the average temperatures in previous years over the months corresponding to the prediction interval (e.g. January-February averages through 2010 to 2016 as imputations for January-February 2017).&lt;/p&gt;
&lt;p&gt;This isn’t a bad idea, per se, but it is a needlessly indirect–and often insufficient–approach to defining a statistical model for &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;. It leaves out critical distributional details, the same details needed to determine how anything using our new &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; estimates might be affected (through &lt;a href="https://en.wikipedia.org/wiki/Propagation_of_uncertainty"&gt;propagation of uncertainty&lt;/a&gt;). Eventually one comes around to specifying these details, but, in situations of sufficient complexity, this practice doesn’t produce a very clean, manageable or easily extensible model.&lt;/p&gt;
&lt;p&gt;The kinds of complicated models arising in these situations are both conceptually and technically difficult to use, and–as a result–it can be very hard to produce anything other than naive asymptotic approximations for errors and intervals. Sadly, these approximations are generally insufficient for all but the simplest scenarios.&lt;/p&gt;
&lt;p&gt;In contrast, we can model the &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; values directly and have a very clear cut path toward out-of-sample predictions and their distributional properties. Even if we hold to the belief that the previous average values are a reasonable imputation, then a number of simple models can account for that assumption.&lt;/p&gt;
&lt;div id="ex:prior_extension" class="example" markdown="" env-number="3" title-name=""&gt;
&lt;div id="ex:prior_extension_math" style="display:none;visibility:hidden"&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation}\tag{3}\label{ex:prior_extension}\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s consider a normal regression model for &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; with seasonal factors, i.e. &lt;span class="math display"&gt;\[\begin{equation}
X_t \sim \operatorname{N}(d(t)^\top \beta, I \sigma_x^2)
    \label{eq:exogenous_model}
\end{equation}\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(d(t)\)&lt;/span&gt; is an indicator vector containing the seasonal factors and &lt;span class="math inline"&gt;\(I\)&lt;/span&gt; is an identity matrix.&lt;/p&gt;
&lt;p&gt;Keep in mind that we’ve stretched the notation a bit by letting &lt;span class="math inline"&gt;\(X_t\)&lt;/span&gt; be a random vector at time &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;, while &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; is still the stacked matrix of observed &lt;span class="math inline"&gt;\(x_t\)&lt;/span&gt; values. Now, we’re simply adding the assumption &lt;span class="math inline"&gt;\(x_t \sim X_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s say that our new &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; vector has terms for each day of the week; this means the matrix of stacked &lt;span class="math inline"&gt;\(d(t)\)&lt;/span&gt; values, &lt;span class="math inline"&gt;\(D\)&lt;/span&gt;, is some classical factor design matrix with levels for each day. The product &lt;span class="math inline"&gt;\(d(t)^\top \beta\)&lt;/span&gt; is then some scalar mean for the day corresponding to &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A simple substitution of this model for our previously constant &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; matrix, results in a sort of hierarchical model, which we can now coherently marginalize and obtain the desired posterior predictive, &lt;span class="math inline"&gt;\(p(Y \mid y)\)&lt;/span&gt;. This time, the posterior predictive is independent of &lt;span class="math inline"&gt;\(X_t\)&lt;/span&gt;, so we can produce results for any &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The change in our complete model is relatively minimal. The model above for &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; results in the following marginal observation model: &lt;span class="math display"&gt;\[\begin{equation*}
\begin{aligned}
    \left(Y_t \mid \beta, \theta \right) &amp;amp;\propto
    \int p(Y_t \mid X_t, \theta) p(X_t \mid \beta) dX
    \\
    &amp;amp;\sim \operatorname{N}\left(
    d(t)^\top \beta \cdot \theta,
    \sigma^2 + \sigma_x^2 \cdot d(t)^\top \beta \beta^\top d(t) \right)
    \;.
  \end{aligned}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The reduction in Example &lt;span class="math inline"&gt;\(\ref{ex:prior_extension}\)&lt;/span&gt; is quite reasonable and could be considered an entire re-definition of our initial observation model in &lt;span class="math inline"&gt;\(\eqref{eq:normal-normal}\)&lt;/span&gt;. A change like this is a natural part of the standard model development cycle. However, this is not the only way to look at it. In the Bayesian setting we can keep the observation model fixed and iterate on the prior’s specification. The resulting marginal distribution could effectively be the same under both approaches (if desired), but the latter has the advantage of at least maintaining–conditionally–our earlier work.&lt;/p&gt;
&lt;div class="example" markdown="" env-number="4" title-name=""&gt;
&lt;p&gt;We haven’t given a prior to &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;, but if we did, in the absence of conflicting assumptions, we might want the product &lt;span class="math inline"&gt;\(\beta \cdot \theta\)&lt;/span&gt; to simplified to a single unknown variables of its own, so that we’re not estimating two “entangled” variables. This idea might be inspired by an understanding of the classical &lt;a href="https://en.wikipedia.org/wiki/Parameter_identification_problem"&gt;identification&lt;/a&gt; issue arising from such products.&lt;/p&gt;
&lt;p&gt;With &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; constant, the form of our marginal observation model is basically unchanged from our initial under &lt;span class="math inline"&gt;\(x_t \to d(t)^\top \beta\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\sigma^2 \to \sigma^2 + \sigma_x^2 \cdot d(t)^\top \beta \beta^\top d(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Adherence to established models or industry standards is not uncommon. Outside of hierarchical model development, it can be very difficult to make these connections and coherently propagate statistical assumptions.&lt;/p&gt;
&lt;p&gt;This model development process expands in complexity and applicability through natural and compartmental extensions of existing terms. Simpler, “base” models are found as marginalizations of the new terms, and all the same estimation techniques apply.&lt;/p&gt;
&lt;p&gt;We’ll close with an illustration of the piecewise exogenous variable model described in Example &lt;span class="math inline"&gt;\(\ref{ex:prior_extension}\)&lt;/span&gt;. A few days are added to demonstrate out-of-sample predictions and the design matrix, &lt;span class="math inline"&gt;\(D\)&lt;/span&gt;, for &lt;span class="math inline"&gt;\(\eqref{eq:exogenous_model}\)&lt;/span&gt; is produced using &lt;a href="https://patsy.readthedocs.io/en/latest/"&gt;Patsy&lt;/a&gt;.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;import&lt;/span&gt; patsy
&lt;span class="im"&gt;import&lt;/span&gt; theano.tensor &lt;span class="im"&gt;as&lt;/span&gt; T

ext_sim_index &lt;span class="op"&gt;=&lt;/span&gt; pd.date_range(start&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;2016-01-01 12:00:00&amp;#39;&lt;/span&gt;,
                              end&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;2016-01-16 12:00:00&amp;#39;&lt;/span&gt;,
                              freq&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;H&amp;#39;&lt;/span&gt;)

y_obs_df &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(y_obs, index&lt;span class="op"&gt;=&lt;/span&gt;sim_index,
                        columns&lt;span class="op"&gt;=&lt;/span&gt;[&lt;span class="vs"&gt;r&amp;#39;y&amp;#39;&lt;/span&gt;])

&lt;span class="co"&gt;# The extra out-of-sample days are set to NaN&lt;/span&gt;
y_obs_df &lt;span class="op"&gt;=&lt;/span&gt; y_obs_df.reindex(ext_sim_index)

&lt;span class="co"&gt;# Create some missing in-sample days&lt;/span&gt;
missing_days_idx &lt;span class="op"&gt;=&lt;/span&gt; np.random.randint(&lt;span class="dv"&gt;0&lt;/span&gt;, np.alen(y_obs), &lt;span class="dv"&gt;10&lt;/span&gt;)

y_obs_df[missing_days_idx] &lt;span class="op"&gt;=&lt;/span&gt; np.nan

_, D_df &lt;span class="op"&gt;=&lt;/span&gt; patsy.dmatrices(&lt;span class="st"&gt;&amp;quot;y ~ C(y.index.weekday)&amp;quot;&lt;/span&gt;,
                          y_obs_df.notnull(),
                          return_type&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;dataframe&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, with PyMC3 our model and its extension are easily expressed, and the missing observations will be sampled automatically.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;theano.config.mode &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="st"&gt;&amp;quot;FAST_RUN&amp;quot;&lt;/span&gt;
&lt;span class="kw"&gt;del&lt;/span&gt; model
&lt;span class="cf"&gt;with&lt;/span&gt; pm.Model() &lt;span class="im"&gt;as&lt;/span&gt; model:
    theta &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;theta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;mu, tau&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;tau2)
    beta &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;beta&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;, sd&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;1e1&lt;/span&gt;,
                     shape&lt;span class="op"&gt;=&lt;/span&gt;(D_df.shape[&lt;span class="op"&gt;-&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;],))
    mu_y &lt;span class="op"&gt;=&lt;/span&gt; T.transpose(T.dot(D_df, beta)) &lt;span class="op"&gt;*&lt;/span&gt; theta

    Y &lt;span class="op"&gt;=&lt;/span&gt; pm.Normal(&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;, mu&lt;span class="op"&gt;=&lt;/span&gt;mu_y, tau&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;1&lt;/span&gt;.&lt;span class="op"&gt;/&lt;/span&gt;sigma2,
                  observed&lt;span class="op"&gt;=&lt;/span&gt;y_obs_df.icol(&lt;span class="dv"&gt;0&lt;/span&gt;))

&lt;span class="cf"&gt;with&lt;/span&gt; model:
    sample_steps &lt;span class="op"&gt;=&lt;/span&gt; [pm.Metropolis([theta]),
                    pm.Metropolis([beta])]
    &lt;span class="cf"&gt;if&lt;/span&gt; Y.missing_values &lt;span class="op"&gt;is&lt;/span&gt; &lt;span class="op"&gt;not&lt;/span&gt; &lt;span class="va"&gt;None&lt;/span&gt;:
        sample_steps &lt;span class="op"&gt;+=&lt;/span&gt; [pm.Metropolis(Y.missing_values)]
    sample_traces &lt;span class="op"&gt;=&lt;/span&gt; pm.sample(&lt;span class="dv"&gt;2000&lt;/span&gt;, sample_steps)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The posterior predictive results are plotted below.&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;ppc_samples &lt;span class="op"&gt;=&lt;/span&gt; pm.sample_ppc(sample_traces, model&lt;span class="op"&gt;=&lt;/span&gt;model)

ppc_y_samples &lt;span class="op"&gt;=&lt;/span&gt; ppc_samples[&lt;span class="st"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;]

ppc_mean_df &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(ppc_y_samples.mean(axis&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="dv"&gt;0&lt;/span&gt;),
                           index&lt;span class="op"&gt;=&lt;/span&gt;ext_sim_index,
                           columns&lt;span class="op"&gt;=&lt;/span&gt;[&lt;span class="vs"&gt;r&amp;#39;$E[Y \mid y]$&amp;#39;&lt;/span&gt;])
ppc_hpd &lt;span class="op"&gt;=&lt;/span&gt; pd.DataFrame(pm.hpd(ppc_y_samples, &lt;span class="fl"&gt;0.05&lt;/span&gt;),
                       index&lt;span class="op"&gt;=&lt;/span&gt;ext_sim_index)

y_obs_df.plot(color&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;, subplots&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;False&lt;/span&gt;)

plt.vlines(y_obs_df.index[missing_days_idx], &lt;span class="op"&gt;*&lt;/span&gt;plt.axes().get_ybound(),
           linestyle&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;dashed&amp;#39;&lt;/span&gt;, alpha&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.1&lt;/span&gt;)

plt.fill_between(y_obs_df.index,
                 ppc_hpd[&lt;span class="dv"&gt;0&lt;/span&gt;].values,
                 ppc_hpd[&lt;span class="dv"&gt;1&lt;/span&gt;].values,
                 label&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="vs"&gt;r&amp;#39;$(Y \mid y)$ 95\&lt;/span&gt;&lt;span class="sc"&gt;% i&lt;/span&gt;&lt;span class="vs"&gt;nterval&amp;#39;&lt;/span&gt;,
                 alpha&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.5&lt;/span&gt;)
ppc_mean_df.plot(ax&lt;span class="op"&gt;=&lt;/span&gt;plt.axes(), alpha&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="fl"&gt;0.7&lt;/span&gt;)
plt.legend()
plt.show()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="https://brandonwillard.github.io/figures/regarding_sample_estimates_temp_ppc_plot_1.png" alt="Posterior predictive results for the stochastic X model" /&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;section id="bibliography" class="level1 unnumbered"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references"&gt;

&lt;/div&gt;
&lt;/section&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</summary></entry><entry><title>SymPy Expression Tree Manipulation</title><link href="https://brandonwillard.github.io/sympy-expression-tree-manipulation.html" rel="alternate"></link><published>2016-10-27T00:00:00-05:00</published><updated>2016-10-27T00:00:00-05:00</updated><author><name>Brandon Willard</name></author><id>tag:brandonwillard.github.io,2016-10-27:sympy-expression-tree-manipulation.html</id><summary type="html">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;
&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;
&lt;head&gt;
  &lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;
  &lt;meta http-equiv="Content-Style-Type" content="text/css" /&gt;
  &lt;meta name="generator" content="pandoc" /&gt;
  &lt;meta name="author" content="Brandon Willard" /&gt;
  &lt;title&gt;SymPy Expression Tree Manipulation&lt;/title&gt;
  &lt;style type="text/css"&gt;code{white-space: pre;}&lt;/style&gt;
  &lt;style type="text/css"&gt;
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #303030; color: #cccccc; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; }
td.sourceCode { padding-left: 5px; }
pre, code { color: #cccccc; background-color: #303030; }
code &gt; span.kw { color: #f0dfaf; } /* Keyword */
code &gt; span.dt { color: #dfdfbf; } /* DataType */
code &gt; span.dv { color: #dcdccc; } /* DecVal */
code &gt; span.bn { color: #dca3a3; } /* BaseN */
code &gt; span.fl { color: #c0bed1; } /* Float */
code &gt; span.ch { color: #dca3a3; } /* Char */
code &gt; span.st { color: #cc9393; } /* String */
code &gt; span.co { color: #7f9f7f; } /* Comment */
code &gt; span.ot { color: #efef8f; } /* Other */
code &gt; span.al { color: #ffcfaf; } /* Alert */
code &gt; span.fu { color: #efef8f; } /* Function */
code &gt; span.er { color: #c3bf9f; } /* Error */
code &gt; span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
code &gt; span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code &gt; span.sc { color: #dca3a3; } /* SpecialChar */
code &gt; span.vs { color: #cc9393; } /* VerbatimString */
code &gt; span.ss { color: #cc9393; } /* SpecialString */
code &gt; span.im { } /* Import */
code &gt; span.va { } /* Variable */
code &gt; span.cf { color: #f0dfaf; } /* ControlFlow */
code &gt; span.op { color: #f0efd0; } /* Operator */
code &gt; span.bu { } /* BuiltIn */
code &gt; span.ex { } /* Extension */
code &gt; span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code &gt; span.at { } /* Attribute */
code &gt; span.do { color: #7f9f7f; } /* Documentation */
code &gt; span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code &gt; span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code &gt; span.in { color: #7f9f7f; font-weight: bold; } /* Information */
  &lt;/style&gt;
  &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;!--  --&gt;
&lt;!-- &lt;div id="header"&gt; --&gt;
&lt;!-- &lt;h1 class="title"&gt;SymPy Expression Tree Manipulation&lt;/h1&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h2 class="author"&gt;Brandon Willard&lt;/h2&gt; --&gt;
&lt;!--  --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;h3 class="date"&gt;2016–10–27&lt;/h3&gt; --&gt;
&lt;!--  --&gt;
&lt;!-- &lt;/div&gt; --&gt;
&lt;!--  --&gt;
&lt;p&gt;I’ve been working on some extensions to our special function computations in &lt;a href="https://arxiv.org/abs/1605.04796"&gt;Prediction risk for global-local shrinkage regression&lt;/a&gt; and decided to employ &lt;a href="https://github.com/sympy/sympy"&gt;SymPy&lt;/a&gt; as much as possible. Out of this came an &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-python-pkg/src/master/hsplus/horn_function.py"&gt;implementation&lt;/a&gt; of a bivariate confluent hypergeometric function: the &lt;a href="https://en.wikipedia.org/wiki/Humbert_series"&gt;Humbert&lt;/a&gt; &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt;. This, and some numeric implementations, are available in a &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-python-pkg"&gt;Python package&lt;/a&gt; and an &lt;a href="https://bitbucket.org/bayes-horseshoe-plus/hsplus-r-pkg"&gt;R package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the course of this work there are expectations that appear as ratios of &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; functions, so it’s helpful to have a symbolic replacement routine to identify them. &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.match"&gt;Pattern matching&lt;/a&gt;, &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.find"&gt;finding&lt;/a&gt;, substitution and &lt;a href="http://docs.sympy.org/dev/modules/core.html#sympy.core.basic.Basic.replace"&gt;replacement&lt;/a&gt; are fairly standard in SymPy, so nothing special there; however, when you want something specific, it can get rather tricky.&lt;/p&gt;
&lt;p&gt;Personally, I’ve found the approach offered by the &lt;a href="https://github.com/sympy/sympy/tree/master/sympy/strategies"&gt;&lt;code&gt;sympy.strategies&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/sympy/sympy/tree/master/sympy/unify"&gt;&lt;code&gt;sympy.unify&lt;/code&gt;&lt;/a&gt; frameworks the most appealing. See the original discussion &lt;a href="https://groups.google.com/d/msg/sympy/fspCavhbd9I/vrzUitvgiuYJ"&gt;here&lt;/a&gt;. The reason for their appeal is mostly due to their organization of the processes behind expression tree traversal and manipulation. It’s much easier to see how a very specific and non-trivial simplification or replacement could be accomplished and iteratively improved. These points are made very well in the posts &lt;a href="http://matthewrocklin.com/blog/tags.html#SymPy-ref"&gt;here&lt;/a&gt;, so check them out.&lt;/p&gt;
&lt;p&gt;Let’s say we want to write a function &lt;code&gt;as_expectations&lt;/code&gt; that takes a &lt;code&gt;sympy.Expr&lt;/code&gt; and replaces ratios of &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; functions according to the following pattern: &lt;span class="math display"&gt;\[\begin{equation}
E[X^n] = \frac{\Phi_1(\alpha, \beta, \gamma + n; x, y)}{\Phi_1(\alpha, \beta, \gamma; x, y)}
\;.
\label{eq:expectation}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As an example, let’s set up a situation in which &lt;code&gt;as_expectations&lt;/code&gt; would be used, and, from there, attempt to construct our function. Naturally, this will involve a test expression with terms that we know match :&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;1&lt;/span&gt;]: &lt;span class="im"&gt;import&lt;/span&gt; sympy &lt;span class="im"&gt;as&lt;/span&gt; sp

In [&lt;span class="dv"&gt;2&lt;/span&gt;]:

In [&lt;span class="dv"&gt;3&lt;/span&gt;]: &lt;span class="im"&gt;from&lt;/span&gt; hsplus.horn_function &lt;span class="im"&gt;import&lt;/span&gt; HornPhi1

In [&lt;span class="dv"&gt;4&lt;/span&gt;]:

In [&lt;span class="dv"&gt;5&lt;/span&gt;]: a, b, g, z_1, z_2 &lt;span class="op"&gt;=&lt;/span&gt; sp.symbols(&lt;span class="st"&gt;&amp;#39;a, b, g, z_1, z_2&amp;#39;&lt;/span&gt;, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

In [&lt;span class="dv"&gt;6&lt;/span&gt;]: phi1_1 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g,), z_1, z_2)

In [&lt;span class="dv"&gt;7&lt;/span&gt;]:

In [&lt;span class="dv"&gt;8&lt;/span&gt;]: n &lt;span class="op"&gt;=&lt;/span&gt; sp.Dummy(&lt;span class="st"&gt;&amp;#39;n&amp;#39;&lt;/span&gt;, integer&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;, positive&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

In [&lt;span class="dv"&gt;9&lt;/span&gt;]: i &lt;span class="op"&gt;=&lt;/span&gt; sp.Dummy(&lt;span class="st"&gt;&amp;#39;i&amp;#39;&lt;/span&gt;, integer&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;, nonnegative&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)

In [&lt;span class="dv"&gt;10&lt;/span&gt;]:

In [&lt;span class="dv"&gt;11&lt;/span&gt;]: phi1_2 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n,), z_1, z_2)

In [&lt;span class="dv"&gt;12&lt;/span&gt;]: phi1_3 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n &lt;span class="op"&gt;-&lt;/span&gt; i,), z_1, z_2)

In [&lt;span class="dv"&gt;13&lt;/span&gt;]:

In [&lt;span class="dv"&gt;14&lt;/span&gt;]: r_1 &lt;span class="op"&gt;=&lt;/span&gt; phi1_2&lt;span class="op"&gt;/&lt;/span&gt;phi1_1

In [&lt;span class="dv"&gt;15&lt;/span&gt;]: r_2 &lt;span class="op"&gt;=&lt;/span&gt; phi1_3&lt;span class="op"&gt;/&lt;/span&gt;phi1_1

In [&lt;span class="dv"&gt;16&lt;/span&gt;]:

In [&lt;span class="dv"&gt;17&lt;/span&gt;]: expr &lt;span class="op"&gt;=&lt;/span&gt; a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;-&lt;/span&gt; b &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;/&lt;/span&gt; g &lt;span class="op"&gt;+&lt;/span&gt; sp.Sum(z_1&lt;span class="op"&gt;/&lt;/span&gt;z_2 &lt;span class="op"&gt;*&lt;/span&gt; r_2 &lt;span class="op"&gt;-&lt;/span&gt; &lt;span class="dv"&gt;3&lt;/span&gt; &lt;span class="op"&gt;*&lt;/span&gt;
r_2, (i, &lt;span class="dv"&gt;0&lt;/span&gt;, n))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our test expression &lt;code&gt;expr&lt;/code&gt; looks like this&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(expr, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\frac{a \operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}} + \sum_{i=0}^{n}
\left(\frac{z_{1} \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad - i + n + g, \quad z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right) - \frac{b
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)}}{g
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ratios &lt;code&gt;r_1&lt;/code&gt; and &lt;code&gt;r_2&lt;/code&gt; should both be replaced by a symbol for &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt;, for &lt;span class="math inline"&gt;\(m = n\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(m = n - i\)&lt;/span&gt; when &lt;span class="math inline"&gt;\(i &amp;lt; n\)&lt;/span&gt; respectively. We could allow &lt;span class="math inline"&gt;\(E[X^0]\)&lt;/span&gt;, I suppose, but–for a more interesting discussion–let’s not.&lt;/p&gt;
&lt;p&gt;We start by creating a SymPy pattern that expresses the mathematical form of &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt; in .&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;18&lt;/span&gt;]: pnames &lt;span class="op"&gt;=&lt;/span&gt; (&lt;span class="st"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z_1&amp;#39;&lt;/span&gt;, &lt;span class="st"&gt;&amp;#39;z_2&amp;#39;&lt;/span&gt;)

In [&lt;span class="dv"&gt;19&lt;/span&gt;]: phi1_wild_args_n &lt;span class="op"&gt;=&lt;/span&gt; sp.symbols(&lt;span class="st"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;.join(n_ &lt;span class="op"&gt;+&lt;/span&gt; &lt;span class="st"&gt;&amp;#39;_w&amp;#39;&lt;/span&gt; &lt;span class="cf"&gt;for&lt;/span&gt; n_ &lt;span class="op"&gt;in&lt;/span&gt;
pnames),

...                               cls&lt;span class="op"&gt;=&lt;/span&gt;sp.Wild, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)
...

In [&lt;span class="dv"&gt;20&lt;/span&gt;]:

In [&lt;span class="dv"&gt;21&lt;/span&gt;]: n_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;n_w&amp;#39;&lt;/span&gt;,

...               properties&lt;span class="op"&gt;=&lt;/span&gt;(&lt;span class="kw"&gt;lambda&lt;/span&gt; x: x.is_integer &lt;span class="op"&gt;and&lt;/span&gt;
x.is_positive,),

...               exclude&lt;span class="op"&gt;=&lt;/span&gt;(phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;],))
...

In [&lt;span class="dv"&gt;22&lt;/span&gt;]:

In [&lt;span class="dv"&gt;23&lt;/span&gt;]: phi1_wild_d &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1(phi1_wild_args_n[&lt;span class="dv"&gt;0&lt;/span&gt;:&lt;span class="dv"&gt;2&lt;/span&gt;],

...                        phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;:&lt;span class="dv"&gt;3&lt;/span&gt;],

...                        &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n[&lt;span class="dv"&gt;3&lt;/span&gt;:&lt;span class="dv"&gt;5&lt;/span&gt;])
...

In [&lt;span class="dv"&gt;24&lt;/span&gt;]:

In [&lt;span class="dv"&gt;25&lt;/span&gt;]: phi1_wild_n &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1(phi1_wild_args_n[&lt;span class="dv"&gt;0&lt;/span&gt;:&lt;span class="dv"&gt;2&lt;/span&gt;],

...                        (phi1_wild_args_n[&lt;span class="dv"&gt;2&lt;/span&gt;] &lt;span class="op"&gt;+&lt;/span&gt; n_w,),

...                        &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n[&lt;span class="dv"&gt;3&lt;/span&gt;:&lt;span class="dv"&gt;5&lt;/span&gt;])
...

In [&lt;span class="dv"&gt;26&lt;/span&gt;]:

In [&lt;span class="dv"&gt;27&lt;/span&gt;]: C_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;C_w&amp;#39;&lt;/span&gt;, exclude&lt;span class="op"&gt;=&lt;/span&gt;[sp.S.Zero])

In [&lt;span class="dv"&gt;28&lt;/span&gt;]: E_pattern &lt;span class="op"&gt;=&lt;/span&gt; phi1_wild_n &lt;span class="op"&gt;/&lt;/span&gt; phi1_wild_d

In [&lt;span class="dv"&gt;29&lt;/span&gt;]:

In [&lt;span class="dv"&gt;30&lt;/span&gt;]: E_fn &lt;span class="op"&gt;=&lt;/span&gt; sp.Function(&lt;span class="st"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;, real&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we find an &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt; we’ll replace it with the symbolic function &lt;code&gt;E_fn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we focus on only one of the terms (one we know matches &lt;code&gt;E_pattern&lt;/code&gt;), &lt;code&gt;r_1&lt;/code&gt;, we should find that our pattern suffices:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;31&lt;/span&gt;]: r_1.match(E_pattern)
Out[&lt;span class="dv"&gt;31&lt;/span&gt;]: {a_w: a, b_w: b, g_w: g, n_w: n, z_1_w: z_1, z_2_w: z_2}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, building up to the complexity of &lt;code&gt;expr&lt;/code&gt;, we see that a simple product doesn’t:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;32&lt;/span&gt;]: (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).match(E_pattern)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Basically, the product has introduced some problems that arise from associativity. Here are the details for the root expression tree:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;33&lt;/span&gt;]: (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).func
Out[&lt;span class="dv"&gt;33&lt;/span&gt;]: sympy.core.mul.Mul
In [&lt;span class="dv"&gt;34&lt;/span&gt;]: (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).args
Out[&lt;span class="dv"&gt;34&lt;/span&gt;]:
                 &lt;span class="dv"&gt;1&lt;/span&gt;
(a, &lt;span class="op"&gt;---------------------------&lt;/span&gt;, HornPhi1(a, b, n &lt;span class="op"&gt;+&lt;/span&gt; g, z_1, z_2))
    HornPhi1(a, b, g, z_1, z_2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The root operation is multiplication and the operation’s arguments are all terms in the product/division.&lt;/p&gt;
&lt;p&gt;Any complete search for matches to &lt;code&gt;E_pattern&lt;/code&gt; would have to consider all possible combinations of terms in &lt;code&gt;(a * r_1).args&lt;/code&gt;, i.e. all possible groupings that arise due to associativity. The simple inclusion of another &lt;code&gt;Wild&lt;/code&gt; term causes the match to succeed, since SymPy’s basic pattern matching does account for associativity in this case.&lt;/p&gt;
&lt;p&gt;Here are a few explicit ways to make the match work:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;35&lt;/span&gt;]: (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).match(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern)
Out[&lt;span class="dv"&gt;35&lt;/span&gt;]: {C_w: a, a_w: a, b_w: b, g_w: g, n_w: n, z_1_w: z_1, z_2_w:
z_2}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or as a replacement:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1).replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and via &lt;code&gt;rewriterule&lt;/code&gt;:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span class="im"&gt;from&lt;/span&gt; sympy.unify.rewrite &lt;span class="im"&gt;import&lt;/span&gt; rewriterule
rl &lt;span class="op"&gt;=&lt;/span&gt; rewriterule(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern,
                 C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w, &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n),
                 phi1_wild_args_n &lt;span class="op"&gt;+&lt;/span&gt; (n_w, C_w))
res &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(rl(a &lt;span class="op"&gt;*&lt;/span&gt; r_1))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left [ a E{\left (n,a,b,g,z_{1},z_{2} \right )}\right ]
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The advantage in using &lt;code&gt;rewriterule&lt;/code&gt; is that multiple matches will be returned. If we add another &lt;span class="math inline"&gt;\(\Phi_1\)&lt;/span&gt; in the numerator, so there are multiple possible &lt;span class="math inline"&gt;\(E[X^m]\)&lt;/span&gt;, we get&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;phi1_4 &lt;span class="op"&gt;=&lt;/span&gt; HornPhi1((a, b), (g &lt;span class="op"&gt;+&lt;/span&gt; n &lt;span class="op"&gt;+&lt;/span&gt; &lt;span class="dv"&gt;1&lt;/span&gt;,), z_1, z_2)

res &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="bu"&gt;list&lt;/span&gt;(rl(a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;*&lt;/span&gt; phi1_4))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left [ a \operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n +
g, \quad z_{1}, \quad z_{2}\right )\right)} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n + 1,a,b,g,z_{1},z_{2}
\right )}, \quad a \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad n + g + 1, \quad z_{1}, \quad z_{2}\right )\right)} E{\left
(n,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n + 1,a,b,g,z_{1},z_{2}
\right )}, \quad a \operatorname{\Phi_1}{\left(\left ( a, \quad b,
\quad n + g, \quad z_{1}, \quad z_{2}\right )\right)} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )}, \quad a
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad n + g + 1, \quad
z_{1}, \quad z_{2}\right )\right)} E{\left (n,a,b,g,z_{1},z_{2} \right
)}\right ]
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;FYI: the associativity of terms inside the function arguments is causing the seemingly duplicate results.&lt;/p&gt;
&lt;p&gt;Naive use of &lt;code&gt;Expr.replace&lt;/code&gt; doesn’t give all results; instead, it does something likely unexpected:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; (a &lt;span class="op"&gt;*&lt;/span&gt; r_1 &lt;span class="op"&gt;*&lt;/span&gt; phi1_4).replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern,
                                 C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w, &lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} E{\left (n +
1,a,b,g,z_{1},z_{2} \right )} \operatorname{\Phi_1}{\left(\left ( a,
\quad b, \quad g, \quad z_{1}, \quad z_{2}\right )\right)}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Returning to our more complicated &lt;code&gt;expr&lt;/code&gt;…Just because we can match products doesn’t mean we’re finished, since we still need a good way to traverse the entire expression tree and match the sub-trees. More importantly, adding the multiplicative &lt;code&gt;Wild&lt;/code&gt; term &lt;code&gt;C_w&lt;/code&gt; is more of a hack than a direct solution, since we don’t want the matched contents of &lt;code&gt;C_w&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;Expr.replace/xreplace&lt;/code&gt; will match sub-expressions, we found above that it produces some odd results. Those results persist when applied to more complicated expressions:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; expr.replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} - \frac{b}{g} E{\left
(n,a,b,g,z_{1},z_{2} \right )} + \sum_{i=0}^{n} \left(\frac{z_{1}
E{\left (n,a,b,- i + g,z_{1},z_{2} \right )}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + g, \quad
z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3 E{\left (n,a,b,- i +
g,z_{1},z_{2} \right )} \operatorname{\Phi_1}{\left(\left ( a, \quad
b, \quad - i + g, \quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right)
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, it looks like the matching was a little too liberal and introduced extra &lt;code&gt;E&lt;/code&gt; and &lt;code&gt;HornPhi1&lt;/code&gt; terms. This is to be expected from the &lt;code&gt;Wild&lt;/code&gt; matching in SymPy; it needs us to specify what &lt;em&gt;not&lt;/em&gt; to match, as well. Our “fix” that introduced &lt;code&gt;C_w&lt;/code&gt; is the exact source of the problem, but we can tell it not to match &lt;code&gt;HornPhi1&lt;/code&gt; terms and get better results:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;C_w &lt;span class="op"&gt;=&lt;/span&gt; sp.Wild(&lt;span class="st"&gt;&amp;#39;C_w&amp;#39;&lt;/span&gt;, exclude&lt;span class="op"&gt;=&lt;/span&gt;[sp.S.Zero, HornPhi1])
res &lt;span class="op"&gt;=&lt;/span&gt; expr.replace(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern, C_w &lt;span class="op"&gt;*&lt;/span&gt; E_fn(n_w,
&lt;span class="op"&gt;*&lt;/span&gt;phi1_wild_args_n))
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
a E{\left (n,a,b,g,z_{1},z_{2} \right )} - \frac{b}{g} E{\left
(n,a,b,g,z_{1},z_{2} \right )} + \sum_{i=0}^{n} \left(\frac{z_{1}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right )\right)}}{z_{2}
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g, \quad z_{1},
\quad z_{2}\right )\right)}} - \frac{3
\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad - i + n + g,
\quad z_{1}, \quad z_{2}\right
)\right)}}{\operatorname{\Phi_1}{\left(\left ( a, \quad b, \quad g,
\quad z_{1}, \quad z_{2}\right )\right)}}\right)
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ve stopped it from introducing those superfluous &lt;code&gt;E&lt;/code&gt; terms, but we’re still not getting replacements for the &lt;code&gt;HornPhi1&lt;/code&gt; ratios in the sums. Let’s single out those terms and see what’s going on:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;res &lt;span class="op"&gt;=&lt;/span&gt; r_2.find(C_w &lt;span class="op"&gt;*&lt;/span&gt; E_pattern)
&lt;span class="bu"&gt;print&lt;/span&gt;(sp.latex(res, mode&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="st"&gt;&amp;#39;equation*&amp;#39;&lt;/span&gt;, itex&lt;span class="op"&gt;=&lt;/span&gt;&lt;span class="va"&gt;True&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{equation*}
\left\{\right\}
\end{equation*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The constrained integer &lt;code&gt;Wild&lt;/code&gt; term, &lt;code&gt;n_w&lt;/code&gt;, probably isn’t matching. Given the form of our pattern, &lt;code&gt;n_w&lt;/code&gt; should match &lt;code&gt;n - i&lt;/code&gt;, but &lt;code&gt;n - i&lt;/code&gt; isn’t strictly positive, as required:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;36&lt;/span&gt;]: (n &lt;span class="op"&gt;-&lt;/span&gt; i).is_positive &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
Out[&lt;span class="dv"&gt;36&lt;/span&gt;]: &lt;span class="va"&gt;False&lt;/span&gt;
In [&lt;span class="dv"&gt;37&lt;/span&gt;]: sp.ask(sp.Q.positive(n &lt;span class="op"&gt;-&lt;/span&gt; i)) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
Out[&lt;span class="dv"&gt;37&lt;/span&gt;]: &lt;span class="va"&gt;False&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math inline"&gt;\(n &amp;gt; 0\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(i &amp;gt;= 0\)&lt;/span&gt;, the only missing piece is that &lt;span class="math inline"&gt;\(n &amp;gt; i\)&lt;/span&gt;. The most relevant mechanism in SymPy to assess this information is the &lt;a href="http://docs.sympy.org/dev/modules/assumptions/index.html"&gt;&lt;code&gt;sympy.assumptions&lt;/code&gt;&lt;/a&gt; interface. We could add and retrieve the assumption &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; via &lt;code&gt;sympy.assume.global_assumptions&lt;/code&gt;, or perform these operations inside of a Python &lt;code&gt;with&lt;/code&gt; block, etc. This context management, via &lt;code&gt;sympy.assumptions.assume.AssumptionsContext&lt;/code&gt;, would have to be performed manually, since I am not aware of any such mechanism offered by &lt;code&gt;Sum&lt;/code&gt; and/or &lt;code&gt;Basic.replace&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, these ideas sound good, but aren’t implemented:&lt;/p&gt;
&lt;div class="sourceCode"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;
In [&lt;span class="dv"&gt;38&lt;/span&gt;]: sp.ask(sp.Q.positive(n &lt;span class="op"&gt;-&lt;/span&gt; i), sp.Q.is_true(n &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; i)) &lt;span class="op"&gt;==&lt;/span&gt; &lt;span class="va"&gt;True&lt;/span&gt;
Out[&lt;span class="dv"&gt;38&lt;/span&gt;]: &lt;span class="va"&gt;False&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the documentation for &lt;code&gt;sympy.assumptions.ask.ask&lt;/code&gt;; it explicitely states that inequalities aren’t handled, yet.&lt;/p&gt;
&lt;p&gt;We could probably perform a manual reworking of &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; to &lt;code&gt;sympy.Q.is_true(n - i &amp;gt; 0)&lt;/code&gt;, which is of course equivalent to &lt;code&gt;sympy.Q.positive(n - i)&lt;/code&gt;: the result we want.&lt;/p&gt;
&lt;p&gt;If one were to provide this functionality, there’s still the question of how the relevant &lt;code&gt;AssumptionsContext&lt;/code&gt;s would be created and passed around/nested during the subexpression replacements. There is no apparent means of adding this sort of functionality through the &lt;code&gt;Basic.replace&lt;/code&gt; interface, so this path looks less appealing. However, nesting &lt;code&gt;with&lt;/code&gt; blocks from strategies in &lt;code&gt;sympy.strategies&lt;/code&gt; does seem quite possible. For example, in &lt;code&gt;sympy.strategies.traverse.sall&lt;/code&gt;, one could possibly wrap the &lt;code&gt;return&lt;/code&gt; statement after the &lt;code&gt;map(rule, ...)&lt;/code&gt; call in a &lt;code&gt;with sympy.assuming(...):&lt;/code&gt; block that contains the assumptions for any variables arising as, say, the index of a &lt;code&gt;Sum&lt;/code&gt;–like in our case. In this scenario, code in the subexpressions would be able to ask questions like &lt;code&gt;sympy.Q.is_true(n &amp;gt; i)&lt;/code&gt; without altering the global assumptions context or the objects involved.&lt;/p&gt;
&lt;p&gt;Anyway, that’s all I wanted to cover here. Perhaps later I’ll post a hack for the assumptions approach, but–at the very least–I’ll try to follow up with a more direct solution that uses &lt;code&gt;sympy.strategies&lt;/code&gt;.&lt;/p&gt;
&lt;div id="refs" class="references"&gt;

&lt;/div&gt;
&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</summary></entry></feed>