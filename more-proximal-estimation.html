<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>More Proximal Estimation - Brandon T. Willard</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://brandonwillard.github.io/more-proximal-estimation.html">

        <meta name="author" content="Brandon T. Willard" />

        <meta property="og:site_name" content="Brandon T. Willard" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="More Proximal Estimation"/>
        <meta property="og:url" content="https://brandonwillard.github.io/more-proximal-estimation.html"/>
        <meta property="og:description" content=""/>
        <meta property="article:published_time" content="2017-03-06" />
            <meta property="article:section" content="articles" />
            <meta property="article:author" content="Brandon T. Willard" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/bootstrap.readable.min.css" type="text/css"/>
    <link href="https://brandonwillard.github.io/theme/css/font-awesome.min.css" rel="stylesheet">
    <link href="https://brandonwillard.github.io/theme/css/academicons.min.css" rel="stylesheet">

    <link href="https://brandonwillard.github.io/theme/css/pygments/vim.css" rel="stylesheet">
    <link rel="stylesheet" href="https://brandonwillard.github.io/theme/css/style.css" type="text/css"/>
        <link href="https://brandonwillard.github.io/extra/custom.css" rel="stylesheet">

        <link href="https://brandonwillard.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Brandon T. Willard ATOM Feed"/>

        <link href="https://brandonwillard.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate"
              title="Brandon T. Willard RSS Feed"/>


        <link href="https://brandonwillard.github.io/feeds/articles.atom.xml" type="application/atom+xml" rel="alternate"
              title="Brandon T. Willard articles ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://brandonwillard.github.io/" class="navbar-brand">
Brandon T. Willard            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="https://brandonwillard.github.io/pages/publications.html">
                             Publications
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/projects.html">
                             Projects
                          </a></li>
                         <li><a href="https://brandonwillard.github.io/pages/about.html">
                             About
                          </a></li>
                        <li class="active">
                            <a href="https://brandonwillard.github.io/category/articles.html">Articles</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://brandonwillard.github.io/more-proximal-estimation.html"
                       rel="bookmark"
                       title="Permalink to More Proximal Estimation">
                        More Proximal Estimation
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-03-06T00:00:00-06:00"> Mon 06 March 2017</time>
    </span>





    
</footer><!-- /.post-info -->                    </div>
                </div>
                <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Brandon T. Willard" />
  <title>More Proximal Estimation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<!--  -->
<!-- <div id="header"> -->
<!-- <h1 class="title">More Proximal Estimation</h1> -->
<!--  -->
<!--  -->
<!-- <h2 class="author">Brandon T. Willard</h2> -->
<!--  -->
<!--  -->
<!-- <h3 class="date">2017–03–06</h3> -->
<!--  -->
<!-- </div> -->
<!--  -->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The focal point of this short exposition will be an elaboration of the basic <span class="math inline">\(\ell_1\)</span> penalization problem discussed in <span class="citation" data-cites="willard_role_2017">Willard (2017)</span>, <span class="math display">\[\begin{equation}
\operatorname*{argmin}_{\beta} \left\{
  \frac{1}{2} \|y - X \beta\|^2_2
    + \lambda \|\beta\|_1
  \right\}
  \;.
  \label{eq:lasso}
\end{equation}\]</span> We continue our discussion on topics concerning automation and symbolic computation in Theano <span class="citation" data-cites="bergstra_theano_2010">(Bergstra et al. 2010)</span>, as well as the mathematical methodology we believe is suitable for such implementations. Again, our framing of the problem is in terms of “proximal methods” <span class="citation" data-cites="parikh_proximal_2014 combettes_proximal_2011">(Parikh and Boyd 2014; Combettes and Pesquet 2011)</span>. Along the way we propose one simple means of placing the well-known technique of coordinate descent within the scope of proximal methods via a general property of proximal operators. These efforts are a continued outgrowth of our work in <span class="citation" data-cites="polson_proximal_2015">Polson, Scott, and Willard (2015)</span>.</p>
</section>
<section id="proximal-and-computational-components" class="level1">
<h1>Proximal and Computational Components</h1>
<p>First, we [re]-introduce the workhorse of proximal methods: the <em>proximal operator</em>.</p>
<div class="definition" data-markdown="" data-title-name="[Proximal Operator]">
<p><span class="math display">\[\begin{equation*}
\operatorname*{prox}_{\phi}(x) =
    \operatorname*{argmin}_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
\end{equation*}\]</span></p>
</div>
<p>Inspired by Equation <span class="math inline">\(\eqref{eq:lasso}\)</span>, we produce a toy dataset as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> theano <span class="im">import</span> shared <span class="im">as</span> tt_shared</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3">M <span class="op">=</span> <span class="dv">50</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">M_nonzero <span class="op">=</span> M <span class="op">*</span> <span class="dv">2</span> <span class="op">//</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">beta_true <span class="op">=</span> np.zeros(M)</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">beta_true[:M_nonzero] <span class="op">=</span> np.exp(<span class="op">-</span>np.arange(M_nonzero)) <span class="op">*</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">N <span class="op">=</span> <span class="bu">int</span>(np.alen(beta_true) <span class="op">*</span> <span class="fl">0.4</span>)</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">X <span class="op">=</span> np.random.randn(N, M)</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">mu_true <span class="op">=</span> X.dot(beta_true)</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">y <span class="op">=</span> mu_true <span class="op">+</span> sc.stats.norm.rvs(np.zeros(N), scale<span class="op">=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1-13" data-line-number="13"></a>
<a class="sourceLine" id="cb1-14" data-line-number="14">X_tt <span class="op">=</span> tt_shared(X, name<span class="op">=</span><span class="st">&#39;X&#39;</span>, borrow<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">y_tt <span class="op">=</span> tt_shared(y, name<span class="op">=</span><span class="st">&#39;y&#39;</span>, borrow<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb1-16" data-line-number="16"></a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="co"># Estimation starting parameters...</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18">beta_0 <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>]).astype(<span class="st">&#39;float64&#39;</span>)</a>
<a class="sourceLine" id="cb1-19" data-line-number="19"></a>
<a class="sourceLine" id="cb1-20" data-line-number="20"><span class="co"># Gradient [starting] step size</span></a>
<a class="sourceLine" id="cb1-21" data-line-number="21">alpha_0 <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> np.linalg.norm(X, <span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1-22" data-line-number="22"><span class="co"># np.linalg.matrix_rank(X)</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23"></a>
<a class="sourceLine" id="cb1-24" data-line-number="24"><span class="co"># Regularization value heuristic</span></a>
<a class="sourceLine" id="cb1-25" data-line-number="25"><span class="co"># beta_ols = np.linalg.lstsq(X, y)[0]</span></a>
<a class="sourceLine" id="cb1-26" data-line-number="26"><span class="co"># lambda_max = 0.1 * np.linalg.norm(beta_ols, np.inf)</span></a>
<a class="sourceLine" id="cb1-27" data-line-number="27">lambda_max <span class="op">=</span> np.linalg.norm(X.T.dot(y), np.inf)</a></code></pre></div>
<p>As in <span class="citation" data-cites="willard_role_2017">Willard (2017)</span>, we can start with a model defined within a system like PyMC3 <span class="citation" data-cites="salvatier_probabilistic_2016">(Salvatier, Wiecki, and Fonnesbeck 2016)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="cf">with</span> pm.Model() <span class="im">as</span> lasso_model:</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    beta_rv <span class="op">=</span> pm.Laplace(<span class="st">&#39;beta&#39;</span>, mu<span class="op">=</span><span class="dv">0</span>, b<span class="op">=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">                         shape<span class="op">=</span>X.shape[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">    y_rv <span class="op">=</span> pm.Normal(<span class="st">&#39;y&#39;</span>, mu<span class="op">=</span>X_tt.dot(beta_rv), sd<span class="op">=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">                     shape<span class="op">=</span>y.shape[<span class="dv">0</span>], observed<span class="op">=</span>y_tt)</a></code></pre></div>
<p>In this setting one might then arrive at the necessary steps toward estimation automatically (i.e. identify the underlying <span class="math inline">\(\ell_1\)</span> estimation problem). We discuss this more in <span class="citation" data-cites="willard_role_2017">Willard (2017)</span>.</p>
<p>For simplicity, we’ll just assume that all components of the estimation problem are know–i.e. loss and penalty functions. The proximal operator that arises in this standard example is the <em>soft thresholding</em> operator. In Theano, it can be implemented with the following:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span> tt_soft_threshold(beta_, lambda_):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="cf">return</span> tt.sgn(beta_) <span class="op">*</span> tt.maximum(tt.abs_(beta_) <span class="op">-</span> lambda_, <span class="dv">0</span>)</a></code></pre></div>
<div class="remark" data-markdown="" data-title-name="">
<p>This operator can take other forms and the one used here is not particularly special. For instance, the <code>maximum</code> can be replaced by other conditional-like statements–such as <span class="math display">\[\begin{equation*}
\operatorname{S}(z, \lambda) =
    \begin{cases}
     {\mathop{\mathrm{sgn}}}(\beta) (\beta - \lambda) &amp; \beta &gt; \lambda
     \\
     0 &amp; \text{otherwise}
    \end{cases}
    \;.
\end{equation*}\]</span> If we were to–say–multiply the output of this operator with another more difficult to compute result, then we might also wish to “optimize” this implementation by pushing the multiplication into the definition of the operator and altogether avoid its computation in the <span class="math inline">\(\beta \leq \lambda\)</span> case.</p>
<p>Barring any reuses of this quantity, or a need to preserve undefined results produced by an expensive product with zero, we would really like a “compiler” to make such an optimization itself. It isn’t clear how a standard compiler–or interpreter/hybrid–could safely make this optimization, whereas it does seem more reasonable as a symbolic/Theano optimization.</p>
<p>Optimizations like this are–I think–a necessary step to enable expressive, generalized methods and truly rapid prototyping at the math level.</p>
</div>
<p>Now, assuming that we’ve obtained the relevant loss and penalty functions–for example, in PyMC3–then we can proceed by setting up the exact context of our proximal problem.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="im">from</span> theano <span class="im">import</span> clone <span class="im">as</span> tt_clone</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co"># Clone the negative log-likelihood of our observation model.</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">nlogl_rv <span class="op">=</span> <span class="op">-</span>lasso_model.observed_RVs[<span class="dv">0</span>].logpt</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">nlogl <span class="op">=</span> tt_clone(nlogl_rv)</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">nlogl.name <span class="op">=</span> <span class="st">&quot;-logl&quot;</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">beta_tt <span class="op">=</span> tt_inputs([nlogl])[<span class="dv">4</span>]</a></code></pre></div>
</section>
<section id="proximal-gradient" class="level1">
<h1>Proximal Gradient</h1>
<p>In what follows it will be convenient to generalize a bit and work in terms of arbitrary loss and penalty functions <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span>, respectively, which in our case corresponds to <span class="math display">\[\begin{equation*}
\begin{gathered}
  l(\beta) = \frac12 \|y - X \beta\|^2_2, \quad
  \text{and}\;
  \phi(\beta) = \|\beta\|_1
  \;.\end{gathered}
\end{equation*}\]</span></p>
<p>The proximal gradient <span class="citation" data-cites="combettes_proximal_2011">(Combettes and Pesquet 2011)</span> algorithm is a staple of the proximal framework that provides solutions to problems of the form <span class="math display">\[\begin{equation*}
\operatorname*{argmin}_\beta \left\{
    l(\beta) + \lambda \phi(\beta)
  \right\}
  \;,
\end{equation*}\]</span> when both <span class="math inline">\(l\)</span> and <span class="math inline">\(\phi\)</span> are lower semi-continuous convex functions, and <span class="math inline">\(l\)</span> is differentiable with Lipschitz gradient.</p>
<p>The solution is given as the following fixed-point: <span class="math display">\[\begin{equation}
\beta = \operatorname*{prox}_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}\]</span> The constant step size <span class="math inline">\(\alpha\)</span> is related to the Lipschitz constant of <span class="math inline">\(\nabla l\)</span>, but can also be a sequence obeying certain constraints. Since our <span class="math inline">\(l\)</span> under consideration is <span class="math inline">\(\ell_2\)</span>, we have the incredibly standard <span class="math inline">\(\nabla l(\beta) = X^\top (X \beta - y)\)</span>.</p>
<section id="implementation" class="level2">
<h2>Implementation</h2>
<p>As in <span class="citation" data-cites="willard_role_2017">Willard (2017)</span>, we provide an implementation of a proximal gradient step.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="im">from</span> theano <span class="im">import</span> function <span class="im">as</span> tt_function</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="im">from</span> theano.<span class="bu">compile</span>.nanguardmode <span class="im">import</span> NanGuardMode</a>
<a class="sourceLine" id="cb5-3" data-line-number="3"></a>
<a class="sourceLine" id="cb5-4" data-line-number="4">tt_func_mode <span class="op">=</span> NanGuardMode(nan_is_error<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">                            inf_is_error<span class="op">=</span><span class="va">False</span>,</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">                            big_is_error<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"></a>
<a class="sourceLine" id="cb5-8" data-line-number="8"></a>
<a class="sourceLine" id="cb5-9" data-line-number="9"><span class="kw">def</span> prox_gradient_step(loss, beta_tt, prox_func,</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">                       alpha_tt<span class="op">=</span><span class="va">None</span>, lambda_tt<span class="op">=</span><span class="va">None</span>,</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">                       return_loss_grad<span class="op">=</span><span class="va">False</span>,</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">                       tt_func_kwargs<span class="op">=</span>{<span class="st">&#39;mode&#39;</span>: tt_func_mode}</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">                       ):</a>
<a class="sourceLine" id="cb5-14" data-line-number="14">    <span class="co">r&quot;&quot;&quot; Creates a function that produces a proximal gradient step.</span></a>
<a class="sourceLine" id="cb5-15" data-line-number="15"></a>
<a class="sourceLine" id="cb5-16" data-line-number="16"><span class="co">    Arguments</span></a>
<a class="sourceLine" id="cb5-17" data-line-number="17"><span class="co">    =========</span></a>
<a class="sourceLine" id="cb5-18" data-line-number="18"><span class="co">    loss: TensorVariable</span></a>
<a class="sourceLine" id="cb5-19" data-line-number="19"><span class="co">        Continuously differentiable &quot;loss&quot; function in the objective</span></a>
<a class="sourceLine" id="cb5-20" data-line-number="20"><span class="co">        function.</span></a>
<a class="sourceLine" id="cb5-21" data-line-number="21"><span class="co">    beta_tt: TensorVariable</span></a>
<a class="sourceLine" id="cb5-22" data-line-number="22"><span class="co">        Variable argument of the loss function.</span></a>
<a class="sourceLine" id="cb5-23" data-line-number="23"><span class="co">    prox_fn: function</span></a>
<a class="sourceLine" id="cb5-24" data-line-number="24"><span class="co">        Function that computes the proximal operator for the &quot;penalty&quot;</span></a>
<a class="sourceLine" id="cb5-25" data-line-number="25"><span class="co">        function.  Must take two parameters: the first a</span></a>
<a class="sourceLine" id="cb5-26" data-line-number="26"><span class="co">TensorVariable</span></a>
<a class="sourceLine" id="cb5-27" data-line-number="27"><span class="co">        of the gradient step, the second a float or Scalar value.</span></a>
<a class="sourceLine" id="cb5-28" data-line-number="28"><span class="co">    alpha_tt: float, Scalar (optional)</span></a>
<a class="sourceLine" id="cb5-29" data-line-number="29"><span class="co">        Gradient step size.</span></a>
<a class="sourceLine" id="cb5-30" data-line-number="30"><span class="co">    lambda_tt: float, Scalar (optional)</span></a>
<a class="sourceLine" id="cb5-31" data-line-number="31"><span class="co">        Additional scalar value passed to `prox_fn`.</span></a>
<a class="sourceLine" id="cb5-32" data-line-number="32"><span class="co">        </span><span class="al">TODO</span><span class="co">: Not sure if this should be here; is redundant.</span></a>
<a class="sourceLine" id="cb5-33" data-line-number="33"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-34" data-line-number="34">    loss_grad <span class="op">=</span> tt.grad(loss, wrt<span class="op">=</span>beta_tt)</a>
<a class="sourceLine" id="cb5-35" data-line-number="35">    loss_grad.name <span class="op">=</span> <span class="st">&quot;loss_grad&quot;</span></a>
<a class="sourceLine" id="cb5-36" data-line-number="36"></a>
<a class="sourceLine" id="cb5-37" data-line-number="37">    <span class="cf">if</span> alpha_tt <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb5-38" data-line-number="38">        alpha_tt <span class="op">=</span> tt.scalar(name<span class="op">=</span><span class="st">&#39;alpha&#39;</span>)</a>
<a class="sourceLine" id="cb5-39" data-line-number="39">        alpha_tt.tag.test_value <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb5-40" data-line-number="40">    <span class="cf">if</span> lambda_tt <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb5-41" data-line-number="41">        lambda_tt <span class="op">=</span> tt.scalar(name<span class="op">=</span><span class="st">&#39;lambda&#39;</span>)</a>
<a class="sourceLine" id="cb5-42" data-line-number="42">        lambda_tt.tag.test_value <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb5-43" data-line-number="43"></a>
<a class="sourceLine" id="cb5-44" data-line-number="44">    beta_grad_step <span class="op">=</span> beta_tt <span class="op">-</span> alpha_tt <span class="op">*</span> loss_grad</a>
<a class="sourceLine" id="cb5-45" data-line-number="45">    beta_grad_step.name <span class="op">=</span> <span class="st">&quot;beta_grad_step&quot;</span></a>
<a class="sourceLine" id="cb5-46" data-line-number="46"></a>
<a class="sourceLine" id="cb5-47" data-line-number="47">    prox_grad_step <span class="op">=</span> prox_func(beta_grad_step, lambda_tt <span class="op">*</span> alpha_tt)</a>
<a class="sourceLine" id="cb5-48" data-line-number="48">    prox_grad_step.name <span class="op">=</span> <span class="st">&quot;prox_grad_step&quot;</span></a>
<a class="sourceLine" id="cb5-49" data-line-number="49"></a>
<a class="sourceLine" id="cb5-50" data-line-number="50">    inputs <span class="op">=</span> []</a>
<a class="sourceLine" id="cb5-51" data-line-number="51">    updates <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb5-52" data-line-number="52">    <span class="cf">if</span> <span class="bu">isinstance</span>(beta_tt, tt.sharedvar.SharedVariable):</a>
<a class="sourceLine" id="cb5-53" data-line-number="53">        updates <span class="op">=</span> [(beta_tt, prox_grad_step)]</a>
<a class="sourceLine" id="cb5-54" data-line-number="54">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb5-55" data-line-number="55">        inputs <span class="op">+=</span> [beta_tt]</a>
<a class="sourceLine" id="cb5-56" data-line-number="56">    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(alpha_tt, tt.sharedvar.SharedVariable):</a>
<a class="sourceLine" id="cb5-57" data-line-number="57">        inputs <span class="op">+=</span> [alpha_tt]</a>
<a class="sourceLine" id="cb5-58" data-line-number="58">    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(lambda_tt, tt.sharedvar.SharedVariable):</a>
<a class="sourceLine" id="cb5-59" data-line-number="59">        inputs <span class="op">+=</span> [lambda_tt]</a>
<a class="sourceLine" id="cb5-60" data-line-number="60"></a>
<a class="sourceLine" id="cb5-61" data-line-number="61">    prox_grad_step_fn <span class="op">=</span> tt_function(inputs,</a>
<a class="sourceLine" id="cb5-62" data-line-number="62">                                    prox_grad_step,</a>
<a class="sourceLine" id="cb5-63" data-line-number="63">                                    updates<span class="op">=</span>updates,</a>
<a class="sourceLine" id="cb5-64" data-line-number="64">                                    <span class="op">**</span>tt_func_kwargs)</a>
<a class="sourceLine" id="cb5-65" data-line-number="65"></a>
<a class="sourceLine" id="cb5-66" data-line-number="66">    res <span class="op">=</span> (prox_grad_step_fn,)</a>
<a class="sourceLine" id="cb5-67" data-line-number="67">    <span class="cf">if</span> return_loss_grad:</a>
<a class="sourceLine" id="cb5-68" data-line-number="68">        res <span class="op">+=</span> (loss_grad,)</a>
<a class="sourceLine" id="cb5-69" data-line-number="69"></a>
<a class="sourceLine" id="cb5-70" data-line-number="70">    <span class="cf">return</span> res</a></code></pre></div>
</section>
<section id="step-sizes" class="level2">
<h2>Step Sizes</h2>
<p>A critical aspect of the proximal gradient approach–and most optimizations–involves the use of an appropriate step size, <span class="math inline">\(\alpha\)</span>. The step sizes needn’t always be fixed values and, because of this, we can search for a suitable value during estimation. Furthermore, in some cases, step sizes can be sequences amenable to acceleration techniques <span class="citation" data-cites="beck_fast_2014">(Beck and Teboulle 2014)</span>.</p>
<p>Step sizes–and the values that drive them–have critical connections to the performance of an optimization method and do not simply ensure convergence. In that sense, the power of an implementation can depend on how much support it has for various types of step size sequences and when they can be/are used.</p>
<p>Often, acceptable ranges of step size values are derived from broad properties of the functions involved and their gradients (e.g. Lipschitz). When explicitly parameterized, these properties can give meaning to what some call “tuning parameters”. The connections between function-analytic properties and “tuning parameters” themselves highlight the need for more mathematical coverage/symbolic assessment within implementations. Currently, most tuning parameter act as stand-ins for information that’s theoretically obtained from the know functions.</p>
<p>In this spirit, one particularly relevant direction of work can be found in Theano’s experimental matrix “Hints”. Matrix-property hints like <code>theano.sandbox.linalg.ops.{psd, spectral_radius_bound}</code> are good examples of the machinery needed to automatically determine applicable and efficient <span class="math inline">\(\alpha\)</span> constants and sequences.</p>
<p>For our example, we will simply use backtracking line-search.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">def</span> backtracking_search(beta_, alpha_,</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">                        prox_fn, loss_fn, loss_grad_fn,</a>
<a class="sourceLine" id="cb6-3" data-line-number="3">                        lambda_<span class="op">=</span><span class="dv">1</span>, bt_rate<span class="op">=</span><span class="fl">0.5</span>, obj_tol<span class="op">=</span><span class="fl">1e-5</span>):</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">    <span class="co"># alpha_start = alpha_</span></a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    z <span class="op">=</span> beta_</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    beta_start_ <span class="op">=</span> beta_</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    loss_start_ <span class="op">=</span> loss_fn(beta_)</a>
<a class="sourceLine" id="cb6-8" data-line-number="8">    loss_grad_start_ <span class="op">=</span> loss_grad_fn(beta_)</a>
<a class="sourceLine" id="cb6-9" data-line-number="9">    <span class="cf">while</span> <span class="va">True</span>:</a>
<a class="sourceLine" id="cb6-10" data-line-number="10"></a>
<a class="sourceLine" id="cb6-11" data-line-number="11">        beta_ <span class="op">=</span> beta_start_ <span class="op">-</span> alpha_ <span class="op">*</span> loss_grad_start_</a>
<a class="sourceLine" id="cb6-12" data-line-number="12">        z <span class="op">=</span> prox_fn(beta_, alpha_ <span class="op">*</span> lambda_)</a>
<a class="sourceLine" id="cb6-13" data-line-number="13"></a>
<a class="sourceLine" id="cb6-14" data-line-number="14">        loss_z <span class="op">=</span> loss_fn(z)</a>
<a class="sourceLine" id="cb6-15" data-line-number="15">        step_diff <span class="op">=</span> z <span class="op">-</span> beta_start_</a>
<a class="sourceLine" id="cb6-16" data-line-number="16">        loss_diff <span class="op">=</span> loss_z <span class="op">-</span> loss_start_</a>
<a class="sourceLine" id="cb6-17" data-line-number="17">        line_diff <span class="op">=</span> alpha_ <span class="op">*</span> (loss_diff <span class="op">-</span></a>
<a class="sourceLine" id="cb6-18" data-line-number="18">loss_grad_start_.T.dot(step_diff))</a>
<a class="sourceLine" id="cb6-19" data-line-number="19">        line_diff <span class="op">-=</span> step_diff.T.dot(step_diff) <span class="op">/</span> <span class="fl">2.</span></a>
<a class="sourceLine" id="cb6-20" data-line-number="20"></a>
<a class="sourceLine" id="cb6-21" data-line-number="21">        <span class="cf">if</span> line_diff <span class="op">&lt;=</span> obj_tol:</a>
<a class="sourceLine" id="cb6-22" data-line-number="22">            <span class="cf">return</span> z, alpha_, loss_z</a>
<a class="sourceLine" id="cb6-23" data-line-number="23"></a>
<a class="sourceLine" id="cb6-24" data-line-number="24">        alpha_ <span class="op">*=</span> bt_rate</a>
<a class="sourceLine" id="cb6-25" data-line-number="25">        <span class="cf">assert</span> alpha_ <span class="op">&gt;=</span> <span class="dv">0</span>, <span class="st">&#39;invalid step size: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(alpha_)</a></code></pre></div>
<div class="remark" data-markdown="" data-title-name="">
<p>Routines–like this–that make use of the gradient and other quantities might also be good candidates for execution in Theano, if only for the graph optimizations that are able to remedy obviously redundant computations.</p>
<p>In this vein, we could consider performing the line-search, and/or the entire optimization loop, within a Theano <code>scan</code> operation. We could also create an <code>Op</code> that represents gradient and line-search steps. These might make graph construction much simpler and be more suited for the current optimization framework.</p>
<p>Although there’s no guarantee that <code>scan</code> and tighter Theano integrations will always produce better results than our current implementation, we wish to emphasize that it’s possible–given work in these symbolic directions.</p>
<p>Likewise, an <code>Op</code> for the proximal operator might also be necessary for solving many proximal operators found within a log-likelihood/objective function graph automatically and in closed-form. An effective implementation could be as simple as the use of lookup tables combined with some algebraic relationships/identities. State-of-the-art symbolic algebra libraries effectively use the same approach for symbolic integration.</p>
</div>
</section>
</section>
<section id="examples" class="level1">
<h1>Examples</h1>
<p>First, to compute anything from our Theano graphs, we need to compile them to Theano functions.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1">lambda_tt <span class="op">=</span> tt.scalar(<span class="st">&#39;lambda&#39;</span>)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">lambda_tt.tag.test_value <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">prox_fn <span class="op">=</span> tt_function([beta_tt, lambda_tt],</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">                      tt_soft_threshold(beta_tt, lambda_tt))</a>
<a class="sourceLine" id="cb7-6" data-line-number="6"></a>
<a class="sourceLine" id="cb7-7" data-line-number="7">prox_grad_step_fn, loss_grad <span class="op">=</span> prox_gradient_step(</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">    nlogl, beta_tt, tt_soft_threshold,</a>
<a class="sourceLine" id="cb7-9" data-line-number="9">    return_loss_grad<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb7-10" data-line-number="10"></a>
<a class="sourceLine" id="cb7-11" data-line-number="11">loss_fn <span class="op">=</span> tt_function([beta_tt], nlogl)</a>
<a class="sourceLine" id="cb7-12" data-line-number="12">loss_grad_fn <span class="op">=</span> tt_function([beta_tt], loss_grad)</a>
<a class="sourceLine" id="cb7-13" data-line-number="13"></a>
<a class="sourceLine" id="cb7-14" data-line-number="14">cols_fns <span class="op">=</span> [</a>
<a class="sourceLine" id="cb7-15" data-line-number="15">    (<span class="kw">lambda</span> i, b: i, <span class="vs">r&#39;$i$&#39;</span>),</a>
<a class="sourceLine" id="cb7-16" data-line-number="16">    (<span class="kw">lambda</span> i, b: np.asscalar(loss_fn(b)),</a>
<a class="sourceLine" id="cb7-17" data-line-number="17">        <span class="vs">r&#39;$l(\beta^{(i)})$&#39;</span>),</a>
<a class="sourceLine" id="cb7-18" data-line-number="18">    (<span class="kw">lambda</span> i, b: np.linalg.norm(b <span class="op">-</span> beta_true, <span class="dv">2</span>),</a>
<a class="sourceLine" id="cb7-19" data-line-number="19">        <span class="vs">r&#39;$\|\beta^{(i)} - \beta^*\|^2_2$&#39;</span>)</a>
<a class="sourceLine" id="cb7-20" data-line-number="20">]</a></code></pre></div>
<p>For a baseline comparison–and sanity check–we’ll use the <code>cvxpy</code> library <span class="citation" data-cites="diamond_cvxpy:_2016">(Diamond and Boyd 2016)</span>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="im">import</span> cvxpy <span class="im">as</span> cvx</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">beta_var_cvx <span class="op">=</span> cvx.Variable(M, name<span class="op">=</span><span class="st">&#39;beta&#39;</span>)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">lambda_cvx <span class="op">=</span> <span class="fl">1e-2</span> <span class="op">*</span> lambda_max <span class="op">*</span> N</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"></a>
<a class="sourceLine" id="cb8-6" data-line-number="6">cvx_obj <span class="op">=</span> cvx.Minimize(<span class="fl">0.5</span> <span class="op">*</span> cvx.sum_squares(y <span class="op">-</span> X <span class="op">*</span> beta_var_cvx)</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">                       <span class="op">+</span> lambda_cvx <span class="op">*</span> cvx.norm(beta_var_cvx, <span class="dv">1</span>) )</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">cvx_prob <span class="op">=</span> cvx.Problem(cvx_obj)</a>
<a class="sourceLine" id="cb8-9" data-line-number="9"></a>
<a class="sourceLine" id="cb8-10" data-line-number="10">_ <span class="op">=</span> cvx_prob.solve(solver<span class="op">=</span>cvx.CVXOPT, verbose<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb8-11" data-line-number="11"></a>
<a class="sourceLine" id="cb8-12" data-line-number="12">beta_cvx <span class="op">=</span> np.asarray(beta_var_cvx.value).squeeze()</a>
<a class="sourceLine" id="cb8-13" data-line-number="13">loss_cvx <span class="op">=</span> loss_fn(beta_cvx)</a>
<a class="sourceLine" id="cb8-14" data-line-number="14">beta_cvx_err <span class="op">=</span> np.linalg.norm(beta_cvx <span class="op">-</span> beta_true, <span class="dv">2</span>)</a></code></pre></div>
<p>We now have the necessary pieces to perform an example estimation. We’ll start with an exceedingly large step size and let backtracking line-search find a good value.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">class</span> ProxGradient(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb9-2" data-line-number="2"></a>
<a class="sourceLine" id="cb9-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, y, X, beta_0,</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">                 prox_fn_, loss_fn_, loss_grad_fn_,</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">                 alpha_0):</a>
<a class="sourceLine" id="cb9-6" data-line-number="6"></a>
<a class="sourceLine" id="cb9-7" data-line-number="7">        <span class="va">self</span>.y <span class="op">=</span> y</a>
<a class="sourceLine" id="cb9-8" data-line-number="8">        <span class="va">self</span>.X <span class="op">=</span> X</a>
<a class="sourceLine" id="cb9-9" data-line-number="9">        <span class="va">self</span>.alpha_val <span class="op">=</span> alpha_0</a>
<a class="sourceLine" id="cb9-10" data-line-number="10">        <span class="va">self</span>.beta_0 <span class="op">=</span> beta_0</a>
<a class="sourceLine" id="cb9-11" data-line-number="11">        <span class="va">self</span>.N, <span class="va">self</span>.M <span class="op">=</span> X.shape</a>
<a class="sourceLine" id="cb9-12" data-line-number="12">        <span class="va">self</span>.prox_fn_ <span class="op">=</span> prox_fn_</a>
<a class="sourceLine" id="cb9-13" data-line-number="13">        <span class="va">self</span>.loss_fn_ <span class="op">=</span> loss_fn_</a>
<a class="sourceLine" id="cb9-14" data-line-number="14">        <span class="va">self</span>.loss_grad_fn_ <span class="op">=</span> loss_grad_fn_</a>
<a class="sourceLine" id="cb9-15" data-line-number="15"></a>
<a class="sourceLine" id="cb9-16" data-line-number="16">    <span class="kw">def</span> step(<span class="va">self</span>, beta):</a>
<a class="sourceLine" id="cb9-17" data-line-number="17">        beta_val <span class="op">=</span> np.copy(beta)</a>
<a class="sourceLine" id="cb9-18" data-line-number="18"></a>
<a class="sourceLine" id="cb9-19" data-line-number="19">        beta_val, <span class="va">self</span>.alpha_val, _ <span class="op">=</span> backtracking_search(</a>
<a class="sourceLine" id="cb9-20" data-line-number="20">            beta_val, <span class="va">self</span>.alpha_val,</a>
<a class="sourceLine" id="cb9-21" data-line-number="21">            <span class="va">self</span>.prox_fn_, <span class="va">self</span>.loss_fn_, <span class="va">self</span>.loss_grad_fn_)</a>
<a class="sourceLine" id="cb9-22" data-line-number="22"></a>
<a class="sourceLine" id="cb9-23" data-line-number="23">        <span class="cf">return</span> beta_val</a></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1">beta_0 <span class="op">=</span> np.zeros(M).astype(<span class="st">&#39;float64&#39;</span>)</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">lambda_val <span class="op">=</span> <span class="fl">1e-2</span> <span class="op">*</span> lambda_max</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">pg_step <span class="op">=</span> ProxGradient(y, X, beta_0,</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">                       <span class="kw">lambda</span> x, a: prox_fn(x, N <span class="op">*</span> lambda_val <span class="op">*</span> a),</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">                       loss_fn, loss_grad_fn, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb10-6" data-line-number="6"></a>
<a class="sourceLine" id="cb10-7" data-line-number="7">pg_cols_fns <span class="op">=</span> cols_fns <span class="op">+</span> [(<span class="kw">lambda</span> <span class="op">*</span>args, <span class="op">**</span>kwargs: pg_step.alpha_val,</a>
<a class="sourceLine" id="cb10-8" data-line-number="8"><span class="vs">r&#39;$\alpha$&#39;</span>)]</a>
<a class="sourceLine" id="cb10-9" data-line-number="9">pg_est_data, _ <span class="op">=</span> iterative_run(pg_step, loss_fn, pg_cols_fns)</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">pg_ls_data <span class="op">=</span> pd.DataFrame(pg_est_data)</a>
<a class="sourceLine" id="cb10-11" data-line-number="11"><span class="co"># pg_ls_data = pg_ls_data.append(pg_est_data, ignore_index=True)</span></a></code></pre></div>
<p><span id="fig:pg_ls_plot"><span id="fig:pg_ls_plot_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{1}\label{fig:pg_ls_plot}\end{equation}\]</span></span><img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_ls_plot_1.png" title="fig:" alt="Minimization by proximal gradient with backtracking line-search." /></span></p>
<p>Figure <span class="math inline">\(\ref{fig:pg_ls_plot}\)</span> shows a couple convergence measures for proximal gradient steps alongside the step size changes due to backtracking line-search. Regarding the latter, in our example a sufficient step size is found within the first few iterations, so the overall result isn’t too interesting. Fortunately, this sort of behaviour isn’t uncommon, which makes line-search quite effective in practice.</p>
</section>
<section id="coordinate-wise-estimation" class="level1">
<h1>Coordinate-wise Estimation</h1>
<p>Given that our loss is a composition of <span class="math inline">\(\ell_2\)</span> and a linear operator of finite dimension (i.e. <span class="math inline">\(X\)</span>), we can conveniently exploit conditional separability and obtain simple estimation steps in each coordinate. This is, effectively, what characterizes coordinate–or cyclic–descent. Since it is a common technique in the estimation of <span class="math inline">\(\ell_1\)</span> models <span class="citation" data-cites="friedman_pathwise_2007 mazumder_regularization_2009 scikit-learn_sklearn.linear_model.elasticnet_2017">(Friedman et al. 2007; Mazumder, Hastie, and Tibshirani 2009; scikit-learn 2017)</span>, it’s worthwhile to consider how it can viewed in terms of proximal operators.</p>
<p>From a statistical perspective, the basics of coordinate-wise methods begin with the “partial residuals”, <span class="math inline">\(r_{-m} \in {{\mathbb{R}}}^{N}\)</span> discussed in <span class="citation" data-cites="friedman_pathwise_2007">Friedman et al. (2007)</span>, and implicitly defined by <span class="math display">\[\begin{equation}
\begin{aligned}
    \beta^*
    &amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|
    y - X(\beta - e_m \beta_m)
        - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
      \right\}
    \\
    &amp;= \operatorname*{argmin}_{\beta} \left\{
      \frac12
      \|r_{-m} - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \dots
    \right\}
  \;.
  \end{aligned}
  \label{eq:partial_resid}
\end{equation}\]</span> The last expression hints at the most basic idea behind the coordinate-wise approach: conditional minimization in each <span class="math inline">\(m\)</span>. Its exact solution in each coordinate is given by the aforementioned soft-thresholding function, which–as we’ve already stated–is a proximal operator. In symbols, <span class="math inline">\(\operatorname*{prox}_{\lambda \left|\cdot\right|}(x) = \operatorname{S}_\lambda(x)\)</span>, where the latter is the soft-thresholding operator.</p>
<p>Now, we can relate Equation <span class="math inline">\(\eqref{eq:partial_resid}\)</span> to proximal methods through the proximal gradient fixed-point solution–i.e. Equation <span class="math inline">\(\eqref{eq:forward-backward}\)</span>–and the following property of proximal operators:</p>
<div id="lem:prox_ortho_basis" class="lemma" data-markdown="" data-title-name="">
<p><span id="lem:prox_ortho_basis_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{1}\label{lem:prox_ortho_basis}\end{equation}\]</span></span></p>
<p><span class="math display">\[\begin{equation*}
\operatorname*{prox}_{\lambda \phi \circ e^\top_m}(z) =
    \sum^M_m \operatorname*{prox}_{\lambda \phi}\left(e^\top_m z\right) e_m
    \;.
\end{equation*}\]</span></p>
<div class="proof" data-markdown="" data-title-name="">
<p>See <span class="citation" data-cites="chaux_variational_2007">Chaux et al. (2007)</span>.</p>
</div>
</div>
<p>The next result yields our desired connection.</p>
<div id="eq:prox_grad_descent" class="proposition" data-markdown="" data-title-name="">
<p><span id="eq:prox_grad_descent_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{1}\label{eq:prox_grad_descent}\end{equation}\]</span></span></p>
<p>For <span class="math inline">\(X\)</span> such that <span class="math inline">\({{\bf 1}}^\top X e_m = 0\)</span> and <span class="math inline">\(e^\top_m X^\top X e_m = 1\)</span>, <span class="math inline">\(m \in \{1, \dots, M\}\)</span>, the coordinate-wise step of the Lasso in <span class="citation" data-cites="friedman_pathwise_2007">Friedman et al. (2007 Equation (9))</span>, <span class="math display">\[\begin{equation*}
\beta_m = \operatorname{S}_{\lambda}\left[
      \sum_{n}^N X_{n,m} \left(
      y_n - \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime}
      \right)
    \right]
    \;,
\end{equation*}\]</span> has a proximal gradient fixed-point solution under a Euclidean basis decomposition with the form <span class="math display">\[\begin{equation*}
\beta =
    \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
      e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
    \right] e_m
    \;.
\end{equation*}\]</span></p>
<div class="proof" data-markdown="" data-title-name="">
<p>We start with an expansion of the terms in <span class="math inline">\(\operatorname*{prox}_{\lambda \phi} \equiv \operatorname{S}_\lambda\)</span>. After simplifying the notation with <span class="math display">\[\begin{equation*}
\begin{gathered}
    \sum^N_{n} X_{n,m} z_n = e^\top_m X^\top z, \quad \text{and} \quad
    \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime} =
    X \left(\beta - \beta_m e_m \right)
    \;,
  \end{gathered}
\end{equation*}\]</span> the expanded argument of <span class="math inline">\(\operatorname{S}\)</span> reduces to <span class="math display">\[\begin{equation*}
\begin{aligned}
      e^\top_m X^\top \left(y - X\left( \beta - e_m \beta_m\right)\right)
      &amp;= e^\top_m X^\top X e_m \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;= \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &amp;= e^\top_m \left(\beta + X^\top \left(y - X \beta\right)\right)
    \end{aligned}
\end{equation*}\]</span> where the last step follows from <span class="math inline">\(X\)</span> standardization. This establishes the relationship with Equation <span class="math inline">\(\eqref{eq:forward-backward}\)</span> only component-wise. Using Lemma <span class="math inline">\(\eqref{lem:prox_ortho_basis}\)</span> together with <span class="math inline">\(z = \beta - \alpha \nabla  l(\beta)\)</span> yields the proximal gradient fixed-point statement, i.e. <span class="math display">\[\begin{equation*}
\begin{aligned}
      \beta
      &amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left[
    e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
      \right] e_m
      \\
      &amp;=
      \sum^M_m \operatorname*{prox}_{\alpha \lambda \phi}\left(
      \beta_m + \alpha e_m^\top X^\top \left(y - X \beta \right)
      \right) e_m
      \;.
    \end{aligned}
\end{equation*}\]</span></p>
</div>
</div>
<div id="rem:bases" class="remark" data-markdown="" data-title-name="">
<p><span id="rem:bases_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{3}\label{rem:bases}\end{equation}\]</span></span></p>
<p>The property in Lemma <span class="math inline">\(\eqref{lem:prox_ortho_basis}\)</span> can be used with other orthonormal bases–providing yet another connection between proximal methods and established dimensionality reduction and sparse estimation techniques <span class="citation" data-cites="chaux_variational_2007">(Chaux et al. 2007)</span>. Also, this property provides a neat way to think about <span class="math inline">\(X\)</span>-based orthogonalizations in estimations for regression and grouped-penalization problems.</p>
</div>
<section id="implementation-1" class="level2">
<h2>Implementation</h2>
<p>The following performs a standard form of coordinate descent:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">class</span> CoordDescent(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb11-2" data-line-number="2"></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, y, X, beta_0, prox_fn_, col_seq<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb11-4" data-line-number="4"></a>
<a class="sourceLine" id="cb11-5" data-line-number="5">        <span class="va">self</span>.y <span class="op">=</span> y</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">        <span class="va">self</span>.X <span class="op">=</span> X</a>
<a class="sourceLine" id="cb11-7" data-line-number="7">        <span class="va">self</span>.beta_0 <span class="op">=</span> beta_0</a>
<a class="sourceLine" id="cb11-8" data-line-number="8">        <span class="va">self</span>.N, <span class="va">self</span>.M <span class="op">=</span> X.shape</a>
<a class="sourceLine" id="cb11-9" data-line-number="9">        <span class="va">self</span>.Xb <span class="op">=</span> np.dot(<span class="va">self</span>.X, <span class="va">self</span>.beta_0)</a>
<a class="sourceLine" id="cb11-10" data-line-number="10">        <span class="va">self</span>.prox_fn_ <span class="op">=</span> prox_fn_</a>
<a class="sourceLine" id="cb11-11" data-line-number="11"></a>
<a class="sourceLine" id="cb11-12" data-line-number="12">        <span class="co"># (Inverse) 2-norm of each column/feature, i.e.</span></a>
<a class="sourceLine" id="cb11-13" data-line-number="13">        <span class="co">#   np.reciprocal(np.diag(np.dot(X.T, X)))</span></a>
<a class="sourceLine" id="cb11-14" data-line-number="14">        <span class="va">self</span>.alpha_vals <span class="op">=</span> np.reciprocal((<span class="va">self</span>.X<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</a>
<a class="sourceLine" id="cb11-15" data-line-number="15"></a>
<a class="sourceLine" id="cb11-16" data-line-number="16">        <span class="cf">if</span> col_seq <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb11-17" data-line-number="17">            <span class="va">self</span>.col_seq <span class="op">=</span> np.arange(<span class="va">self</span>.M)</a>
<a class="sourceLine" id="cb11-18" data-line-number="18"></a>
<a class="sourceLine" id="cb11-19" data-line-number="19">    <span class="kw">def</span> reset(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb11-20" data-line-number="20">        <span class="va">self</span>.Xb <span class="op">=</span> np.dot(<span class="va">self</span>.X, <span class="va">self</span>.beta_0)</a>
<a class="sourceLine" id="cb11-21" data-line-number="21"></a>
<a class="sourceLine" id="cb11-22" data-line-number="22">    <span class="kw">def</span> step(<span class="va">self</span>, beta):</a>
<a class="sourceLine" id="cb11-23" data-line-number="23">        beta_val <span class="op">=</span> np.copy(beta)</a>
<a class="sourceLine" id="cb11-24" data-line-number="24"></a>
<a class="sourceLine" id="cb11-25" data-line-number="25">        <span class="cf">for</span> j <span class="kw">in</span> <span class="va">self</span>.col_seq:</a>
<a class="sourceLine" id="cb11-26" data-line-number="26">            X_j <span class="op">=</span> <span class="va">self</span>.X[:, j]</a>
<a class="sourceLine" id="cb11-27" data-line-number="27">            alpha_val <span class="op">=</span> <span class="va">self</span>.alpha_vals[j]</a>
<a class="sourceLine" id="cb11-28" data-line-number="28"></a>
<a class="sourceLine" id="cb11-29" data-line-number="29">            <span class="co"># A little cheaper to just subtract the column&#39;s</span></a>
<a class="sourceLine" id="cb11-30" data-line-number="30">contribution...</a>
<a class="sourceLine" id="cb11-31" data-line-number="31">            <span class="va">self</span>.Xb <span class="op">-=</span> X_j <span class="op">*</span> beta_val[j]</a>
<a class="sourceLine" id="cb11-32" data-line-number="32"></a>
<a class="sourceLine" id="cb11-33" data-line-number="33">            Xt_r <span class="op">=</span> np.dot(X_j.T, <span class="va">self</span>.y <span class="op">-</span> <span class="va">self</span>.Xb) <span class="op">*</span> alpha_val</a>
<a class="sourceLine" id="cb11-34" data-line-number="34">            beta_val[j] <span class="op">=</span> <span class="va">self</span>.prox_fn_(np.atleast_1d(Xt_r),</a>
<a class="sourceLine" id="cb11-35" data-line-number="35">alpha_val)</a>
<a class="sourceLine" id="cb11-36" data-line-number="36"></a>
<a class="sourceLine" id="cb11-37" data-line-number="37">            <span class="co"># ...and add the updated column back.</span></a>
<a class="sourceLine" id="cb11-38" data-line-number="38">            <span class="va">self</span>.Xb <span class="op">+=</span> X_j <span class="op">*</span> beta_val[j]</a>
<a class="sourceLine" id="cb11-39" data-line-number="39"></a>
<a class="sourceLine" id="cb11-40" data-line-number="40">        <span class="va">self</span>.beta_last <span class="op">=</span> beta_val</a>
<a class="sourceLine" id="cb11-41" data-line-number="41"></a>
<a class="sourceLine" id="cb11-42" data-line-number="42">        <span class="cf">return</span> beta_val</a></code></pre></div>
<p>Our example randomizes the order of coordinates to loosely demonstrate the range of efficiency possible in coordinate descent.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">beta_0 <span class="op">=</span> np.zeros(M).astype(<span class="st">&#39;float64&#39;</span>)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">lambda_val <span class="op">=</span> <span class="fl">1e-2</span> <span class="op">*</span> lambda_max</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">cd_step <span class="op">=</span> CoordDescent(y, X, beta_0,</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">                       <span class="kw">lambda</span> x, a: prox_fn(x, N <span class="op">*</span> lambda_val <span class="op">*</span> a))</a>
<a class="sourceLine" id="cb12-5" data-line-number="5"></a>
<a class="sourceLine" id="cb12-6" data-line-number="6">cd_cols_fns <span class="op">=</span> cols_fns <span class="op">+</span> [(<span class="kw">lambda</span> <span class="op">*</span>args, <span class="op">**</span>kwargs: j, <span class="st">&quot;replication&quot;</span>)]</a>
<a class="sourceLine" id="cb12-7" data-line-number="7"></a>
<a class="sourceLine" id="cb12-8" data-line-number="8">pg_coord_data <span class="op">=</span> pd.DataFrame()</a>
<a class="sourceLine" id="cb12-9" data-line-number="9"><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>):</a>
<a class="sourceLine" id="cb12-10" data-line-number="10">    est_data, _ <span class="op">=</span> iterative_run(cd_step, loss_fn, cd_cols_fns)</a>
<a class="sourceLine" id="cb12-11" data-line-number="11">    pg_coord_data <span class="op">=</span> pg_coord_data.append(est_data,</a>
<a class="sourceLine" id="cb12-12" data-line-number="12">                                         ignore_index<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb12-13" data-line-number="13">    <span class="co"># Reset internal state of our step method, since we&#39;re</span></a>
<a class="sourceLine" id="cb12-14" data-line-number="14">    <span class="co"># running multiple replications.</span></a>
<a class="sourceLine" id="cb12-15" data-line-number="15">    cd_step.reset()</a>
<a class="sourceLine" id="cb12-16" data-line-number="16">    np.random.shuffle(cd_step.col_seq)</a></code></pre></div>
<p><span id="fig:pg_coord_plot"><span id="fig:pg_coord_plot_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{2}\label{fig:pg_coord_plot}\end{equation}\]</span></span><img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_coord_plot_1.png" title="fig:" alt="Minimization by coordinate descent." /></span></p>
<p>Figure <span class="math inline">\(\ref{fig:pg_coord_plot}\)</span> shows convergence measures for each randomized coordinate order. The [average] difference in the number of iterations required for coordinate descent and proximal gradient is fairly noticeable. Nonetheless, both reach effectively the same limits.</p>
<div class="remark" data-markdown="" data-title-name="">
<p>Similar ideas behind batched vs. non-batched steps and block sampling–found within the Gibbs sampling literature <span class="citation" data-cites="roberts_updating_1997">(Roberts and Sahu 1997)</span>–could explain the variation due to coordinate order and the relative efficiency of coordinate descent. There are also connections with our comments in Remark <span class="math inline">\(\ref{rem:bases}\)</span> and, to some extent, stochastic gradient descent (SGD) <span class="citation" data-cites="bertsekas_incremental_2010">(Bertsekas 2010)</span>.</p>
<p>In a woefully lacking over-generalization, let’s say that it comes down to the [spectral] properties of the composite operator(s) <span class="math inline">\(l \circ X\)</span> and/or <span class="math inline">\(\nabla l \circ X\)</span>. These determine the bounds of efficiency for steps in certain directions and how blocking or partitioning the dimensions of <span class="math inline">\(\beta\)</span> nears or distances from those bounds.</p>
</div>
<section id="regularization-paths" class="level3">
<h3>Regularization Paths</h3>
<p>Also, due to the relatively fast convergence of coordinate descent, the method is a little more suitable for the computation of regularization paths– i.e. varying <span class="math inline">\(\lambda\)</span> between iterations. There is much more to this topic, but for simplicity let’s just note that each <span class="math inline">\(\lambda\)</span> step has a “warm-start” from the previous descent iteration–which helps–and that we’re otherwise fine with the solution provided by this approach.</p>
<p>Next, we make a small extension to demonstrate the computation of regularization paths–using <code>lasso_path</code> for comparison.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="im">from</span> sklearn.linear_model <span class="im">import</span> lasso_path, enet_path</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">beta_0 <span class="op">=</span> np.zeros(M).astype(<span class="st">&#39;float64&#39;</span>)</a>
<a class="sourceLine" id="cb13-4" data-line-number="4"></a>
<a class="sourceLine" id="cb13-5" data-line-number="5">lambda_path, beta_path, _ <span class="op">=</span> lasso_path(X, y)</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">path_len <span class="op">=</span> np.alen(lambda_path)</a>
<a class="sourceLine" id="cb13-7" data-line-number="7"></a>
<a class="sourceLine" id="cb13-8" data-line-number="8">beta_last <span class="op">=</span> beta_0</a>
<a class="sourceLine" id="cb13-9" data-line-number="9">pg_path_data <span class="op">=</span> pd.DataFrame()</a>
<a class="sourceLine" id="cb13-10" data-line-number="10"><span class="cf">for</span> i, lambda_ <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_path):</a>
<a class="sourceLine" id="cb13-11" data-line-number="11">    cd_path_step <span class="op">=</span> CoordDescent(y, X, beta_last,</a>
<a class="sourceLine" id="cb13-12" data-line-number="12">                        <span class="kw">lambda</span> x, a: prox_fn(x, N <span class="op">*</span> lambda_ <span class="op">*</span> a))</a>
<a class="sourceLine" id="cb13-13" data-line-number="13"></a>
<a class="sourceLine" id="cb13-14" data-line-number="14">    cd_cols_fns <span class="op">=</span> cols_fns[<span class="dv">1</span>:] <span class="op">+</span> [</a>
<a class="sourceLine" id="cb13-15" data-line-number="15">        (<span class="kw">lambda</span> <span class="op">*</span>args, <span class="op">**</span>kwargs: lambda_, <span class="vs">r&#39;$\lambda$&#39;</span>)]</a>
<a class="sourceLine" id="cb13-16" data-line-number="16">    est_data, beta_last <span class="op">=</span> iterative_run(cd_path_step, loss_fn,</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">                                        cd_cols_fns,</a>
<a class="sourceLine" id="cb13-18" data-line-number="18">                                        stop_tol<span class="op">=</span><span class="fl">1e-4</span>,</a>
<a class="sourceLine" id="cb13-19" data-line-number="19">                                        stop_loss<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb13-20" data-line-number="20"></a>
<a class="sourceLine" id="cb13-21" data-line-number="21">    pg_path_data <span class="op">=</span> pg_path_data.append(est_data.iloc[<span class="op">-</span><span class="dv">1</span>, :],</a>
<a class="sourceLine" id="cb13-22" data-line-number="22">                                       ignore_index<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">cd_cols_fns <span class="op">=</span> cols_fns[<span class="dv">1</span>:] <span class="op">+</span> [</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">    (<span class="kw">lambda</span> <span class="op">*</span>args, <span class="op">**</span>kwargs: lambda_path[args[<span class="dv">0</span>]], <span class="vs">r&#39;$\lambda$&#39;</span>)]</a>
<a class="sourceLine" id="cb14-3" data-line-number="3"></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">iter_values <span class="op">=</span> []</a>
<a class="sourceLine" id="cb14-5" data-line-number="5"><span class="cf">for</span> i, beta_ <span class="kw">in</span> <span class="bu">enumerate</span>(beta_path.T):</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">    iter_values.append([col_fn(i, beta_)</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">                        <span class="cf">for</span> col_fn, _ <span class="kw">in</span> cd_cols_fns])</a>
<a class="sourceLine" id="cb14-8" data-line-number="8"></a>
<a class="sourceLine" id="cb14-9" data-line-number="9">sklearn_path_data <span class="op">=</span> pd.DataFrame(iter_values,</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">                                 columns<span class="op">=</span><span class="bu">zip</span>(<span class="op">*</span>cd_cols_fns)[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">sklearn_path_data <span class="op">=</span> sklearn_path_data.assign(</a>
<a class="sourceLine" id="cb14-12" data-line-number="12">    replication<span class="op">=</span><span class="va">None</span>, <span class="bu">type</span><span class="op">=</span><span class="st">&#39;sklearn&#39;</span>)</a>
<a class="sourceLine" id="cb14-13" data-line-number="13"></a>
<a class="sourceLine" id="cb14-14" data-line-number="14">pg_path_data <span class="op">=</span> pg_path_data.assign(<span class="bu">type</span><span class="op">=</span><span class="st">&#39;pg&#39;</span>)</a>
<a class="sourceLine" id="cb14-15" data-line-number="15">pg_path_data <span class="op">=</span> pg_path_data.append(sklearn_path_data,</a>
<a class="sourceLine" id="cb14-16" data-line-number="16">                                   ignore_index<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<p><span id="fig:pg_path_plot"><span id="fig:pg_path_plot_span" style="display:none;visibility:hidden"><span class="math display">\[\begin{equation}\tag{3}\label{fig:pg_path_plot}\end{equation}\]</span></span><img src="https://brandonwillard.github.io/figures/more_proximal_estimation_pg_path_plot_1.png" title="fig:" alt="Regularization paths via coordinate descent." /></span></p>
</section>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Among the changes discussed earlier regarding Theano <code>Op</code>s for the proximal objects used here, we would also like to motivate much larger changes to the applied mathematician/statistician’s standard tools by demonstrating the relevance of less common–yet increasingly useful–abstractions. For instance, the proximal methods are neatly framed within operator theory and set-valued analysis, where concepts like the resolvent, sub-differential/gradient and others are common. Abstractions like these provide a compact means of extending familiar ideas into new contexts–such as non-differentiable functions.</p>
<p>Unfortunately, numerical libraries do not provide much in the way of utilizing these abstractions. Most are strictly founded in the representation of point-valued mappings, which can require significant work-arounds to handle even the most basic non-differentiable functions (e.g. the absolute value within our example problem). Our use of the proximal framework is, in part, motivated by its near seamless use <em>and</em> simultaneous bypassing of set-valued maps–in implementation, at least.</p>
<p>There is no fundamental restriction blocking support for set-valued maps, however–aside from the necessary labor and community interest. Even minimal support could provide a context that makes frameworks like ours merely minor abstractions. A similar idea can be found in the symbolic calculation of limits via filters <span class="citation" data-cites="beeson_meaning_2005">(Beeson and Wiedijk 2005)</span>. Perhaps we can liken these changes to the modern evolution of linear algebra libraries to tensor libraries.</p>
<p>We would also like to stress that the value provided by the symbolic tools discussed here (Theano, really) are not <em>just</em> in their ability to act as compilers at a “math level”, but more for their ability to concretely encode mathematical characterizations of optimization problems and methods. Work in this direction is not new by any means; however, the combination of open-source tools and industry interest in algorithms that fall under the broad class of proximal methods (e.g. gradient descent, ADMM, EM, etc.) provides a more immediate reason to pursue these abstractions in code and automate their use.</p>
<p>Regarding the proximal methods, we can consider Theano optimizations that make direct use of the orthonormal basis property in Lemma <span class="math inline">\(\eqref{lem:prox_ortho_basis}\)</span>, or the Moreau-Fenchel theorem, and automate consideration for various estimation methods via splitting (e.g. ADMM, Douglas-Rachford, etc.)–perhaps by making decisions based on inferred or specified tensor, function, and operator properties. In future installments we’ll delve into the details of these ideas.</p>
<p><span class="citation" data-cites="wytock_new_2016">(Wytock et al. 2016)</span> also discuss similar ideas in an optimization setting, such as the use of symbolic graphs and a close coupling with useful mathematical abstractions–including proximal operators. Additionally, there are many other good examples <span class="citation" data-cites="diamond_cvxpy:_2016">(Diamond and Boyd 2016)</span> of constructive mathematical abstractions applied in code.</p>
<p>In most cases, libraries providing optimization tools and supporting model estimation do not attempt to root their implementations within an independently developed symbolic framework and then realize their relevant methodologies in that context. Too often the mathematical abstractions–or the resulting methods alone–are directly implemented at the highest levels of abstraction possible. This is what we see as the result of popular libraries like <code>scikit-learn</code> and the body of <code>R</code> packages. One can also find the same efforts for proximal methods themselves–e.g. in <span class="citation" data-cites="svaiter_pyprox_2017">(svaiter 2017)</span>, where individual functions for ADMM, forward-backward/proximal gradient and Douglas-Rachford are the end result. This is the most common approach and it makes sense in terms of simplicity, but offers very little of the extensibility, generalization, or efficiencies provided by shared efforts across related projects and fields.</p>
<p>In the context of Theano, implementations immediately benefit from its code conversion, parallelization and relevant improvements to its basic graph optimizations. The latter covers both low-level computational efficiency–such as relevant application of BLAS functions–and high-level tensor algebra simplifications.</p>
<p>In a development community that builds on these tools, related efficiency and performance gains can occur much more often, without necessarily sacrificing the specificity inherent to certain areas of application. For example, we can safely use the Rao-Blackwell theorem as the basis of a graph optimization in PyMC3, so it could be included among that project’s default offerings; however, it would be far too cumbersome to use productively in a less specific context.</p>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-beck_fast_2014">
<p>Beck, Amir, and Marc Teboulle. 2014. “A Fast Dual Proximal Gradient Algorithm for Convex Minimization and Applications.” <em>Operations Research Letters</em> 42 (1): 1–6. <a href="http://www.sciencedirect.com/science/article/pii/S0167637713001454" class="uri">http://www.sciencedirect.com/science/article/pii/S0167637713001454</a>.</p>
</div>
<div id="ref-beeson_meaning_2005">
<p>Beeson, Michael, and Freek Wiedijk. 2005. “The Meaning of Infinity in Calculus and Computer Algebra Systems.” <em>Journal of Symbolic Computation</em>, Automated reasoning and computer algebra systems (ar-ca)AR-ca, 39 (5): 523–38. <a href="https://www.sciencedirect.com/science/article/pii/S074771710500026X" class="uri">https://www.sciencedirect.com/science/article/pii/S074771710500026X</a>.</p>
</div>
<div id="ref-bergstra_theano_2010">
<p>Bergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. “Theano: A CPU and GPU Math Expression Compiler.” In <em>Proceedings of the Python for Scientific Computing Conference (SciPy)</em>. Austin, TX.</p>
</div>
<div id="ref-bertsekas_incremental_2010">
<p>Bertsekas, Dimitri P. 2010. “Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey.” <a href="http://web.mit.edu/dimitrib/www/Incremental_Survey_LIDS.pdf" class="uri">http://web.mit.edu/dimitrib/www/Incremental_Survey_LIDS.pdf</a>.</p>
</div>
<div id="ref-chaux_variational_2007">
<p>Chaux, Caroline, Patrick L Combettes, Jean-Christophe Pesquet, and Valérie R Wajs. 2007. “A Variational Formulation for Frame-Based Inverse Problems.” <em>Inverse Problems</em> 23 (4): 1495.</p>
</div>
<div id="ref-combettes_proximal_2011">
<p>Combettes, Patrick L, and Jean-Christophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” <em>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</em>, 185–212.</p>
</div>
<div id="ref-diamond_cvxpy:_2016">
<p>Diamond, Steven, and Stephen Boyd. 2016. “CVXPY: A Python-Embedded Modeling Language for Convex Optimization.” <em>Journal of Machine Learning Research</em> 17 (83): 1–5.</p>
</div>
<div id="ref-friedman_pathwise_2007">
<p>Friedman, Jerome, Trevor Hastie, Holger Höfling, Robert Tibshirani, and others. 2007. “Pathwise Coordinate Optimization.” <em>The Annals of Applied Statistics</em> 1 (2): 302–32. <a href="http://projecteuclid.org/euclid.aoas/1196438020" class="uri">http://projecteuclid.org/euclid.aoas/1196438020</a>.</p>
</div>
<div id="ref-mazumder_regularization_2009">
<p>Mazumder, Rahul, Trevor Hastie, and Rob Tibshirani. 2009. “Regularization Methods for Learning Incomplete Matrices.” <em>arXiv Preprint arXiv:0906.2034</em>. <a href="https://arxiv.org/abs/0906.2034" class="uri">https://arxiv.org/abs/0906.2034</a>.</p>
</div>
<div id="ref-parikh_proximal_2014">
<p>Parikh, Neal, and Stephen Boyd. 2014. “Proximal Algorithms.” <em>Foundations and Trends in Optimization</em> 1 (3): 123–231. <a href="https://doi.org/10.1561/2400000003" class="uri">https://doi.org/10.1561/2400000003</a>.</p>
</div>
<div id="ref-polson_proximal_2015">
<p>Polson, Nicholas G., James G. Scott, and Brandon T. Willard. 2015. “Proximal Algorithms in Statistics and Machine Learning.” <em>Statistical Science</em> 30 (4): 559–81. <a href="http://projecteuclid.org/euclid.ss/1449670858" class="uri">http://projecteuclid.org/euclid.ss/1449670858</a>.</p>
</div>
<div id="ref-roberts_updating_1997">
<p>Roberts, Gareth O., and Sujit K. Sahu. 1997. “Updating Schemes, Correlation Structure, Blocking and Parameterization for the Gibbs Sampler.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 59 (2): 291–317. <a href="http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00070/abstract" class="uri">http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00070/abstract</a>.</p>
</div>
<div id="ref-salvatier_probabilistic_2016">
<p>Salvatier, John, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using PyMC3.” <em>PeerJ Computer Science</em> 2 (April): e55. <a href="https://peerj.com/articles/cs-55" class="uri">https://peerj.com/articles/cs-55</a>.</p>
</div>
<div id="ref-scikit-learn_sklearn.linear_model.elasticnet_2017">
<p>scikit-learn. 2017. “Sklearn.Linear_model.ElasticNet Scikit-Learn 0.19.Dev0 Documentation.” <a href="http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet" class="uri">http://scikit-learn.org/dev/modules/generated/sklearn.linear_model.ElasticNet.html\#sklearn-linear-model-elasticnet</a>.</p>
</div>
<div id="ref-svaiter_pyprox_2017">
<p>svaiter. 2017. “Pyprox.” <a href="https://github.com/svaiter/pyprox" class="uri">https://github.com/svaiter/pyprox</a>.</p>
</div>
<div id="ref-willard_role_2017">
<p>Willard, Brandon T. 2017. “A Role for Symbolic Computation in the General Estimation of Statistical Models.” <a href="https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html" class="uri">https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html</a>.</p>
</div>
<div id="ref-wytock_new_2016">
<p>Wytock, Matt, Steven Diamond, Felix Heide, and Stephen Boyd. 2016. “A New Architecture for Optimization Modeling Frameworks.” <em>arXiv Preprint arXiv:1609.03488</em>. <a href="https://arxiv.org/abs/1609.03488" class="uri">https://arxiv.org/abs/1609.03488</a>.</p>
</div>
</div>
</section>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
  SVG: { linebreaks: { automatic: true } },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
</body>
</html>

            </div>
            <!-- /.entry-content -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

                    var disqus_identifier = 'more-proximal-estimation';
                var disqus_url = 'https://brandonwillard.github.io/more-proximal-estimation.html';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="https://brandonwillard.github.io//images/profile-pic.png"/>
        </p>
    <p>
      <strong>About Brandon T. Willard</strong><br/>
        applied math/stats person
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://github.com/brandonwillard"><i class="fa fa-github-square fa-lg"></i> github</a></li>
    <li class="list-group-item"><a href="https://scholar.google.com/citations?user=g0oUxG4AAAAJ&hl=en"><i class="ai ai-google-scholar ai-lg"></i> google scholar</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/brandon-t-willard-468bb410/"><i class="fa fa-linkedin fa-lg"></i> linkedin</a></li>
    <li class="list-group-item"><a href="https://bitbucket.io/brandonwillard"><i class="fa fa-bitbucket-square fa-lg"></i> bitbucket</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2019 Brandon T. Willard
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/deed.en"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">Creative Commons Attribution-NonCommercial 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://brandonwillard.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://brandonwillard.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://brandonwillard.github.io/theme/js/respond.min.js"></script>


    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'brandonwillard-github-io'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-91585967-1', '');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->


</body>
</html>