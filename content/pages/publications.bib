
@techreport{willard_functions_2007,
  title = {Functions {{Alternative}} to the {{Likelihood}}},
  institution = {{Cornell, Wayne State}},
  author = {Willard, Brandon T.},
  year = {2007},
  file = {/home/bwillard/projects/papers/references/Willard_Functions Alternative to the Likelihood.pdf}
}

@article{BhadraGlobalLocalMixtures2016,
  title = {Global-Local Mixtures},
  urldate = {2016-05-05},
  url = {http://arxiv.org/abs/1604.07487},
  journal = {arXiv preprint arXiv:1604.07487},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
  year = {2016},
  file = {/home/bwillard/projects/papers/references/Bhadra_et_al_2016_Global-Local_Mixtures.pdf;/home/bwillard/projects/papers/references/Polson et al_Global-Local Mixtures.pdf;/home/bwillard/Zotero/storage/8RD3BM7T/1604.html}
}

@article{PolsonStatisticalTheoryDeep2015,
  title = {A {{Statistical Theory}} of {{Deep Learning}} via {{Proximal Splitting}}},
  urldate = {2016-09-06},
  url = {http://arxiv.org/abs/1509.06061},
  journal = {arXiv preprint arXiv:1509.06061},
  author = {Polson, Nicholas G. and Willard, Brandon T. and Heidari, Massoud},
  year = {2015},
  file = {/home/bwillard/projects/papers/references/Polson_et_al_2015_A_Statistical_Theory_of_Deep_Learning_via_Proximal_Splitting.pdf;/home/bwillard/Zotero/storage/FK2UAS46/1509.html}
}

@article{BhadraHorseshoeRegularizationFeature2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.07400},
  primaryClass = {stat},
  title = {Horseshoe Regularization for Feature Subset Selection},
  abstract = {Feature subset selection arises in many high-dimensional applications in machine learning and statistics, such as compressed sensing and genomics. The $\ell_0$ penalty is ideal for this task, the caveat being it requires the NP-hard combinatorial evaluation of all models. A recent area of considerable interest is to develop efficient algorithms to fit models with a non-convex $\ell_\gamma$ penalty for $\gamma\in (0,1)$, which results in sparser models than the convex $\ell_1$ or lasso penalty, but is harder to fit. We propose an alternative, termed the horseshoe regularization penalty for feature subset selection, and demonstrate its theoretical and computational advantages. The distinguishing feature from existing non-convex optimization approaches is a full probabilistic representation of the penalty as the negative of the logarithm of a suitable prior, which in turn enables an efficient expectation-maximization algorithm for optimization and MCMC for uncertainty quantification. In synthetic and real data, the resulting algorithm provides better statistical performance, and the computation requires a fraction of time of state of the art non-convex solvers.},
  urldate = {2017-03-30},
  url = {http://arxiv.org/abs/1702.07400},
  journal = {arXiv:1702.07400 [stat]},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
  month = feb,
  year = {2017},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/bwillard/projects/papers/references/Bhadra_et_al_2017_Horseshoe_Regularization_for_Feature_Subset_Selection.pdf;/home/bwillard/Zotero/storage/BWERGR7M/1702.html}
}

@article{WillardRealtimeRoadGPS2013,
  title = {Real-Time {{On}} and {{Off Road GPS Tracking}}},
  urldate = {2016-09-06},
  url = {http://arxiv.org/abs/1303.1883},
  journal = {arXiv preprint arXiv:1303.1883},
  author = {Willard, Brandon T.},
  year = {2013},
  keywords = {gps,particle filtering,particle filters,particle learning,tracking,vehicle tracking},
  file = {/home/bwillard/projects/papers/references/Willard_2013_Real-time_On_and_Off_Road_GPS_Tracking.pdf;/home/bwillard/Zotero/storage/2FU9DXNF/1303.html}
}

@techreport{willard_computation_2004,
  title = {Computation of {{NRQCD Parameters}} in {{Potential Models}}},
  urldate = {2015-12-14},
  url = {http://rhig.physics.wayne.edu/REU/reports/summer2007/BrandonWillard/willard_report_reu.pdf},
  institution = {{Cornell, Wayne State}},
  author = {Willard, Brandon},
  year = {2004},
  file = {/home/bwillard/projects/papers/references/Willard_2004_Computation of NRQCD Parameters in Potential Models.pdf}
}

@techreport{willard_recursive_2014,
  title = {Recursive {{Bayesian Computation}} of the {{Dynamic Logit Model}}},
  url = {https://bitbucket.org/brandonwillard/particlebayes/downloads/rbc-logit-example.pdf},
  institution = {{University of Chicago}},
  author = {Willard, Brandon T.},
  month = dec,
  year = {2014},
  pages = {9},
  file = {/home/bwillar0/projects/papers/rbc-paper/output/rbc-logit-example.pdf}
}

@article{BhadraDefaultBayesiananalysis2016,
  title = {Default Bayesian analysis with global-local shrinkage priors},
  volume = {103},
  issn = {0006-3444},
  doi = {10.1093/biomet/asw041},
  number = {4},
  urldate = {2017-02-10},
  url = {https://academic.oup.com/biomet/article-abstract/103/4/955/2659031/Default-Bayesian-analysis-with-global-local},
  journal = {Biometrika},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
  month = dec,
  year = {2016},
  keywords = {default bayes,james-stein,shrinkage},
  pages = {955-969},
  file = {/home/bwillard/projects/papers/references/Bhadra_et_al_2015_Default_Bayesian_Analysis_with_Global-Local_Shrinkage_Priors.pdf;/home/bwillard/projects/papers/references/Bhadra_et_al_2016_Default_Bayesian_Analysis_with_Global-Local_Shrinkage_Priors_2.pdf;/home/bwillard/Zotero/storage/ZN4RHA3A/1510.html;/home/bwillard/Zotero/storage/ZUK53397/Default-Bayesian-analysis-with-global-local.html}
}

@article{BhadraHorseshoeEstimatorUltraSparse2016,
  title = {The Horseshoe+ Estimator of Ultra-Sparse Signals},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/16-BA1028},
  abstract = {We propose a new prior for ultra-sparse signal detection that we term the “horseshoe+ prior.” The horseshoe+ prior is a natural extension of the horseshoe prior that has achieved success in the estimation and detection of sparse signals and has been shown to possess a number of desirable theoretical properties while enjoying computational feasibility in high dimensions. The horseshoe+ prior builds upon these advantages. Our work proves that the horseshoe+ posterior concentrates at a rate faster than that of the horseshoe in the Kullback–Leibler (K-L) sense. We also establish theoretically that the proposed estimator has lower posterior mean squared error in estimating signals compared to the horseshoe and achieves the optimal Bayes risk in testing up to a constant. For one-group global–local scale mixture priors, we develop a new technique for analyzing the marginal sparse prior densities using the class of Meijer-G functions. In simulations, the horseshoe+ estimator demonstrates superior performance in a standard design setting against competing methods, including the horseshoe and Dirichlet–Laplace estimators. We conclude with an illustration on a prostate cancer data set and by pointing out some directions for future research.},
  language = {EN},
  urldate = {2016-10-08},
  url = {http://projecteuclid.org/euclid.ba/1474572263},
  journal = {Bayesian Analysis},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
  year = {2016},
  keywords = {\#duplicates,Bayesian,bayesian,global-local shrinkage,global–local shrinkage,hierarchical,horseshoe,horseshoe+,minimax,normal means,sparsity},
  file = {/home/bwillard/projects/papers/references/Bhadra et al_2014_The Horseshoe+ Estimator of Ultra-Sparse Signals.pdf;/home/bwillard/projects/papers/references/Bhadra et al_2014_The Horseshoe+ Estimator of Ultra-Sparse Signals4.pdf;/home/bwillard/projects/papers/references/Bhadra_et_al_2015_The_horseshoe+_estimator_of_ultra-sparse_signals.pdf;/home/bwillard/projects/papers/references/Bhadra_et_al_2016_The_Horseshoe+_Estimator_of_Ultra-Sparse_Signals.pdf;/home/bwillard/Zotero/storage/64HN4S4G/1474572263.html;/home/bwillard/Zotero/storage/E2PZD2EI/1474572263.html;/home/bwillard/Zotero/storage/WQKNT8PQ/1502.html}
}

@article{BhadraPredictionriskgloballocal2016,
  title = {Prediction risk for global-local shrinkage regression},
  urldate = {2016-07-16},
  url = {http://arxiv.org/abs/1605.04796},
  journal = {arXiv preprint arXiv:1605.04796},
  author = {Bhadra, Anindya and Datta, Jyotishka and Li, Yunfan and Polson, Nicholas G. and Willard, Brandon},
  year = {2016},
  file = {/home/bwillard/projects/papers/references/Bhadra_et_al_2016_Prediction_risk_for_global-local_shrinkage_regression.pdf;/home/bwillard/Zotero/storage/J6EINZRB/1605.html}
}

@article{polson_proximal_2015,
  title = {Proximal Algorithms in Statistics and Machine Learning},
  volume = {30},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/15-STS530},
  abstract = {Proximal algorithms are useful for obtaining solutions to difficult optimization problems, especially those involving nonsmooth or composite objective functions. A proximal algorithm is one whose basic iterations involve the proximal operator of some function, whose evaluation requires solving a specific optimization problem that is typically easier than the original problem. Many familiar algorithms can be cast in this form, and this “proximal view” turns out to provide a set of broad organizing principles for many algorithms useful in statistics and machine learning. In this paper, we show how a number of recent advances in this area can inform modern statistical practice. We focus on several main themes: (1) variable splitting strategies and the augmented Lagrangian; (2) the broad utility of envelope (or variational) representations of objective functions; (3) proximal algorithms for composite objective functions; and (4) the surprisingly large number of functions for which there are closed-form solutions of proximal operators. We illustrate our methodology with regularized Logistic and Poisson regression incorporating a nonconvex bridge penalty and a fused lasso penalty. We also discuss several related issues, including the convergence of nondescent algorithms, acceleration and optimization for nonconvex functions. Finally, we provide directions for future research in this exciting area at the intersection of statistics and optimization.},
  language = {EN},
  number = {4},
  urldate = {2015-12-14},
  url = {http://projecteuclid.org/euclid.ss/1449670858},
  journal = {Statistical Science},
  author = {Polson, Nicholas G. and Scott, James G. and Willard, Brandon T.},
  month = nov,
  year = {2015},
  keywords = {\#duplicates,ADMM,Bayes MAP,Divide and Concur,KL,Kurdyka–Łojasiewicz,Optimization,Regularization,bayes MAP,divide and concur,envelopes,fused lasso,non-convex optimisation,nonconvex,optimization,proximal,proximal operators,regularization,shrinkage,sparsity,splitting},
  pages = {559-581},
  file = {/home/bwillard/projects/papers/references/Polson et al_2015_Proximal Algorithms in Statistics and Machine Learning4.pdf;/home/bwillard/Zotero/storage/ATRB24GU/1502.html;/home/bwillard/Zotero/storage/KJ239REQ/1449670858.html}
}

@article{BhadraLassoMeetsHorseshoe2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.10179},
  primaryClass = {stat},
  title = {Lasso {{Meets Horseshoe}}},
  abstract = {The goal of our paper is to survey and contrast the major advances in two of the most commonly used high-dimensional techniques, namely, the Lasso and horseshoe regularization methodologies. Lasso is a gold standard for best subset selection of predictors while the horseshoe is a state-of-the-art Bayesian estimator for sparse signals. Lasso is scalable and fast using convex optimization whilst the horseshoe is a non-convex penalty. Our novel perspective focuses on three aspects, (i) efficiency and scalability of computation and (ii) methodological development and performance and (iii) theoretical optimality in high dimensional inference for the Gaussian sparse model and beyond.},
  urldate = {2018-02-07},
  url = {http://arxiv.org/abs/1706.10179},
  journal = {arXiv:1706.10179 [stat]},
  author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon T.},
  month = jun,
  year = {2017},
  keywords = {Statistics - Methodology,Primary 62J07; 62J05; Secondary 62H15; 62F03},
  file = {/home/bwillard/projects/papers/references/Bhadra_et_al_2017_Lasso_Meets_Horseshoe.pdf;/home/bwillard/Zotero/storage/B3VB34HB/1706.html}
}


