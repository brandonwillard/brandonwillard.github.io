#+TITLE: Dynamic Linear Model Optimizations in Theano
#+AUTHOR: Brandon T. Willard
#+DATE: 2020-03-18
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :draft:pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:t title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session dlm-optimizations :comments noweb
#+PROPERTY: header-args:python :noweb-sep "\n\n"
#+PROPERTY: header-args:latex :results html replace :exports results :eval yes

#+BEGIN_abstract
In this document we construct dynamic linear models (DLMs) in Theano and explore
ideas for automating the production of efficient samplers.
#+END_abstract

* Introduction

We start by considering the simple form of a DLM [[citep:harrison_bayesian_1999]] with
prior \(\theta_0 \sim \operatorname{N}\left( m_0, C_0 \right)\):

#+BEGIN_SRC latex
\begin{align}
  y_t &= F_t^{\top} \theta_{t} + \epsilon_t, \quad \epsilon_t \sim \operatorname{N}\left( 0, V \right)
  \label{eq:basic-dlm-obs}
  \\
  \theta_t &= G_t \theta_{t-1} + \nu_t, \quad \nu_t \sim \operatorname{N}\left( 0, W \right)
  \label{eq:basic-dlm-state}
\end{align}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{align}
  y_t &= F_t^{\top} \theta_{t} + \epsilon_t, \quad \epsilon_t \sim \operatorname{N}\left( 0, V \right)
  \label{eq:basic-dlm-obs}
  \\
  \theta_t &= G_t \theta_{t-1} + \nu_t, \quad \nu_t \sim \operatorname{N}\left( 0, W \right)
  \label{eq:basic-dlm-state}
\end{align}
#+end_export

for \(t \in \{1, \dots, T\}\), \(y_t \in \mathbb{R}\), and \(\theta_t \in \mathbb{R}^{M}\).

The most "notationally" faithful representation of the timeseries model in
[[eqref:eq:basic-dlm-state]] using Theano is provided in Listing
[[basic-normal-model]].  It represents the notion of a recursion--to the best of
Theano's ability--by way of the src_python[:eval never]{scan} operator.

#+NAME: basic-imports
#+BEGIN_SRC python :results silent
import numpy as np

import theano
import theano.tensor as tt

import matplotlib.pyplot as plt

from cycler import cycler

from matplotlib.collections import LineCollection

from theano.printing import debugprint as tt_dprint

from symbolic_pymc.theano.random_variables import NormalRV, MvNormalRV, GammaRV, observed


plt.style.use('ggplot')
plt_orig_cycler = plt.rcParams['axes.prop_cycle']
plt.rc('text', usetex=True)

# theano.config.cxx = ""
# theano.config.mode = "FAST_COMPILE"
tt.config.compute_test_value = 'ignore'
#+END_SRC

#+NAME: basic-normal-model
#+BEGIN_SRC python :results silent

N_obs_tt = tt.iscalar("N_obs")
N_theta_tt = tt.iscalar("N_theta")

G_tt = tt.specify_shape(tt.matrix(), [N_theta_tt, N_theta_tt])
G_tt.name = 'G_t'

F_tt = tt.specify_shape(tt.col(), [N_theta_tt, 1])
F_tt.name = 'F_t'

rng_state = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1234)))
rng_init_state = rng_state.get_state()
rng_tt = theano.shared(rng_state, name='rng', borrow=True)
rng_tt.tag.is_rng = True
rng_tt.default_update = rng_tt

m_0_tt = tt.zeros([N_theta_tt])
m_0_tt.name = "m_0"
C_0_tt = 1000.0 * tt.eye(N_theta_tt)
C_0_tt.name = "C_0"

theta_0_rv = MvNormalRV(m_0_tt, C_0_tt, rng=rng_tt, name='theta_0')

phi_W_true = np.r_[5.0, 10.0]
phi_W_tt = theano.shared(phi_W_true, name='phi_W')
W_tt = tt.eye(N_theta_tt) * tt.inv(phi_W_tt)
W_tt.name = "W_t"

phi_V_true = 0.1
phi_V_tt = theano.shared(phi_V_true, name='phi_V')
V_tt = tt.inv(phi_V_tt)
V_tt.name = "V_t"

def state_step(theta_tm1, G_t, W_t, N_theta, rng):
    nu_rv = MvNormalRV(tt.zeros([N_theta]), W_t, rng=rng, name='nu')
    theta_t = G_t.dot(theta_tm1) + nu_rv
    return theta_t


theta_t_rv, theta_t_updates = theano.scan(fn=state_step,
                                          outputs_info={"initial": theta_0_rv, "taps": [-1]},
                                          non_sequences=[G_tt, W_tt, N_theta_tt, rng_tt],
                                          n_steps=N_obs_tt,
                                          strict=True,
                                          name='theta_t')

def obs_step(theta_t, F_t, V_t, rng):
    eps_rv = NormalRV(0.0, tt.sqrt(V_t), rng=rng, name='eps')
    y_t = F_t.T.dot(theta_t) + eps_rv
    return y_t


Y_t_rv, Y_t_updates = theano.scan(fn=obs_step,
                                  sequences=[theta_t_rv],
                                  non_sequences=[F_tt, V_tt, rng_tt],
                                  strict=True,
                                  name='Y_t')
#+END_SRC

The model in Listing [[basic-normal-model]] is our starting point.  We assume
that a PyMC3 user--for instance--would define a timeseries model in this way,
alongside distributional assumptions on parameters (e.g. inverse-gamma
variances).  From there, we'll explore some ideas behind manually producing
model-specific efficient samplers--generally by first manually deriving and
then demonstrating said samplers.

Throughout we'll use data sampled from [[eqref:eq:basic-dlm-state]] for demonstration
purposes.  Specifically, our simulation has the following values:
#+BEGIN_SRC latex
\begin{gather}
  T = 200,\quad M = 2
  \\
  \phi_W = \left(1.1, 10\right),\quad \phi_V = 0.7
  \\
  G_t = \begin{pmatrix}
  1 & 0.1 \\
  0 & 1 \\
  \end{pmatrix},\quad
  F_t = \begin{pmatrix}
  1 \\
  0
  \end{pmatrix}
  \\
  \theta_0 = \begin{pmatrix}
  0 \\
  0
  \end{pmatrix}
  \label{eq:sim-settings}
\end{gather}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{gather}
  T = 200,\quad M = 2
  \\
  \phi_W = \left(1.1, 10\right),\quad \phi_V = 0.7
  \\
  G_t = \begin{pmatrix}
  1 & 0.1 \\
  0 & 1 \\
  \end{pmatrix},\quad
  F_t = \begin{pmatrix}
  1 \\
  0
  \end{pmatrix}
  \\
  \theta_0 = \begin{pmatrix}
  0 \\
  0
  \end{pmatrix}
  \label{eq:sim-settings}
\end{gather}
#+end_export

#+NAME: basic-dlm-sim
#+BEGIN_SRC python :results silent
from theano import function as tt_function

dlm_sim_values = {
    N_obs_tt: 200,
    N_theta_tt: 2,
    G_tt: np.r_['0,2',
                [1.0, 0.1],
                [0.0, 1.0]].astype(tt.config.floatX),
    F_tt: np.r_[[[1.0],
                 [0.0]]].astype(tt.config.floatX)
}

rng_tt.get_value(borrow=True).set_state(rng_init_state)

simulate_dlm = tt_function([N_obs_tt, N_theta_tt, G_tt, F_tt],
                           [Y_t_rv, theta_t_rv],
                           givens={theta_0_rv: np.r_[0.0, 0.0]},
                           updates=Y_t_updates)

y_sim, theta_t_sim = simulate_dlm(dlm_sim_values[N_obs_tt], dlm_sim_values[N_theta_tt], dlm_sim_values[G_tt], dlm_sim_values[F_tt])

rng_sim_state = rng_tt.get_value(borrow=True).get_state()
#+END_SRC

In Figure [[fig:basic-dlm-sim-plot-fig]] we plot a sample from the model in Listing
[[basic-normal-model]] for a fixed RNG seed.

#+NAME: fig:basic-dlm-sim-plot-fig
#+BEGIN_SRC python :results graphics file :file ../../figures/basic-dlm-sim-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))
_ = ax.plot(y_sim, label=r'$y_t$', color='black', linewidth=0.7)

plt.tight_layout()
plt.legend()
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION:
#+RESULTS: fig:basic-dlm-sim-plot-fig
[[file:../../figures/basic-dlm-sim-plot.png]]


Since our goal is to automate some of the basic steps in the process of analytically
manipulating and/or solving DLMs (for the purpose of producing efficient and accurate
posterior estimates), we will want to compute as many closed-form operations
as possible, and the prior predictive state and observation distributions are a
good place to start.

Given all the prior and observed data up to time \(t\), \(D_t\), these
distribution are given by the following:
#+BEGIN_SRC latex
\begin{align}
  \theta_{t} \mid D_{t-1} &\sim \operatorname{N}\left( a_{t}, R_{t} \right)
  \\
  y_{t} \mid D_{t-1} &\sim \operatorname{N}\left( f_{t}, Q_{t} \right)
\end{align}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{align}
  \theta_{t} \mid D_{t-1} &\sim \operatorname{N}\left( a_{t}, R_{t} \right)
  \\
  y_{t} \mid D_{t-1} &\sim \operatorname{N}\left( f_{t}, Q_{t} \right)
\end{align}
#+end_export

The prior predictive moments are as follows:
#+BEGIN_SRC latex
\begin{equation}
  \begin{gathered}
    a_t = G_t m_{t-1}, \quad R_t = G_t C_{t-1} G_t^\top + W_t
    \\
    f_t = F_t^\top a_{t}, \quad Q_t = F_t^\top C_{t-1} F_t + V_t
  \end{gathered}
  \label{eq:dlm-prior-predictive}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{gathered}
    a_t = G_t m_{t-1}, \quad R_t = G_t C_{t-1} G_t^\top + W_t
    \\
    f_t = F_t^\top a_{t}, \quad Q_t = F_t^\top C_{t-1} F_t + V_t
  \end{gathered}
  \label{eq:dlm-prior-predictive}
\end{equation}
#+end_export

We'll also want to compute the posterior moments for \(\theta_t \mid D_t\),
which are as follows:
#+BEGIN_SRC latex
\begin{equation}
  \begin{gathered}
    m_t = a_{t} + R_t F_t Q_t^{-1} \left(y_t - f_t\right),
    \quad C_t = R_t  - R_t F_t Q_t^{-1} F_t^\top R_t
  \end{gathered}
  \label{eq:dlm-post-moments}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{gathered}
    m_t = a_{t} + R_t F_t Q_t^{-1} \left(y_t - f_t\right),
    \quad C_t = R_t  - R_t F_t Q_t^{-1} F_t^\top R_t
  \end{gathered}
  \label{eq:dlm-post-moments}
\end{equation}
#+end_export

These "filtered" moments/distributions are only *one* kind of posterior result
for a DLM, and they only take into account the data up to time \(t\).  The other
kind are the "smoothed" distributions, which provided posterior distributions
for each time \(t\) given all observations.

Notationally, we've used \(D_t\) to signify all conditional observations and
parameters up to time \(t\), so the smoothed distributions are given by
\(\theta_t \mid D_T\) and the following moments:
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    s_t &= m_t + C_t G_{t+1}^\top R_{t+1}^{-1} \left( s_{t+1} - a_{t+1} \right)
    \\
    S_t &= C_t - C_t G_{t+1}^\top R_{t+1}^{-1} \left( R_{t+1} - S_{t+1} \right) R_{t+1}^{-1} G_{t+1} C_t
  \end{aligned}
  \label{eq:dlm-smooth-moments}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    s_t &= m_t + C_t G_{t+1}^\top R_{t+1}^{-1} \left( s_{t+1} - a_{t+1} \right)
    \\
    S_t &= C_t - C_t G_{t+1}^\top R_{t+1}^{-1} \left( R_{t+1} - S_{t+1} \right) R_{t+1}^{-1} G_{t+1} C_t
  \end{aligned}
  \label{eq:dlm-smooth-moments}
\end{equation}
#+end_export

:REMARK:
In most cases, models will not be as simple as the standard DLM.  Even so, these
basic closed-form solutions can still be relevant.
For instance, efficient MCMC algorithms can be constructed using these
closed-form results for *conditionally linear* models.  In those cases, we can
compute the posterior moments--in closed-form--conditional on samples generated
by other means.
:END:

The standard approach is called forward-filtering backward-sampling
(FFBS) and uses smoothed posteriors \(\theta_t \mid \theta_{t+1}, D_T\)
conditioned on all other parameters.

We'll build up to forward-backward sampling in what follows, but, first, we need
to establish how the requisite quantities can be computed symbolically.

* Posterior Estimation

In Listings [[filter-svd-scan]] and [[smoother-svd-scan]], we demonstrate how the posterior
moments in [[eqref:eq:dlm-post-moments]] and [[eqref:eq:dlm-smooth-moments]] can
be computed in Theano.

Unfortunately, if we attempt to implement the exact closed-form updates in
[[eqref:eq:dlm-post-moments]] or [[eqref:eq:dlm-smooth-moments]], our results
will be fraught with numerical errors.  This is a very basic issue with naively
implemented Kalman filters.  The solution to these issues usually involves some
analytic reformulations that compensate for the covariance matrix subtractions.
The standard approaches generally use some form of matrix decomposition that
directly accounts for the positive semi-definite nature of the covariance
matrices.

The approach taken here is based on the singular value decomposition (SVD) and
effectively computes only one symmetric "half" of the updated covariances.  The
SVD also allows for easy inversions.
See [[citet:ZhangFixedintervalsmoothingalgorithm1996]] for more details, or
[[citet:PetrisDynamiclinearmodels2009]] for a concise overview of the procedure in
the context of DLMs.

#+NAME: linalg-theano-ops
#+BEGIN_SRC python :results silent
import warnings

warnings.filterwarnings("ignore", category=FutureWarning, message="Using a non-tuple sequence")

from theano.tensor.nlinalg import matrix_dot


def tt_finite_inv(x, eps_truncate=False):
    """Compute the element-wise reciprocal with special handling for small inputs.

    Parameters
    ==========
    x: Tensor-like
        The value for which the reciprocal, i.e. `1/x`, is computed.

    eps_truncate: bool (optional)
        Determines whether or not a floating-point epsilon truncation is used to
        upper-bound the returned values.
        If not (the default), infinite values are simply set to zero.
    """
    if eps_truncate:
        eps = np.finfo(getattr(x, 'dtype', None) or theano.config.floatX).eps
        return tt.minimum(tt.inv(x), np.reciprocal(np.sqrt(eps)))
    else:
        y = tt.inv(x)
        res_subtensor = y[tt.isinf(y)]
        return tt.set_subtensor(res_subtensor, 0.0)

#+END_SRC

#+NAME: linalg-theano-ops-LDL
#+BEGIN_SRC python :exports none :results silent
import scipy

from theano.gof import Op, Apply


class LDL(Op):
    """Compute `L` and `D` in `A = L D L^H`."""
    __props__ = ('lower', 'hermitian')

    def __init__(self, lower=True, hermitian=True):
        self.lower = lower
        self.hermitian = hermitian

    def make_node(self, a):
        a = tt.as_tensor_variable(a)
        assert a.ndim == 2, "The input of LDL function should be a matrix."
        lu = tt.matrix(dtype=a.dtype)
        d = tt.matrix(dtype=a.dtype)
        perm = tt.vector(dtype=a.dtype)
        return Apply(self, [a], [lu, d, perm])

    def perform(self, node, inputs, outputs):
        (a,) = inputs
        assert a.ndim == 2 and a.shape[0] == a.shape[1], "The input should be a square matrix."
        lu, d, perm = outputs
        lu[0], d[0], perm[0] = scipy.linalg.ldl(a, lower=self.lower, hermitian=self.hermitian)

    def infer_shape(self, node, shapes):
        # XXX: Scipy doesn't seem clear on the return shapes, so this might not
        # always be true.
        a_shape, = shapes
        M = a_shape[0]
        return [(M, M), (M, M), (M,)]


ldl = LDL()
#+END_SRC

#+NAME: linalg-theano-ops-Solve
#+BEGIN_SRC python :exports none :results silent
from theano.tensor.slinalg import Solve, MATRIX_STRUCTURES


class Solve(Solve):
    def __init__(self,
                 A_structure='general',
                 lower=False,
                 overwrite_A=False,
                 overwrite_b=False,
                 transposed=False):
        if A_structure not in MATRIX_STRUCTURES + ('positive_definite', 'svd'):
            raise ValueError('Invalid matrix structure argument', A_structure)
        self.A_structure = A_structure
        self.lower = lower
        self.overwrite_A = overwrite_A
        self.overwrite_b = overwrite_b
        self.transposed = transposed

    def perform(self, node, inputs, output_storage):
        A, b = inputs
        if self.A_structure == 'lower_triangular':
            rval = scipy.linalg.solve_triangular(
                A, b, lower=True)
        elif self.A_structure == 'upper_triangular':
            rval = scipy.linalg.solve_triangular(
                A, b, lower=False)
        elif self.A_structure == 'symmetric':
            rval = scipy.linalg.solve(A, b, assume_a='sym', transposed=self.transposed)
        elif self.A_structure == 'positive_definite':
            rval = scipy.linalg.solve(A, b, assume_a='pos', transposed=self.transposed)
        elif self.A_structure == 'svd':
            rval = scipy.linalg.lstsq(A, b)[0]
        else:
            rval = scipy.linalg.solve(A, b, transposed=self.transposed)
        output_storage[0][0] = rval


solve_sym_T = Solve('symmetric', transposed=True)
#+END_SRC

** SVD-based Filtering

The SVD forms of the filtering equations in [[eqref:eq:dlm-post-moments]] are
produced through creative use of the SVDs of its component matrices.  Using a
slightly modified version of the formulation established in
[[citet:PetrisDynamiclinearmodels2009]], the SVD for a matrix \(M\) is given by
\(M = U_{M} D_{M} V_{M}^\top\).  A symmetric matrix then takes the form \(M =
U_{M} D_{M} U_{M}^\top\) and its "square-root" is given by \(M = N_M^\top N_M\)
with \(N_M = S_{M} U_{M}^\top\) and \(S_{M} = D_{M}^{1/2}\).  Likewise, matrix
(generalized) inverses take the form \(M^{-1} = U_{M} S_{M}^{-1} U_{M}^\top\).

The idea here is that we can combine these SVD identities to derive square-root
relationship between the SVD of \(C_t^{-1}\) and the SVDs of \(C_{t-1}\), \(W_t\), \(V_t\),
and \(R_t\), then we can easily invert \(C_t^{-1}\) to arrive at the desired
numerically stable SVD of \(C_t\).

First, note that \(N_{R_t}^\top N_{R_t} = G_t C_{t-1} G_t^\top + W_t = R_t\) for
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    N_{R_t} &=
      \begin{pmatrix}
        S_{C_{t-1}} U_{C_{t-1}}^\top G_t^\top
        \\
        N_{W_t}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_R_t}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    N_{R_t} &=
      \begin{pmatrix}
        S_{C_{t-1}} U_{C_{t-1}}^\top G_t^\top
        \\
        N_{W_t}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_R_t}
\end{equation}
#+end_export

From this, we know that the SVD of \(R_t\) can be easily derived from the SVD of
its square root, \(N_{R_t}\), i.e. \(U_{R_t} = V_{N_{R_t}}\) and \(S_{R_t} =
D_{N_{R_t}}\).  In other words, we can obtain a matrix's SVD by computing the SVD of
its "half", which is itself entirely comprised of previous SVD components.  The
inherent symmetry of our covariance matrices is nicely preserved because we're only
ever using and computing one "half" of these matrices.

With the updated SVD of \(R_t\), we can use the identity \(C_t^{-1} = F_t V_t^{-1} F_t^\top +
R_t^{-1}\)--obtained via the classic
[[https://en.wikipedia.org/wiki/Woodbury_matrix_identity][Sherman-Morrison-Woodbury matrix inverse identity]]--to employ the same technique
as before and produce the SVD of \(C_t^{-1}\) by way of the SVD of yet another
block square-root matrix,
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    N_{C_t^{-1}} &=
      \begin{pmatrix}
        N_{V_t^{-1}} F_t^\top U_{R_t}
        \\
        S_{R_t}^{-1}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_C_t_inv}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    N_{C_t^{-1}} &=
      \begin{pmatrix}
        N_{V_t^{-1}} F_t^\top U_{R_t}
        \\
        S_{R_t}^{-1}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_C_t_inv}
\end{equation}
#+end_export

Again, we compute the SVD of \(N_{C_t^{-1}}\) at this step and obtain
\(V_{N_{C_t^{-1}}}\) and \(D_{N_{C_t^{-1}}}\).

This time, the block square-root matrix relationship isn't so direct, and we have to
multiply by \(U_{R_t}\): \(U_{R_t} N_{C_t^{-1}}^\top N_{C_t^{-1}} U_{R_t}^\top =
C_t^{-1}\).  However, since the additional \(U_{R_t}\) terms are orthogonal, we are
able to derive the SVD of \(C_t\) as \(U_{C_t} = U_{R_t} V_{N_{C_t^{-1}}}\) and
\(S_{C_t} = D_{N_{C_t^{-1}}}^{-1}\).

These quantities are computed in Listing [[filter-svd-scan]].

#+NAME: filter-svd-scan
#+BEGIN_SRC python :results silent
from theano.tensor.nlinalg import svd


y_tt = tt.specify_shape(tt.col(), [N_obs_tt, 1])
y_tt.name = 'y_t'


def filtering_step(y_t, m_tm1, U_C_tm1, S_C_tm1, F_t, G_t, N_W_t, N_V_t_inv):
    """Compute the sequential posterior state and prior predictive parameters."""

    # R_t = N_R.T.dot(N_R)
    N_R = tt.join(0,
                  matrix_dot(S_C_tm1, U_C_tm1.T, G_t.T),
                  N_W_t)
    # TODO: All this could be much more efficient if we only computed *one* set of singular
    # vectors for these non-square matrices.
    _, d_N_R_t, V_N_R_t_T = svd(N_R)

    U_R_t = V_N_R_t_T.T
    S_R_t = tt.diag(d_N_R_t)
    S_R_t_inv = tt.diag(tt_finite_inv(d_N_R_t))

    N_C_t_inv = tt.join(0,
                        matrix_dot(N_V_t_inv, F_t.T, U_R_t),
                        S_R_t_inv)
    _, d_N_C_t_inv, V_N_C_t_inv_T = svd(N_C_t_inv)

    U_C_t = U_R_t.dot(V_N_C_t_inv_T.T)
    d_C_t = tt_finite_inv(tt.square(d_N_C_t_inv))
    D_C_t = tt.diag(d_C_t)
    S_C_t = tt.diag(tt.sqrt(d_C_t))

    C_t = matrix_dot(U_C_t, D_C_t, U_C_t.T)

    a_t = G_t.dot(m_tm1)
    f_t = F_t.T.dot(a_t)
    # A_t = R_t @ F_t @ inv(Q_t) = C_t @ F_t @ inv(V_t)
    m_t = a_t + matrix_dot(C_t, F_t, N_V_t_inv.T, N_V_t_inv, y_t - f_t)

    return [m_t, U_C_t, S_C_t, a_t, U_R_t, S_R_t]


_, d_C_0_tt, Vt_C_0_tt = svd(C_0_tt)
U_C_0_tt = Vt_C_0_tt.T
S_C_0_tt = tt.diag(tt.sqrt(d_C_0_tt))

_, d_W_tt, Vt_W_tt = svd(W_tt)
U_W_tt = Vt_W_tt.T
s_W_tt = tt.sqrt(d_W_tt)
N_W_tt = tt.diag(s_W_tt).dot(U_W_tt.T)

_, D_V_tt, Vt_V_tt = svd(tt.as_tensor_variable(V_tt, ndim=2) if V_tt.ndim < 2 else V_tt)
U_V_tt = Vt_V_tt.T
S_V_inv_tt = tt.diag(tt.sqrt(tt_finite_inv(D_V_tt, eps_truncate=True)))
N_V_inv_tt = S_V_inv_tt.dot(U_V_tt.T)


filter_res, filter_updates = theano.scan(fn=filtering_step,
                                         sequences=y_tt,
                                         outputs_info=[
                                             {"initial": m_0_tt, "taps": [-1]},
                                             {"initial": U_C_0_tt, "taps": [-1]},
                                             {"initial": S_C_0_tt, "taps": [-1]},
                                             {}, {}, {}, # a_t, U_R_t, S_R_t
                                         ],
                                         non_sequences=[F_tt, G_tt, N_W_tt, N_V_inv_tt],
                                         strict=True,
                                         name='theta_filtered')

(m_t, U_C_t, S_C_t, a_t, U_R_t, S_R_t) = filter_res
#+END_SRC

** SVD-based Smoothing

We can use the ideas to produce SVD versions of the smoothing equations in
[[eqref:eq:dlm-smooth-moments]].  In this case, some extra steps are required in
order to SVD-decompose \(S_t\) in the same manner as \(R_t\) and \(C_t^{-1}\) were.

First, notice that our target, \(S_t\), is a difference of matrices, unlike the
matrix sums that comprised \(R_t\) and \(C_t^{-1}\) above.  Furthermore,
\(S_t\) is given as a difference of a (transformed) difference.  To address the
latter, we start by expanding \(S_t\) and setting \(B_t = C_t G_{t+1}^\top
R_{t+1}^{-1}\) to obtain
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    S_t &= C_t - B_t R_{t+1} B_t^\top + B_t S_{t+1} B_t^\top
      \\
      &= H_t + B_t S_{t+1} B_t^\top
  \end{aligned}
  \label{eq:S_t_decomp}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    S_t &= C_t - B_t R_{t+1} B_t^\top + B_t S_{t+1} B_t^\top
      \\
      &= H_t + B_t S_{t+1} B_t^\top
  \end{aligned}
  \label{eq:S_t_decomp}
\end{equation}
#+end_export

Having turned \(S_t\) into a sum of two terms, we can now consider another
blocked SVD-based square-root reformulation, which starts with the reformulation
of \(H_t\).

We can use the definition of \(R_t = G_{t+1} C_t G_{t+1}^\top + W_{t+1}\)
to get
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    H_t &= C_t - B_t R_{t+1} B_t^\top
    \\
    &= C_t - C_t G_{t+1}^\top R_{t+1}^{-1} G_{t+1} C_t
    \\
    &= C_t - C_t G_{t+1}^\top \left(G_{t+1} C_t G_{t+1}^\top + W_{t+1}\right)^{-1} G_{t+1} C_t
    .
  \end{aligned}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    H_t &= C_t - B_t R_{t+1} B_t^\top
    \\
    &= C_t - C_t G_{t+1}^\top R_{t+1}^{-1} G_{t+1} C_t
    \\
    &= C_t - C_t G_{t+1}^\top \left(G_{t+1} C_t G_{t+1}^\top + W_{t+1}\right)^{-1} G_{t+1} C_t
    .
  \end{aligned}
\end{equation}
#+end_export

This form of \(H_t\) fits the Woodbury identity and results in \(H_t^{-1} =
G_{t+1}^\top W_{t+1}^{-1} G_{t+1} + C_t^{-1}\), which is amenable to our
square-root formulation.

Specifically, \(H_t^{-1} = U_{C_t} N_{H_t}^{-\top} N_{H_t}^{-1} U_{C_t}^\top\), where
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    N_{H_t}^{-1} &=
      \begin{pmatrix}
        N_{W_{t+1}}^{-1} G_{t+1} U_{C_t}
        \\
        S_{C_t}^{-1}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_H_t_inv}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    N_{H_t}^{-1} &=
      \begin{pmatrix}
        N_{W_{t+1}}^{-1} G_{t+1} U_{C_t}
        \\
        S_{C_t}^{-1}
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_H_t_inv}
\end{equation}
#+end_export

By inverting the SVD of \(N_{H_t}^{-1}\) we obtain the SVD of \(H_t\) as \(U_{H_t} =
U_{C_t} V_{N_{H_t}^{-1}}\) and \(D_{H_t} = {D_{N_{H_t}^{-1}}}^{-2} = S_{H_t}^2\).

Finally, using [[eqref:eq:S_t_decomp]] and [[eqref:eq:N_H_t_inv]] we can derive the
last blocked square-root decomposition \(S_t = N_{S_t}^\top N_{S_t}\):
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    N_{S_t} &=
      \begin{pmatrix}
        S_{H_t} U_{H_t}^\top
        \\
        S_{S_{t+1}} U_{S_{t+1}}^\top B_t^\top
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_S_t}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    N_{S_t} &=
      \begin{pmatrix}
        S_{H_t} U_{H_t}^\top
        \\
        S_{S_{t+1}} U_{S_{t+1}}^\top B_t^\top
      \end{pmatrix}
  \end{aligned}
  .
  \label{eq:N_S_t}
\end{equation}
#+end_export

Again, we take the SVD of \(N_{S_t}\) and derive the SVD of \(S_t\) as
\(U_{S_t} = V_{N_{S_t}}\) and \(D_{S_t} = D_{N_{S_t}}^2 = S_{S_t}^2\).

#+NAME: smoother-svd-scan
#+BEGIN_SRC python :results silent
def smoother_step(m_t, U_C_t, S_C_t, a_tp1, U_R_tp1, S_R_tp1, s_tp1, U_S_tp1, S_S_tp1, G_tp1, N_W_tp1_inv):
    """Smooth a series starting from the "forward"/sequentially computed posterior moments."""

    N_C_t = S_C_t.dot(U_C_t.T)

    S_R_tp1_inv = tt_finite_inv(S_R_tp1)
    N_R_tp1_inv = S_R_tp1_inv.dot(U_R_tp1.T)

    # B_t = C_t @ G_tp1.T @ inv(R_tp1)
    B_t = matrix_dot(N_C_t.T, N_C_t, G_tp1.T, N_R_tp1_inv.T, N_R_tp1_inv)

    S_C_t_inv = tt_finite_inv(S_C_t)

    # U_C_t @ N_H_t_inv.T @ N_H_t_inv @ U_C_t.T = G_tp1.T @ W_tp1_inv @ G_tp1 + C_t_inv
    N_H_t_inv = tt.join(0,
                        matrix_dot(N_W_tp1_inv, G_tp1, U_C_t),
                        S_C_t_inv)
    _, d_N_H_t_inv, V_N_H_t_T = svd(N_H_t_inv)

    # H_t = inv(U_C_t @ N_H_t_inv.T @ N_H_t_inv @ U_C_t.T) = C_t - B_t @ R_tp1 @ B_t.T
    U_H_t = U_C_t.dot(V_N_H_t_T.T)
    S_H_t = tt.diag(tt_finite_inv(d_N_H_t_inv))

    # S_t = N_S_t.T.dot(N_S_t) = C_t - matrix_dot(B_t, R_tp1 - S_tp1, B_t.T)
    N_S_t = tt.join(0,
                     S_H_t.dot(U_H_t.T),
                     matrix_dot(S_S_tp1, U_S_tp1.T, B_t.T))
    _, d_N_S_t, V_N_S_t_T = svd(N_S_t)

    U_S_t = V_N_S_t_T.T
    S_S_t = tt.diag(d_N_S_t)

    s_t = m_t + B_t.dot(s_tp1 - a_tp1)

    return [s_t, U_S_t, S_S_t]


N_W_inv_tt = tt.diag(tt_finite_inv(s_W_tt, eps_truncate=True)).dot(U_W_tt.T)

m_T = m_t[-1]
U_C_T = U_C_t[-1]
S_C_T = S_C_t[-1]

# These series only go from N_obs - 1 to 1
smoother_res, _ = theano.scan(fn=smoother_step,
                              sequences=[
                                  {"input": m_t, "taps": [-1]},
                                  {"input": U_C_t, "taps": [-1]},
                                  {"input": S_C_t, "taps": [-1]},
                                  {"input": a_t, "taps": [1]},
                                  {"input": U_R_t, "taps": [1]},
                                  {"input": S_R_t, "taps": [1]}
                              ],
                              outputs_info=[
                                  {"initial": m_T, "taps": [-1]},
                                  {"initial": U_C_T, "taps": [-1]},
                                  {"initial": S_C_T, "taps": [-1]},
                              ],
                              non_sequences=[G_tt, N_W_inv_tt],
                              go_backwards=True,
                              strict=True,
                              name='theta_smoothed_obs')

(s_t_rev, U_S_t_rev, S_S_t_rev) = smoother_res

s_t = s_t_rev[::-1]
U_S_t = U_S_t_rev[::-1]
S_S_t = S_S_t_rev[::-1]

s_t = tt.join(0, s_t, [m_T])
U_S_t = tt.join(0, U_S_t, [U_C_T])
S_S_t = tt.join(0, S_S_t, [S_C_T])
#+END_SRC

** Example
Listing [[filter-smooth-steps-sim-svd]] computes the filtered and smoothed means for our
simulated series, and Figure [[fig:svd-steps-sim-plot]] shows the results.

#+NAME: filter-smooth-steps-sim-svd
#+BEGIN_SRC python :results silent
filter_smooth_dlm = tt_function([y_tt, N_theta_tt, G_tt, F_tt], [m_t, s_t])

# phi_W_tt.set_value(phi_W_true)
# phi_V_tt.set_value(phi_V_true)
phi_W_tt.set_value(np.r_[100.0, 100.0])
phi_V_tt.set_value(1.5)

(m_t_sim, s_t_sim) = filter_smooth_dlm(y_sim, dlm_sim_values[N_theta_tt], dlm_sim_values[G_tt], dlm_sim_values[F_tt])
#+END_SRC

#+NAME: fig:svd-steps-sim-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/svd-steps-sim-plot.png
from cycler import cycler

bivariate_cycler = plt_orig_cycler * cycler('linestyle', ['-', '--'])
plt.close(fig='all')

fig, ax = plt.subplots(figsize=(8, 4.8))
ax.set_prop_cycle(bivariate_cycler)
ax.plot(theta_t_sim, label=r'$\theta_t$', linewidth=0.8, color='black')
ax.autoscale(enable=False)
ax.plot(m_t_sim, label=r'$E[\theta_t \mid D_{t}]$', alpha=0.9, linewidth=0.8)
ax.plot(s_t_sim, label=r'$E[\theta_t \mid D_{T}]$', alpha=0.9, linewidth=0.8)
plt.legend(framealpha=0.4)
plt.tight_layout()
#+END_SRC

#+ATTR_ORG: :width 1000
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Filtered and smoothed \(\theta_t\)--against the true \(\theta_t\)--computed using the SVD approach.
#+RESULTS: fig:svd-steps-sim-plot
[[file:../../figures/svd-steps-sim-plot.png]]

* Forward-filtering Backward-sampling

We can use the smoothing and filtering steps in the previous section to perform
more efficient MCMC estimation than would otherwise be possible without the
Rao-Blackwellization inherent to both steps.

Forward-filtering backward-sampling
[[citep:Fruhwirth-SchnatterDataaugmentationdynamic1994]] works by first
computing the forward filtered moments, allowing one to draw \(\theta_T\) from \(
\left(\theta_T \mid D_T\right) \sim \operatorname{N}\left(m_T, C_T\right) \) and, subsequently,
\(\theta_t\) from
\(\left(\theta_t \mid \theta_{t+1}, D_T \right) \sim \operatorname{N}\left(h_t, H_t\right)\).

The latter distribution's moments are easily derived from the filtered and
smoothed moments:
#+BEGIN_SRC latex
\begin{equation}
  \begin{gathered}
    h_t = m_t + B_t \left(\theta_{t+1} - a_{t+1}\right)
    \\
    H_t = C_t - B_t R_{t+1} B^\top_t
  \end{gathered}
  \label{eq:ffbs-moments}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{gathered}
    h_t = m_t + B_t \left(\theta_{t+1} - a_{t+1}\right)
    \\
    H_t = C_t - B_t R_{t+1} B^\top_t
  \end{gathered}
  \label{eq:ffbs-moments}
\end{equation}
#+end_export

Since all the quantities in [[eqref:eq:ffbs-moments]] appear in the filtering and
smoothing moments, we can use the SVD-based approach described earlier to
perform the updates and sampling.  We reproduce the relevant subset of calculations
in Listing [[svd-ffbs-sampler]].

#+NAME: svd-ffbs-sampler
#+BEGIN_SRC python :results silent
def ffbs_step(m_t, U_C_t, S_C_t, a_tp1, U_R_tp1, S_R_tp1, theta_tp1, F_tp1, G_tp1, N_W_tp1_inv, rng):
    """Perform forward-filtering backward-sampling."""

    S_C_t_inv = tt_finite_inv(S_C_t)

    # H_t_inv = U_C_t @ N_H_t_inv.T @ N_H_t_inv @ U_C_t.T = G_tp1^T @ W_tp1_inv @ G_tp1.T + C_t_inv
    N_H_t_inv = tt.join(0,
                        matrix_dot(N_W_tp1_inv, G_tp1, U_C_t),
                        S_C_t_inv)
    _, d_N_H_t_inv, V_N_H_t_inv_T = svd(N_H_t_inv)

    U_H_t = U_C_t.dot(V_N_H_t_inv_T.T)
    s_H_t = tt_finite_inv(d_N_H_t_inv)

    N_C_t = S_C_t.dot(U_C_t.T)

    S_R_tp1_inv = tt_finite_inv(S_R_tp1)
    N_R_tp1_inv = S_R_tp1_inv.dot(U_R_tp1.T)

    # B_t = C_t @ G_tp1.T @ inv(R_tp1)
    # B_t = matrix_dot(U_H_t * s_H_t, s_H_t * U_H_t.T,
    #                  G_tp1.T, N_W_tp1_inv.T, N_W_tp1_inv)
    B_t = matrix_dot(N_C_t.T, N_C_t, G_tp1.T, N_R_tp1_inv.T, N_R_tp1_inv)

    h_t = m_t + B_t.dot(theta_tp1 - a_tp1)
    h_t.name = 'h_t'

    # TODO: Add an option or optimization to use the SVD to sample in
    # `MvNormalRV`.
    # theta_t = MvNormalRV(h_t, H_t, rng=rng, name='theta_t_ffbs')
    theta_t = h_t + tt.dot(U_H_t, s_H_t *
                           MvNormalRV(tt.zeros_like(h_t),
                                      tt.eye(h_t.shape[0]),
                                      rng=rng))

    # These are statistics we're gathering for other posterior updates
    theta_tp1_diff = theta_tp1 - G_tp1.dot(theta_t)
    f_tp1 = F_tp1.T.dot(theta_t)

    # Sequentially sample/update quantities conditional on `theta_t` here...

    return [theta_t, theta_tp1_diff, f_tp1]


# C_T = matrix_dot(U_C_T, tt.square(S_C_T), U_C_T.T)
# theta_T_post = MvNormalRV(m_T, C_T, rng=rng_tt)
theta_T_post = m_T + matrix_dot(U_C_T, S_C_T,
                                MvNormalRV(tt.zeros_like(m_T),
                                           tt.eye(m_T.shape[0]),
                                           rng=rng_tt))
theta_T_post.name = "theta_T_post"


ffbs_output, ffbs_updates = theano.scan(fn=ffbs_step,
                                        sequences=[
                                            {"input": m_t, "taps": [-1]},
                                            {"input": U_C_t, "taps": [-1]},
                                            {"input": S_C_t, "taps": [-1]},
                                            {"input": a_t, "taps": [1]},
                                            {"input": U_R_t, "taps": [1]},
                                            {"input": S_R_t, "taps": [1]}
                                        ],
                                        outputs_info=[
                                            {"initial": theta_T_post, "taps": [-1]},
                                            {}, {}, # theta_tp1_diff, f_tp1
                                        ],
                                        non_sequences=[F_tt, G_tt, N_W_inv_tt, rng_tt],
                                        go_backwards=True,
                                        strict=True,
                                        name='ffbs_samples')

(theta_t_post_rev, theta_t_diff_rev, f_t_rev) = ffbs_output

theta_t_post = tt.join(0, theta_t_post_rev[::-1], [theta_T_post])

# We need to add the missing end-points onto these statistics...
f_t_post = tt.join(0, f_t_rev[::-1], [F_tt.T.dot(theta_T_post)])
#+END_SRC

** Example
Quantities besides the state values, \(\theta_t\), can be sampled sequentially (i.e.
within the function src_python[:eval never]{ffbs_step} in Listing [[svd-ffbs-sampler]]),
or after FFBS when all \(\theta_t \mid D_T\) have been sampled.  These quantities can use
the conditionally normal form of \(\left(\theta_t \mid \theta_{t+1}, D_T \right)\)
to derive Gibbs steps, further Rao-Blackwellize hierarchical quantities, or
apply any other means of producing posterior samples conditional on
\(\left(\theta_t \mid \theta_{t+1}, D_T \right)\).

In this example, we will augment our original model by adding the classic gamma
priors to our previously fixed state and observation scale parameters, \(\phi_W\)
and \(\phi_V\), respectively.

This classical conjugate prior allows one to derive simple closed-form
posteriors for a Gibbs sampler conditional on \(\theta_t \mid D_T\).
Those posterior computations are defined in Listing [[ffbs-covar-updates]] and
used to update the shared Theano variables for \(\phi_W\) and \(\phi_V\)
within a Gibbs sampling loop in Listing [[ffbs-sim]].

#+NAME: ffbs-covar-updates
#+BEGIN_SRC python :results silent
phi_W_a, phi_W_b = np.r_[2.5, 2.5], np.r_[0.5, 0.5]
phi_V_a, phi_V_b = 0.125, 0.25

phi_W_a_post_tt = phi_W_a + N_obs_tt * 0.5
phi_W_SS_tt = tt.square(theta_t_diff_rev).sum(0)
phi_W_b_post_tt = phi_W_b + 0.5 * phi_W_SS_tt
phi_W_post_tt = GammaRV(phi_W_a_post_tt, phi_W_b_post_tt, rng=rng_tt, name='phi_W_post')

phi_V_a_post_tt = phi_V_a + N_obs_tt * 0.5
phi_V_SS_tt = tt.square(y_tt - f_t_post).sum()
phi_V_b_post_tt = phi_V_b + 0.5 * phi_V_SS_tt
phi_V_post_tt = GammaRV(phi_V_a_post_tt, phi_V_b_post_tt, rng=rng_tt, name='phi_V_post')

ffbs_dlm = tt_function([y_tt, N_obs_tt, N_theta_tt, G_tt, F_tt],
                       [theta_t_post, phi_W_post_tt, phi_V_post_tt,
                        phi_W_SS_tt, phi_V_SS_tt],
                       updates=ffbs_updates)
#+END_SRC

#+NAME: inspect-priors
#+BEGIN_SRC python :eval never :results never :exports none
plt.close('all')

fig, ax = plt.subplots(figsize=(8, 4.8))

x = np.linspace(1e-5, # scipy.stats.gamma.ppf(0.01, phi_W_a[0], scale=1.0/phi_W_b[0]),
                11.0, # scipy.stats.gamma.ppf(0.99, phi_W_a[0], scale=1.0/phi_W_b[0]),
                10000)

plt.cla()

import random

# phi_W_a[0], phi_W_b[0] = 1.0**2/1000.0, 1.0/1000.0
phi_W_a[0], phi_W_b[0]

rnd_smpls = np.asarray([random.gammavariate(phi_W_a[0], 1.0 / phi_W_b[0]) for i in range(10000)])
rnd_smpls.mean()
rnd_smpls.var()

scipy_smpls = scipy.stats.gamma.rvs(phi_W_a[0], scale=1.0 / phi_W_b[0], size=10000)
scipy_smpls.mean()
scipy_smpls.var()

ax.hist(rnd_smpls[rnd_smpls < 20], density=True)
ax.hist(scipy_smpls, density=True)

scipy.stats.gamma.pdf(0.0001, phi_W_a[0], scale=1.0 / phi_W_b[0])

ax.plot(x, scipy.stats.gamma.pdf(x, phi_W_a[0], scale=1.0 / phi_W_b[0]),
        'r-', lw=1, alpha=0.6, label='gamma pdf')
#+END_SRC

#+NAME: ffbs-sim
#+BEGIN_SRC python :results silent
rng_tt.get_value(borrow=True).set_state(rng_sim_state)

phi_W_0 = phi_W_a/phi_W_b
phi_V_0 = phi_V_a/phi_V_b

phi_W_tt.set_value(phi_W_0)
phi_V_tt.set_value(phi_V_0)

chain = 0
theta_label = r'$\theta_t \mid D_T$'
phi_W_label = r'$\phi_W \mid D_T$'
phi_V_label = r'$\phi_V \mid D_T$'
theta_t_post_sim, phi_W_post_sim, phi_V_post_sim = None, None, None
posterior_samples = {theta_label: [[]], phi_W_label: [[]], phi_V_label: [[]]}

for i in range(1000):

    theta_t_post_sim, phi_W_post_sim, phi_V_post_sim, phi_W_SS_sim, phi_V_SS_sim = ffbs_dlm(
        y_sim,
        dlm_sim_values[N_obs_tt], dlm_sim_values[N_theta_tt],
        dlm_sim_values[G_tt], dlm_sim_values[F_tt])

    # Update variance scale parameters
    phi_W_tt.set_value(phi_W_post_sim)
    phi_V_tt.set_value(phi_V_post_sim)

    posterior_samples[theta_label][chain].append(theta_t_post_sim)
    posterior_samples[phi_W_label][chain].append(phi_W_post_sim)
    posterior_samples[phi_V_label][chain].append(phi_V_post_sim)

    print(f'i={i},\tphi_W={phi_W_post_sim}\t({phi_W_SS_sim}),\tphi_V={phi_V_post_sim} ({phi_V_SS_sim})')

posterior_samples = {k: np.asarray(v) for k,v in posterior_samples.items()}
#+END_SRC

Figure [[fig:ffbs-sim-theta-plot]] shows the posterior \(\theta_t\) samples and Figure
[[fig:ffbs-sim-trace-plot]] plots the posterior sample traces.

#+NAME: fig:ffbs-sim-theta-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/ffbs-sim-theta-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))
ax.autoscale(enable=False)

# bivariate_cycler =  cycler('linestyle', ['-', '--']) * plt_orig_cycler
# ax.set_prop_cycle(bivariate_cycler)

thetas_shape = posterior_samples[theta_label][0].shape

cycle = ax._get_lines.prop_cycler

bivariate_obs_cycler =  cycler('linestyle', ['-', '--']) * cycler('color', ['black'])

ax.set_prop_cycle(bivariate_obs_cycler)
ax.plot(theta_t_sim, label=r'$\theta_t$', linewidth=1.0)

ax.autoscale(enable=True)
ax.autoscale(enable=False)

for d in range(thetas_shape[-1]):

    styles = next(cycle)
    thetas = posterior_samples[theta_label][0].T[d].T

    theta_lines = np.empty(thetas_shape[:-1] + (2,))
    theta_lines.T[0] = np.tile(np.arange(thetas_shape[-2]), [thetas_shape[-3], 1]).T
    theta_lines.T[1] = thetas.T

    ax.add_collection(
        LineCollection(theta_lines,
                       label=theta_label,
                       alpha=0.3, linewidth=0.9,
                       ,**styles)
    )

plt.tight_layout()

plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Posterior \(\theta_t\) samples generated by a FFBS-based Gibbs sampler.
#+RESULTS: fig:ffbs-sim-theta-plot
[[file:../../figures/ffbs-sim-theta-plot.png]]

#+NAME: fig:ffbs-sim-trace-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/ffbs-sim-trace-plot.png
import arviz as az

az_trace = az.from_dict(posterior=posterior_samples)
az.plot_trace(az_trace, compact=True)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Posterior sample traces for the FFBS-based Gibbs sampler.
#+RESULTS: fig:ffbs-sim-trace-plot
[[file:../../figures/ffbs-sim-trace-plot.png]]


#+NAME: fig:ffbs-sim-pred-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/ffbs-sim-pred-plot.png
plt.close('all')

fig, ax = plt.subplots(figsize=(8, 4.8))

ax.plot(y_sim, label=r'$y_t$', linewidth=1.0, color='black')

ax.autoscale(enable=True)
ax.autoscale(enable=False)

f_t_ordinates = np.dot(posterior_samples[theta_label][0], dlm_sim_values[F_tt].squeeze())

f_t_lines = np.empty(f_t_ordinates.shape + (2,))
f_t_lines.T[0] = np.tile(np.arange(f_t_ordinates.shape[1]), [f_t_ordinates.shape[0], 1]).T
f_t_lines.T[1] = f_t_ordinates.T

ax.add_collection(
    LineCollection(f_t_lines,
                   label=r'$E[y_t \mid \theta_t, D_T]$',
                   alpha=0.3, linewidth=0.9, color='red')
)

plt.tight_layout()

plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Posterior predicive sample means generated by a FFBS-based Gibbs sampler.
#+RESULTS: fig:ffbs-sim-pred-plot
[[file:../../figures/ffbs-sim-pred-plot.png]]

* Non-Gaussian Example

Let's say we want to model count observations that are driven by a smooth time-varying process.
We can assume a negative-binomial observation model with a log-link function--in standard GLM fashion
[[citep:mccullagh_generalized_1989]]--and connect it to the same state and
observation dynamics as the basic DLM in [[eqref:eq:basic-dlm-state]] via its mean \(\mu_t = \exp\left(\eta_t\right)\):
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    Y_t &\sim \operatorname{NB}\left(r, p_t\right)
    \\
    E[Y_t \mid \theta_t] &= \mu_t = \exp\left( F_t^\top \theta_t \right)
    \\
    \theta_t &= G_t \theta_{t-1} + \nu_t, \quad \nu_t \sim \operatorname{N}\left( 0, W \right)
  \end{aligned}
\label{eq:nb-dlm}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    Y_t &\sim \operatorname{NB}\left(r, p_t\right)
    \\
    E[Y_t \mid \theta_t] &= \mu_t = \exp\left( F_t^\top \theta_t \right)
    \\
    \theta_t &= G_t \theta_{t-1} + \nu_t, \quad \nu_t \sim \operatorname{N}\left( 0, W \right)
  \end{aligned}
\label{eq:nb-dlm}
\end{equation}
#+end_export

Under the parameterization \(p_t = \frac{\mu_t}{\mu_t + r}\), the
negative-binomial density function takes the following form:
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    \operatorname{p}\left(Y_t = y_t \mid r, p_t\right) &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \left( 1 - p_t \right)^r \left( p_t \right)^{y_t}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \left( \frac{r}{r + \mu_t} \right)^r \left( \frac{\mu_t}{r + \mu_t} \right)^{y_t}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \frac{\left( \mu_t / r \right)^{y_t}}{\left( 1 + \mu_t / r \right)^{r + y_t}}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \frac{\left( e^{\eta_t - \log r} \right)^{y_t}}{\left( 1 + e^{\eta_t - \log r} \right)^{r + y_t}}
    .
  \end{aligned}
\label{eq:nb-pmf}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    \operatorname{p}\left(Y_t = y_t \mid r, p_t\right) &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \left( 1 - p_t \right)^r \left( p_t \right)^{y_t}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \left( \frac{r}{r + \mu_t} \right)^r \left( \frac{\mu_t}{r + \mu_t} \right)^{y_t}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \frac{\left( \mu_t / r \right)^{y_t}}{\left( 1 + \mu_t / r \right)^{r + y_t}}
    \\
    &=
    \frac{\Gamma\left( y_t + r \right)}{y_t!\,\Gamma(r)}
    \frac{\left( e^{\eta_t - \log r} \right)^{y_t}}{\left( 1 + e^{\eta_t - \log r} \right)^{r + y_t}}
    .
  \end{aligned}
\label{eq:nb-pmf}
\end{equation}
#+end_export

The logit-inverse form in [[eqref:eq:nb-pmf]] has a normal scale-mixture
representation in the PoÌlya-Gamma distribution [[citep:polson_bayesian_2013]].
Said scale-mixture is as follows:
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    \frac{e^{\psi a}}{\left(1 + e^{\psi}\right)^b} &=
    2^{-b} e^{\kappa \psi} \int^{\infty}_{0} e^{-\frac{\omega}{2} \psi^2} \operatorname{p}(\omega) d\omega
    \\
    &=
    2^{-b} \int^{\infty}_{0} e^{-\frac{\omega}{2} \left( \psi - \frac{\kappa}{\omega} \right)^2}
      e^{\frac{\kappa^2}{2 \omega} } \operatorname{p}(\omega) d\omega
    .
  \end{aligned}
\label{eq:pg-identity}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    \frac{e^{\psi a}}{\left(1 + e^{\psi}\right)^b} &=
    2^{-b} e^{\kappa \psi} \int^{\infty}_{0} e^{-\frac{\omega}{2} \psi^2} \operatorname{p}(\omega) d\omega
    \\
    &=
    2^{-b} \int^{\infty}_{0} e^{-\frac{\omega}{2} \left( \psi - \frac{\kappa}{\omega} \right)^2}
      e^{\frac{\kappa^2}{2 \omega} } \operatorname{p}(\omega) d\omega
    .
  \end{aligned}
\label{eq:pg-identity}
\end{equation}
#+end_export

where \(\kappa = a - b / 2\) and \(\omega_t \sim \operatorname{PG}\left(b, 0\right)\).

When the normal scale-mixture identity of [[eqref:eq:pg-identity]] is applied to our observation model in
[[eqref:eq:nb-pmf]], \(a = y_t\), \(b = r + y_t\), \(\kappa = (y_t - r)/2\), and \(\psi = \eta_t - \log r\).

From the scale mixture formulation, we obtain the following augmented joint
observation model density:
#+BEGIN_SRC latex
\begin{equation}
  \begin{aligned}
    \operatorname{p}(\theta_t \mid \omega, y_t, r, D_{t-1}) &\propto
    \exp\left(-\frac{\omega}{2} \left( F_t^\top \theta_t - \left( \log r + \frac{y_t - r}{2 \omega} \right) \right)^2 \right)
    \\
    &=
    e^{-\frac{\omega}{2} \left(y^*_t - F_t^\top \theta_t \right)^2}
    \\
    &\propto \operatorname{p}\left( y^*_t \mid F_t^\top \theta_t, \omega^{-1} \right)
  \end{aligned}
\label{eq:nb-aug-obs}
\end{equation}
#+END_SRC

#+RESULTS:
#+begin_export html
\begin{equation}
  \begin{aligned}
    \operatorname{p}(\theta_t \mid \omega, y_t, r, D_{t-1}) &\propto
    \exp\left(-\frac{\omega}{2} \left( F_t^\top \theta_t - \left( \log r + \frac{y_t - r}{2 \omega} \right) \right)^2 \right)
    \\
    &=
    e^{-\frac{\omega}{2} \left(y^*_t - F_t^\top \theta_t \right)^2}
    \\
    &\propto \operatorname{p}\left( y^*_t \mid F_t^\top \theta_t, \omega^{-1} \right)
  \end{aligned}
\label{eq:nb-aug-obs}
\end{equation}
#+end_export

where \(y^*_t = \log r + \frac{y_t - r}{2 \omega}\).

The density of our "virtual" observations, \(y^*_t\), takes the form of a normal
distribution, which fits nicely into our DLM framework.  All that's needed to
incorporate this scale-mixture augmentation to our FFBS sampler is a step that
samples \(\omega_t \sim \operatorname{PG}\left(r + y_t, F_t^\top \theta_t - \log r \right)\).

** Simulation

#+NAME: nb-example-requirements
#+BEGIN_SRC python :eval never-export :noweb no-export :results silent
<<basic-imports>>
<<basic-normal-model>>
<<basic-dlm-sim>>
<<linalg-theano-ops>>
<<filter-svd-scan>>
<<smoother-svd-scan>>
<<svd-ffbs-sampler>>
<<ffbs-covar-updates>>
#+END_SRC

Listing [[nb-obs-model]] creates a Theano graph for negative-binomial model defined
in [[eqref:eq:nb-dlm]].

#+NAME: nb-obs-model
#+BEGIN_SRC python :results silent
from symbolic_pymc.theano.random_variables import NegBinomialRV


r_tt = tt.iscalar('r')


def nb_obs_step(theta_t, F_t, r, rng):
    mu_t = tt.exp(F_t.T.dot(theta_t))
    p_t = mu_t / (mu_t + r)
    y_t = NegBinomialRV(r, (1. - p_t), rng=rng, name='y_t')
    return y_t, p_t


nb_obs_res, nb_Y_t_updates = theano.scan(fn=nb_obs_step,
                                         sequences=[theta_t_rv],
                                         non_sequences=[F_tt, r_tt, rng_tt],
                                         outputs_info=[
                                             {}, {}, # y_t, p_t
                                         ],
                                         strict=True,
                                         name='Y_t')

nb_Y_t_rv, nb_p_t_tt = nb_obs_res
#+END_SRC

Listing [[nb-obs-sim]] specifies parameters for a simulation from [[eqref:eq:nb-dlm]]
and samples a series.

#+NAME: nb-obs-sim
#+BEGIN_SRC python :results silent
nb_dlm_sim_values = dlm_sim_values.copy()
nb_dlm_sim_values[F_tt] = np.array([[1.0],
                                    [0.0]], dtype=theano.config.floatX)
nb_dlm_sim_values[G_tt] = np.array([[1.0, 0.1],
                                    [0.0, 0.8]], dtype=theano.config.floatX)
nb_dlm_sim_values[r_tt] = 1000

phi_W_tt.set_value(np.r_[10.0, 10.0])

rng_tt.get_value(borrow=True).set_state(rng_init_state)

simulate_nb_dlm = tt_function([N_obs_tt, N_theta_tt, G_tt, F_tt, r_tt],
                              [nb_Y_t_rv, theta_t_rv, nb_p_t_tt],
                              givens={theta_0_rv: np.r_[1.0, 0.5]},
                              updates=nb_Y_t_updates)

sim_nb_res = simulate_nb_dlm(nb_dlm_sim_values[N_obs_tt],
                             nb_dlm_sim_values[N_theta_tt],
                             nb_dlm_sim_values[G_tt],
                             nb_dlm_sim_values[F_tt],
                             nb_dlm_sim_values[r_tt])

nb_y_sim, nb_theta_t_sim, nb_p_t_sim = sim_nb_res
#+END_SRC

In Figure [[fig:nb-dlm-sim-plot-fig]] we plot the sample generated in Listing
[[nb-obs-model]].

#+NAME: fig:nb-dlm-sim-plot-fig
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-dlm-sim-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))
_ = ax.plot(nb_y_sim, label=r'$y_t$', color='black', linewidth=1.2, drawstyle='steps-pre')

plt.tight_layout()
plt.legend()
#+END_SRC

#+ATTR_ORG: :width 700
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: A series sampled from our negative-binomial model defined in [[eqref:eq:nb-dlm]].
#+RESULTS: fig:nb-dlm-sim-plot-fig
[[file:../../figures/nb-dlm-sim-plot.png]]


#+NAME: fig:nb-theta-sim-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-theta-sim-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))
ax.autoscale(enable=False)

bivariate_obs_cycler =  cycler('linestyle', ['-', '--']) * cycler('color', ['black'])

ax.set_prop_cycle(bivariate_obs_cycler)
ax.plot(nb_theta_t_sim, label=r'$\theta_t$', linewidth=1.0)

ax.autoscale(enable=True)

plt.tight_layout()

plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Simulated \(\theta_t\) values from [[eqref:eq:nb-dlm]].
#+RESULTS: fig:nb-theta-sim-plot
[[file:../../figures/nb-theta-sim-plot.png]]

#+NAME: fig:nb-p-sim-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-p-sim-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))

ax.plot(nb_p_t_sim, label=r'$p_t$', linewidth=1.0, color='black')

plt.tight_layout()
plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Simulated \(p_t \mid \theta_t\) values from [[eqref:eq:nb-dlm]].
#+RESULTS: fig:nb-p-sim-plot
[[file:../../figures/nb-p-sim-plot.png]]

** Augmented FFBS Sampler

In order to create a FFBS sampler for our PoÌlya-Gamma DLM in
[[eqref:eq:nb-aug-obs]], we need to update our filtering code so that it can take
time-varying "virtual" observation variances, \(V_t\).  After this change is
made, all Theano graphs that depend on the resulting objects need to be
recreated, as well.  This is done in Listing [[nb-aug-obs]].

#+NAME: nb-aug-obs
#+BEGIN_SRC python :results silent
V_t_tt = tt.specify_shape(tt.col(), [N_obs_tt, 1])
V_t_tt.name = 'V_t'


def nb_filtering_step(y_t, V_t, m_tm1, U_C_tm1, S_C_tm1, F_t, G_t, N_W_t):
    N_V_t_inv = tt.diag(tt_finite_inv(tt.sqrt(V_t), eps_truncate=True))
    return filtering_step(y_t, m_tm1, U_C_tm1, S_C_tm1, F_t, G_t, N_W_t, N_V_t_inv)


filter_res, filter_updates = theano.scan(fn=nb_filtering_step,
                                         sequences=[y_tt, V_t_tt],
                                         outputs_info=[
                                             {"initial": m_0_tt, "taps": [-1]},
                                             {"initial": U_C_0_tt, "taps": [-1]},
                                             {"initial": S_C_0_tt, "taps": [-1]},
                                             {}, {}, {}  # a_t, U_R_t, S_R_t
                                         ],
                                         non_sequences=[F_tt, G_tt, N_W_tt],
                                         strict=True,
                                         name='theta_filtered')

(m_t, U_C_t, S_C_t, a_t, U_R_t, S_R_t) = filter_res

m_T = m_t[-1]
U_C_T = U_C_t[-1]
S_C_T = S_C_t[-1]

C_T = matrix_dot(U_C_T, tt.square(S_C_T), U_C_T.T)
theta_T_post = MvNormalRV(m_T, C_T, rng=rng_tt)
theta_T_post.name = "theta_T_post"

ffbs_output, ffbs_updates = theano.scan(fn=ffbs_step,
                                        sequences=[
                                            {"input": m_t, "taps": [-1]},
                                            {"input": U_C_t, "taps": [-1]},
                                            {"input": S_C_t, "taps": [-1]},
                                            {"input": a_t, "taps": [1]},
                                            {"input": U_R_t, "taps": [1]},
                                            {"input": S_R_t, "taps": [1]}
                                        ],
                                        outputs_info=[
                                            {"initial": theta_T_post, "taps": [-1]},
                                            {}, {}, # theta_tp1_diff, f_tp1
                                        ],
                                        non_sequences=[F_tt, G_tt, N_W_inv_tt, rng_tt],
                                        go_backwards=True,
                                        strict=True,
                                        name='ffbs_samples')

(theta_t_post_rev, theta_t_diff_rev, f_t_rev) = ffbs_output

theta_t_post = tt.join(0, theta_t_post_rev[::-1], [theta_T_post])

f_t_post = tt.join(0, f_t_rev[::-1], [F_tt.T.dot(theta_T_post)])

phi_W_a_post_tt = phi_W_a + N_obs_tt * 0.5
phi_W_b_post_tt = phi_W_b + 0.5 * tt.square(theta_t_diff_rev).sum(0)
phi_W_post_tt = GammaRV(phi_W_a_post_tt, phi_W_b_post_tt, rng=rng_tt, name='phi_W_post')
#+END_SRC

Next, in Listing [[nb-ffbs-setup]] we sample the initial values and create Theano
terms for posterior/updated \(\omega_t\)-related values.

#+NAME: nb-ffbs-setup
#+BEGIN_SRC python :results silent
from pypolyagamma import PyPolyaGamma

from symbolic_pymc.theano.random_variables import PolyaGammaRV


r_sim = np.array(nb_dlm_sim_values[r_tt], dtype='double')

# XXX: testing
F_t_theta_0 = np.dot(nb_theta_t_sim, nb_dlm_sim_values[F_tt])

omega_0 = np.empty(nb_y_sim.shape[0], dtype='double')
PyPolyaGamma(12344).pgdrawv(r_sim + nb_y_sim.squeeze(),
                            F_t_theta_0.squeeze() - np.log(r_sim),
                            omega_0)
omega_0 = np.expand_dims(omega_0, -1)

y_aug_0 = np.log(r_sim) + (nb_y_sim - r_sim) / (2.0 * omega_0)

omega_t_tt = theano.shared(omega_0, name='omega_t')

omega_post_tt = PolyaGammaRV(r_tt + nb_y_sim, theta_t_post.dot(F_tt) - tt.log(r_tt), rng=rng_tt, name='omega_post')

y_aug_post_tt = tt.log(r_tt) + (nb_y_sim - r_tt) / (2.0 * omega_post_tt)
y_aug_post_tt.name = 'y_aug_post'
#+END_SRC

Finally, the sampler steps are defined and executed in Listing [[nb-ffbs-sim]].

#+NAME: nb-ffbs-sim
#+BEGIN_SRC python :results silent
nb_ffbs_dlm = tt_function([N_obs_tt, N_theta_tt, y_tt, G_tt, F_tt, V_t_tt, r_tt],
                          [theta_t_post, phi_W_post_tt, omega_post_tt, y_aug_post_tt],
                          updates=ffbs_updates)

rng_tt.get_value(borrow=True).set_state(rng_sim_state)

phi_W_tt.set_value(np.r_[10.0, 10.0])

chain = 0

theta_label = r'$\theta_t \mid D_T$'
phi_W_label = r'$\phi_W \mid D_T$'
omega_label = r'$\omega_t \mid D_T$'

theta_t_post_sim, phi_W_post_sim, omega_post_sim, y_aug_post_sim = None, None, None, None
nb_posterior_samples = {theta_label: [[]], phi_W_label: [[]], omega_label: [[]]}

V_t_sim = np.reciprocal(omega_0)
y_aug_sim = y_aug_0

for i in range(1000):

    nb_ffbs_res = nb_ffbs_dlm(
        nb_dlm_sim_values[N_obs_tt],
        nb_dlm_sim_values[N_theta_tt],
        y_aug_sim,
        nb_dlm_sim_values[G_tt],
        nb_dlm_sim_values[F_tt],
        V_t_sim,
        nb_dlm_sim_values[r_tt])

    theta_t_post_sim, phi_W_post_sim, omega_post_sim, y_aug_post_sim = nb_ffbs_res

    phi_W_tt.set_value(phi_W_post_sim)
    omega_t_tt.set_value(omega_post_sim)

    V_t_sim = np.reciprocal(omega_post_sim)
    y_aug_sim = y_aug_post_sim

    nb_posterior_samples[theta_label][chain].append(theta_t_post_sim)
    nb_posterior_samples[phi_W_label][chain].append(phi_W_post_sim)
    nb_posterior_samples[omega_label][chain].append(omega_post_sim)

    print(f'i={i},\tphi_W={phi_W_post_sim}')

nb_posterior_samples = {k: np.asarray(v) for k,v in nb_posterior_samples.items()}
#+END_SRC

Figure [[fig:nb-ffbs-sim-plot]] shows the posterior \(\theta_t\) samples, Figure
[[fig:nb-ffbs-trace-plot]] plots the posterior sample traces, and Figure
[[fig:nb-ffbs-sim-pred-plot]] shows \(p_t \mid \theta_t\).

#+NAME: fig:nb-ffbs-sim-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-ffbs-sim-plot.png
plt.clf()

fig, ax = plt.subplots(figsize=(8, 4.8))
ax.autoscale(enable=False)

thetas_shape = nb_posterior_samples[theta_label][0].shape

cycle = ax._get_lines.prop_cycler

bivariate_obs_cycler =  cycler('linestyle', ['-', '--']) * cycler('color', ['black'])

ax.set_prop_cycle(bivariate_obs_cycler)
ax.plot(nb_theta_t_sim, label=r'$\theta_t$', linewidth=1.0)

ax.autoscale(enable=True)
ax.autoscale(enable=False)

for d in range(thetas_shape[-1]):

    styles = next(cycle)
    thetas = nb_posterior_samples[theta_label][0].T[d].T

    theta_lines = np.empty(thetas_shape[:-1] + (2,))
    theta_lines.T[0] = np.tile(np.arange(thetas_shape[-2]), [thetas_shape[-3], 1]).T
    theta_lines.T[1] = thetas.T

    ax.add_collection(
        LineCollection(theta_lines,
                       label=theta_label,
                       alpha=0.05, linewidth=0.9,
                       ,**styles)
    )

plt.tight_layout()

plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Posterior \(\theta_t\) samples generated by our Polya-Gamma FFBS sampler.
#+RESULTS: fig:nb-ffbs-sim-plot
[[file:../../figures/nb-ffbs-sim-plot.png]]


#+NAME: fig:nb-ffbs-trace-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-ffbs-trace-plot.png
import arviz as az

az_trace = az.from_dict(posterior=nb_posterior_samples)
az.plot_trace(az_trace, compact=True)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: Posterior sample traces for our Polya-Gamma FFBS Gibbs sampler.
#+RESULTS: fig:nb-ffbs-trace-plot
[[file:../../figures/nb-ffbs-trace-plot.png]]


#+NAME: fig:nb-ffbs-sim-pred-plot
#+BEGIN_SRC python :results graphics file :file ../../figures/nb-ffbs-sim-pred-plot.png
plt.close('all')

fig, ax = plt.subplots(figsize=(8, 4.8))

ax.plot(nb_p_t_sim, label=r'$p_t$', linewidth=1.0, color='black')

ax.autoscale(enable=True)
ax.autoscale(enable=False)

mu_t_sim = np.exp(np.dot(nb_posterior_samples[theta_label][0], nb_dlm_sim_values[F_tt].squeeze()))
p_t_sim = mu_t_sim / (mu_t_sim + nb_dlm_sim_values[r_tt])

p_t_lines = np.empty(p_t_sim.shape + (2,))
p_t_lines.T[0] = np.tile(np.arange(p_t_sim.shape[1]), [p_t_sim.shape[0], 1]).T
p_t_lines.T[1] = p_t_sim.T

ax.add_collection(
    LineCollection(p_t_lines,
                   label=r'$p_t \mid \theta_t, D_T$',
                   alpha=0.3, linewidth=0.9, color='red')
)

plt.tight_layout()

plt.legend(framealpha=0.4)
#+END_SRC

#+ATTR_ORG: :width 900
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION: \(p_t \mid \theta_t, D_T\) samples generated by our Polya-Gamma FFBS sampler.
#+RESULTS: fig:nb-ffbs-sim-pred-plot
[[file:../../figures/nb-ffbs-sim-pred-plot.png]]
* Theano Optimizations                                             :noexport:

Another reason to use src_python[:eval never]{scan} is that it comes with a
number of symbolic simplifications that can result in better estimates and/or
performance.

Let's start by simply canonicalizing the model's graph.

#+NAME: basic-dlm-canon
#+BEGIN_SRC python :results silent
from theano.gof.graph import inputs as tt_inputs

from symbolic_pymc.theano.utils import canonicalize


Y_t_opt = canonicalize(Y_t_rv, in_place=False)
#+END_SRC

#+NAME: basic-dlm-canon-dprint
#+BEGIN_SRC python :wrap "SRC python :eval never"
tt_dprint(Y_t_opt, depth=5)
#+END_SRC

#+RESULTS: basic-dlm-canon-dprint
#+begin_SRC python :eval never
for{cpu,Y} [id A] ''
 |Subtensor{int64} [id B] ''
 | |Shape [id C] ''
 | | |Subtensor{int64:int64:int8} [id D] ''
 | |   |for{cpu,theta} [id E] ''
 | |   |ScalarFromTensor [id F] ''
 | |   |ScalarFromTensor [id G] ''
 | |   |Constant{1} [id H]
 | |Constant{0} [id I]
 |Subtensor{int64:int64:int64} [id J] ''
 | |for{cpu,theta} [id E] ''
 | |ScalarFromTensor [id K] ''
 | | |Elemwise{switch,no_inplace} [id L] ''
 | |   |Elemwise{le,no_inplace} [id M] ''
 | |   |TensorConstant{0} [id N]
 | |   |Elemwise{minimum,no_inplace} [id O] ''
 | |ScalarFromTensor [id P] ''
 | | |Elemwise{switch,no_inplace} [id Q] ''
 | |   |Elemwise{le,no_inplace} [id M] ''
 | |   |TensorConstant{0} [id N]
 | |   |Elemwise{minimum,no_inplace} [id R] ''
 | |Constant{1} [id S]
 |Subtensor{int64} [id B] ''
 |rng [id T]
 |F_tt [id U]
 |invgamma_rv.1 [id V] 'eps_scale'
   |TensorConstant{0.5} [id W]
   |TensorConstant{0.5} [id W]
   |TensorConstant{1.0} [id X]
   |TensorConstant{[]} [id Y]
   |rng [id T]

Inner graphs of the scan ops:

for{cpu,Y} [id A] ''
 >Elemwise{add,no_inplace} [id Z] ''
 > |InplaceDimShuffle{0} [id BA] ''
 > | |Dot22 [id BB] ''
 > |   |F_tt_copy [id BC] -> [id U]
 > |   |InplaceDimShuffle{0,x} [id BD] ''
 > |     |<TensorType(float64, vector)> [id BE] -> [id J]
 > |InplaceDimShuffle{x} [id BF] ''
 >   |normal_rv.1 [id BG] 'eps'
 >     |TensorConstant{0} [id BH]
 >     |eps_scale_copy [id BI] -> [id V]
 >     |TensorConstant{[]} [id BJ]
 >     |rng_copy [id BK] -> [id T]

for{cpu,theta} [id E] ''
 >Elemwise{add,no_inplace} [id BL] ''
 > |InplaceDimShuffle{0} [id BM] ''
 > | |Dot22 [id BN] ''
 > |   |G_tt_copy [id BO] -> [id BP]
 > |   |InplaceDimShuffle{0,x} [id BQ] ''
 > |     |theta_0[t-1] [id BR] -> [id BS]
 > |multivariate_normal_rv.1 [id BT] 'nu'
 >   |Elemwise{second,no_inplace} [id BU] ''
 >   | |theta_0[t-1] [id BR] -> [id BS]
 >   | |TensorConstant{(1,) of 0.0} [id BV]
 >   |<TensorType(float64, matrix)> [id BW] -> [id BX]
 >   |TensorConstant{[]} [id BY]
 >   |rng_copy [id BZ] -> [id T]

for{cpu,theta} [id E] ''
 >Elemwise{add,no_inplace} [id BL] ''


#+end_SRC

The canonicalized graph clones the original variables, so we need to use those from now on.

#+NAME: basic-dlm-canon-remap-values
#+BEGIN_SRC python :results silent
names_to_inputs = {i.name: i for i in tt_inputs([Y_t_opt])}

dlm_opt_sim_values = {names_to_inputs[k.name]: v for k, v in dlm_sim_values.items()}
#+END_SRC

We can reset the seed and recompute the simulated values to confirm that our
canonicalized graph is--numerically--the same as our original graph.

#+NAME: basic-dlm-canon-sim
#+BEGIN_SRC python :results silent
rng_state = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1234)))
rng.set_value(rng_state)

y_opt_sim = Y_t_opt.eval(dlm_opt_sim_values)

assert np.allclose(y_opt_sim, y_sim)
#+END_SRC

#+NAME: basic-dlm-canon-sim-plot
#+BEGIN_SRC python :results silent
plt.plot(y_opt_sim, label='y_opt_sim')
plt.legend()
#+END_SRC

Considering the prior predictive equations, how might we go about
transforming the graphs above so that they represent the "implied
distributions" like the prior predictive?

More specifically, consider the src_python[:eval never]{scan} sub-graphs
(i.e. labeled by src_python[:eval never]{for} in the debug print-outs).  These
graphs represent the evolution equations and contain terms like
\(G_t \theta_{t-1} + \nu_t\), which we know correspond to prior predictive
distributions like \(\theta_t \mid D_{t-1}\).

Really, there is no reason to leave terms like \(G_t \theta_{t-1} + \nu_t\)
in that form; instead, we can prefer a "canonical" form of our terms that
better represent everything we know about them.  In this case, we know
that the term is the random variable \(\theta_t \mid D_{t-1}\), we know what
its moments are, and we clearly have a means of codifying that in Theano.

This leads us to the need for random variable-specific canonicalizations
in Theano.  These canonicalizations will apparently involve some general
properties of random variables, such as
\(A \epsilon \sim \operatorname{N}\left(A \mu, A^\top \Sigma^2 A \right)\)
for \(\epsilon \sim \operatorname{N}\left(0, \Sigma^2\right)\), but
it will also involve some Theano-specific details that aren't always clearly
mapped to mathematical properties.

For instance, in our canonicalized model graph,
new src_python[:eval never]{InplaceDimShuffle} operations appear within the
sub-graphs corresponding to our \(G_t \theta_{t-1} + \nu_t\) term.  These
operations manipulate the dimensions of terms and correspond to simple
[[https://en.wikipedia.org/wiki/Tensor_reshaping][tensor reshaping]].
With an understanding of the relationship between these algebraic
properties and the operators/implementations in Theano, we can construct Theano
optimizations that produce robust canonical terms for random variables
(i.e. terms that are more amenable to performance or accuracy optimizations).

* Random Variable Canonicalization                                 :noexport:

Let's start with some optimizations that will produce prior predictive
distributions.  The prior predictive moments are derived from a few simple
linear algebraic and probability theoretic properties.  Namely,
the following identities:
#+NAME: canon-identities
\begin{align}
  A \beta + b &\sim \operatorname{N}\left( b + \mu, A^\top \Sigma^2 A \right), \quad
  \beta \sim \operatorname{N}\left( \mu, \Sigma^2 \right)
  \\
  \beta + \epsilon &\sim \operatorname{N}\left( \mu + \nu, \Sigma^2 + \Omega^2 \right), \quad
  \epsilon \sim \operatorname{N}\left( \nu, \Omega^2 \right)
\end{align}

These identities can be interpreted as replacement rules from left-to-right, so that their
application results in more random variable forms in a graph.  The effect of these rules
is that linear algebraic operations are "lifted" into the arguments of random variables.

#+NAME: kanren-normal-imports
#+BEGIN_SRC python :results silent
from operator import add
from functools import partial

from unification import var

from etuples import etuple

from theano.tensor.nlinalg import matrix_dot, matrix_inverse

from kanren import run, eq
from kanren.core import lall, conde
from kanren.graph import reduceo, walko, applyo
from kanren.constraints import isinstanceo

from symbolic_pymc.meta import MetaSymbol
from symbolic_pymc.theano.ops import RandomVariable
from symbolic_pymc.theano.meta import mt, TheanoMetaTensorVariable, TheanoMetaTensorConstant, TheanoMetaApply
#+END_SRC

#+NAME: kanren-normal-helpers
#+BEGIN_SRC python :results silent
def tt_at_least_nd(x, n=1):

    if isinstance(x, MetaSymbol):
        x = x.reify()

    ndim = getattr(x, 'ndim', None)

    if ndim < n:
        return x.dimshuffle(*(['x'] * n))
    else:
        return x

#+END_SRC

#+NAME: kanren-normal-helpers-tests
#+BEGIN_SRC python :exports none :results silent
assert np.array_equal(tt_at_least_nd(tt.as_tensor_variable(1.0), n=1).eval(), np.r_[1.0])
assert np.array_equal(tt_at_least_nd(tt.as_tensor_variable([1.0]), n=1).eval(), np.r_[1.0])
assert np.array_equal(tt_at_least_nd(tt.as_tensor_variable(1.0), n=2).eval(), np.c_[[1.0]])
assert np.array_equal(tt_at_least_nd(tt.as_tensor_variable([1.0]), n=2).eval(), np.c_[[1.0]])
#+END_SRC

#+NAME: kanren-normal-canonicalizations
#+BEGIN_SRC python :results silent
from kanren.goals import permuteo


def normal_lifto(in_expr, out_expr):
    """Create a goal that lifts normal random variable operations."""
    A, b = var(), var()
    mu, Sigma, sd = var(), var(), var()
    size, rng, name = var(), var(), var()

    var_op_lv = var()
    var_apply_lv = TheanoMetaTensorVariable(var(), TheanoMetaApply(var_op_lv, var(), var()), var(), var())

    mu_2, Sigma_2, rng_2, size_2, name_2 = [var() for i in range(5)]

    ds_input = var()
    ds_in_args_1 = (var(), var(), var())
    ds_in_args_2 = (var(), var(), var())

    return conde(
        [
            # tt.squeeze(A.dimshuffle('x')) == A
            eq(in_expr, mt.DimShuffle(*ds_in_args_1)(mt.DimShuffle(*ds_in_args_2)(ds_input))),
            dbgo((ds_in_args_1, ds_in_args_2), msg='squeeze/dimshuffle'),
            permuteo((ds_in_args_1, ds_in_args_2), (((True,), (), var()), ((), ('x',), var()))),
            eq(out_expr, ds_input)
        ],
        [
            # Univariate normals to multivariates
            #
            # Canonicalization should remove inverses like the following:
            #
            # sqd_test = tt.squeeze(tt.scalar('a').dimshuffle('x'))
            #
            # sqd_canon = canonicalize(sqd_test)
            #
            # tt_dprint(sqd_test)
            # tt_dprint(sqd_canon)
            eq(in_expr, mt.NormalRV(mu, sd, size, rng, name=name)),
            eq(out_expr, etuple(mt.squeeze,
                                etuple(mt.MvNormalRV,
                                       etuple(tt_at_least_nd, mu),
                                       etuple(tt_at_least_nd, sd, 2),
                                       size, rng, name=name))),
        ],
        [
            # MvNormal convolution
            eq(in_expr, mt.add(mt.MvNormalRV(mu, Sigma, size, rng, name=name),
                               mt.MvNormalRV(mu_2, Sigma_2, size_2, rng_2, name=name_2))),
            eq(out_expr, etuple(mt.MvNormalRV,
                                mt.add(mu, mu_2),
                                mt.add(Sigma, Sigma_2),
                                size, rng, name=etuple(add, name, '-', name_2))),
        ],
        [
            # Constant addition
            isinstanceo(b, TheanoMetaTensorConstant),
            eq(in_expr, mt.add(b, mt.MvNormalRV(mu, Sigma, size, rng, name=name))),
            eq(out_expr, mt.MvNormalRV(mt.add(b, mu), Sigma, size, rng, name=name)),
        ],
        [
            # Constant dot product
            isinstanceo(A, TheanoMetaTensorConstant),
            eq(in_expr, mt.dot(A, mt.MvNormalRV(mu, Sigma, size, rng, name=name))),
            eq(out_expr, etuple(mt.MvNormalRV,
                                etuple(mt.dot, A, mu),
                                etuple(matrix_dot, etuple(tt.transpose, A), Sigma, A),
                                size, rng, name=name))
        ],
    )

#+END_SRC

#+NAME: test-normal-canonicalizations
#+BEGIN_SRC python :exports none :results silent
test_norm_1_tt = NormalRV(0, 1)
# test_norm_1_tt = NormalRV(0, 1, size=(2, 3))

q_lv = var()
res = run(0, q_lv, walko(partial(reduceo, normal_lifto), test_norm_1_tt, q_lv))

uni_to_mv_norm_tt = res[0].eval_obj.reify()

assert isinstance(uni_to_mv_norm_tt.owner.op, tt.DimShuffle)

from symbolic_pymc.theano.random_variables import MvNormalRVType

mv_owner = uni_to_mv_norm_tt.owner.inputs[0].owner

assert isinstance(mv_owner.op, MvNormalRVType)
assert mv_owner.inputs[0].ndim == 1
assert mv_owner.inputs[1].ndim == 2
assert np.array_equal(mv_owner.inputs[2].data, [])
assert uni_to_mv_norm_tt.owner.inputs[0].name == test_norm_1_tt.name
#+END_SRC

#+NAME: inspect-scan-properties
#+BEGIN_SRC python :exports none :results silent

tt_dprint(theta_t_rv)

theta_scan_op = theta_t_rv.owner.inputs[0].owner.op
theta_scan_inputs = theta_t_rv.owner.inputs[0].owner.inputs

# Why isn't the initial value here?!
theta_scan_op.info

tt_dprint(theta_scan_inputs)

# Looks like this is the closest we'll get to the initial value's
# (i.e. `theta[0]`) graph.
# It's wrapped in a `theano.scan_module.scan_utils.expand_empty` call.
# More specifically this graph is created by
# theano.scan_module.scan_utils.expand_empty(
#     tt.unbroadcast(tensor.shape_padleft(actual_arg), 0),
#     actual_n_steps)
tt_dprint(theta_scan_inputs[1])

theta_tm1 = theta_scan_op.inputs[0]

tt_dprint(theta_scan_op.inputs)
tt_dprint(theta_scan_op.outputs)

# Note: `tt.second(x, y)` allocates a tensor with shape `x.shape` with values `y`
# E.g. `tt.second(np.c_[[1, 3], [4, 5]], 0)`
#+END_SRC

#+NAME: test-scan-canonicalization
#+BEGIN_SRC python :exports none :results silent

def exprs_in_scan_output(in_expr, out_expr):
    inputs_lv, info_lv = var(), var()
    scan_output_in, scan_output_out = var(), var()

    in_scan_lv = mt.Scan(inputs_lv, [scan_output_in], info_lv)
    out_scan_lv = etuple(mt.Scan, inputs_lv, scan_output_out, info_lv)

    def make_list(x):
        return [x]

    scan_output_out_raw = var()

    # XXX: Why isn't this matching?!
    ds_input = var()
    ds_in_args_1 = (var(), var(), var())
    ds_in_args_2 = (var(), var(), var())
    pat_expr = mt.DimShuffle(*ds_in_args_1)(mt.DimShuffle(*ds_in_args_2)(ds_input))


    return conde([normal_lifto(in_expr, out_expr)],
                 [
                     eq(in_expr, pat_expr),
                     # XXX: This case should be present!
                     dbgo((ds_in_args_1, ds_in_args_2), msg='dimshuffle')
                 ],
                 [
                     eq(in_expr, in_scan_lv),
                     walko(partial(reduceo, exprs_in_scan_output), scan_output_in, scan_output_out_raw),
                     # Force etuple results to evaluate
                     applyo(make_list, etuple(scan_output_out_raw), scan_output_out),
                     eq(out_expr, out_scan_lv)
                 ])


q_lv = var()
output_mt = run(1, q_lv, walko(partial(reduceo, exprs_in_scan_output), Y_t_rv, q_lv))

output_new = output_mt[0].eval_obj.reify()


tt_dprint(output_new)

output_new_canon = canonicalize(output_new)

tt_dprint(output_new_canon)


# XXX: What's the deal with this missed squeeze/dimshuffle?
theta_scan_output = output_new.owner.op.outputs[0]
tt_dprint(theta_scan_output)

in_expr = mt(theta_scan_output.owner.inputs[1])

in_expr.owner.op.rands

unify(in_expr, pat_expr)
#+END_SRC

#+NAME: canon-rules-opt
#+BEGIN_SRC python :results silent
from symbolic_pymc.theano.ops import RandomVariable

normal_rv_canonicalize_patterns = [
    # XXX: Doesn't handle different sizes!
    tt.gof.opt.PatternSub(
        (tt.add,
         (MvNormalRV, 'mu', 'Sigma', 'size', 'rng'),
         (MvNormalRV, 'mu_2', 'Sigma_2', 'size', 'rng'),
         ),
        (MvNormalRV,
         (tt.add, 'mu', 'mu_2'),
         (tt.add, 'Sigma', 'Sigma_2'), 'size', 'rng'),
        allow_multiple_clients=True,
        name='mv_normal_add_fuse'
    ),
    tt.gof.opt.PatternSub(
        (tt.add,
         {'pattern': 'A', 'constraint': lambda e: not isinstance(e.type, RandomVariable)},
         (MvNormalRV, 'mu', 'Sigma', 'size', 'rng')),
        (MvNormalRV, (tt.add, 'A', 'mu'), 'Sigma', 'size', 'rng'),
        allow_multiple_clients=True,
        name='mv_normal_add_promote'
    ),
    tt.gof.opt.PatternSub(
        (tt.mul,
         'a',
         (NormalRV, 'mu', 'sd', 'size', 'rng')),
        (NormalRV,
         (tt.mul, 'a', 'mu'),
         (tt.mul, 'sd', (tt.sqrt, 'a')), 'size', 'rng'),
        allow_multiple_clients=True,
        name='scalar_normal_mul_lift'
    ),
    # XXX: Doesn't handle different sizes!
    tt.gof.opt.PatternSub(
        (tt.add,
         (NormalRV, 'mu', 'sd', 'size', 'rng'),
         (NormalRV, 'mu_2', 'sd_2', 'size', 'rng')),
        (NormalRV,
         (tt.add, 'mu', 'mu_2'),
         (tt.add, 'sd', 'sd_2'), 'size', 'rng'),
        allow_multiple_clients=True,
        name='scalar_normal_add_fuse'
    ),
    tt.gof.opt.PatternSub(
        (tt.add,
         {'pattern': 'a', 'constraint': lambda e: not isinstance(e.type, RandomVariable)},
         (NormalRV, 'mu', 'sd', 'size', 'rng'),
         ),
        (NormalRV, (tt.add, 'a', 'mu'), 'sd', 'size', 'rng'),
        allow_multiple_clients=True,
        name='scalar_normal_add_promote'
    ),
]

normal_rv_canonicalize = tt.gof.opt.EquilibriumOptimizer(normal_rv_canonicalize_patterns,
                                                         max_use_ratio=10)

# optdb.register('normal_rv_canonicalize',
#                normal_rv_canonicalize, 0.5,
#                'rv_canonicalize')
#+END_SRC

In Listing [[canon-rules-opt]], we reproduced the same rules for univariate and
multivariate normal distributions.  Instead, we could've canonicalized such that
univariate distributions are turned into degenerate multivariate distributions,
putting everything on the same dimensional "footing".

While perhaps not the best for numeric sampling, this is acceptable for canonicalization,
especially because canonicalization is distinct from concerns about computational efficiency.
A separate efficiency-based reformulation can be done once a model's graph has been assessed
in canonical form.

These new replacement rules can be applied as in Listing [[basic-dlm-canon-rv-example]].

#+NAME: basic-dlm-canon-rv-example
#+BEGIN_SRC python :results silent
from symbolic_pymc.theano.utils import optimize_graph

Y_t_canon_rv = optimize_graph(Y_t_opt, normal_rv_canonicalize)
#+END_SRC

Unfortunately, as Listing [[basic-dlm-canon-rv-example-dprint]] shows, the resulting
graph is in no way affected by these rules!  Again, we would like to see our
rules applied to the \( G_t \theta_{t-1} + \nu \) sub-graph.

#+NAME: basic-dlm-canon-rv-example-dprint
#+BEGIN_SRC python :wrap "SRC python :eval never"
tt_dprint(Y_t_canon_rv, depth=5)
#+END_SRC

#+RESULTS: basic-dlm-canon-rv-example-dprint
#+begin_SRC python :eval never
for{cpu,Y} [id A] ''
 |Subtensor{int64} [id B] ''
 | |Shape [id C] ''
 | | |Subtensor{int64:int64:int8} [id D] ''
 | |   |for{cpu,theta} [id E] ''
 | |   |ScalarFromTensor [id F] ''
 | |   |ScalarFromTensor [id G] ''
 | |   |Constant{1} [id H]
 | |Constant{0} [id I]
 |Subtensor{int64:int64:int64} [id J] ''
 | |for{cpu,theta} [id E] ''
 | |ScalarFromTensor [id K] ''
 | | |Elemwise{switch,no_inplace} [id L] ''
 | |   |Elemwise{le,no_inplace} [id M] ''
 | |   |TensorConstant{0} [id N]
 | |   |Elemwise{minimum,no_inplace} [id O] ''
 | |ScalarFromTensor [id P] ''
 | | |Elemwise{switch,no_inplace} [id Q] ''
 | |   |Elemwise{le,no_inplace} [id M] ''
 | |   |TensorConstant{0} [id N]
 | |   |Elemwise{minimum,no_inplace} [id R] ''
 | |Constant{1} [id S]
 |Subtensor{int64} [id B] ''
 |rng [id T]
 |F_tt [id U]
 |invgamma_rv.1 [id V] 'eps_scale'
   |TensorConstant{0.5} [id W]
   |TensorConstant{0.5} [id W]
   |TensorConstant{1.0} [id X]
   |TensorConstant{[]} [id Y]
   |rng [id T]

Inner graphs of the scan ops:

for{cpu,Y} [id A] ''
 >Elemwise{add,no_inplace} [id Z] ''
 > |InplaceDimShuffle{0} [id BA] ''
 > | |Dot22 [id BB] ''
 > |   |F_tt_copy [id BC] -> [id U]
 > |   |InplaceDimShuffle{0,x} [id BD] ''
 > |     |<TensorType(float64, vector)> [id BE] -> [id J]
 > |InplaceDimShuffle{x} [id BF] ''
 >   |normal_rv.1 [id BG] 'eps'
 >     |TensorConstant{0} [id BH]
 >     |eps_scale_copy [id BI] -> [id V]
 >     |TensorConstant{[]} [id BJ]
 >     |rng_copy [id BK] -> [id T]

for{cpu,theta} [id E] ''
 >Elemwise{add,no_inplace} [id BL] ''
 > |InplaceDimShuffle{0} [id BM] ''
 > | |Dot22 [id BN] ''
 > |   |G_tt_copy [id BO] -> [id BP]
 > |   |InplaceDimShuffle{0,x} [id BQ] ''
 > |     |theta[t-1] [id BR] -> [id BS]
 > |multivariate_normal_rv.1 [id BT] 'nu'
 >   |Elemwise{second,no_inplace} [id BU] ''
 >   | |theta[t-1] [id BR] -> [id BS]
 >   | |TensorConstant{(1,) of 0.0} [id BV]
 >   |<TensorType(float64, matrix)> [id BW] -> [id BX]
 >   |TensorConstant{[]} [id BY]
 >   |rng_copy [id BZ] -> [id T]

for{cpu,theta} [id E] ''
 >Elemwise{add,no_inplace} [id BL] ''


#+end_SRC

The problem is that--among other
things--our src_python[:eval never]{RandomVariable} terms are wrapped
by src_python[:eval never]{DimShuffle} operations that do not appear in our
replacement rules.  If we were to add these src_python[:eval never]{DimShuffle}s
to the rules, we would have twice as many rules to run!  The best approach to
fixing this problem is to consider src_python[:eval never]{DimShuffle}
and src_python[:eval never]{RandomVariable} interactions in the context of
canonicalization.

** DimShuffle

As we mentioned, src_python[:eval never]{DimShuffle}s get in the way of more
general math-level considerations.  For example,
the src_python[:eval never]{DimShuffle}
in src_python[:eval never]{tt.dot(A, DimShuffle(NormalRV(...)))} prevents us
from easily spotting the
underlying src_python[:eval never]{tt.dot(A, NormalRV(...))}.

First, let's look at the src_python[:eval never]{DimShuffle}s that are
currently getting in the way of our src_python[:eval never]{RandomVariable}
rewrite rules.

#+NAME: basic-dlm-dimshuffle-example-dprint
#+BEGIN_SRC python :wrap "SRC python :eval never"
tt_dprint(Y_t_opt.owner.op.fn, depth=5)
#+END_SRC

#+RESULTS: basic-dlm-dimshuffle-example-dprint
#+begin_SRC python :eval never
Elemwise{add,no_inplace} [id A] ''   5
 |InplaceDimShuffle{0} [id B] ''   4
 | |Dot22 [id C] ''   3
 |   |F_tt_copy [id D]
 |   |InplaceDimShuffle{0,x} [id E] ''   2
 |     |<TensorType(float64, vector)> [id F]
 |InplaceDimShuffle{x} [id G] ''   1
   |normal_rv.1 [id H] 'eps'   0
     |TensorConstant{0} [id I]
     |eps_scale_copy [id J]
     |TensorConstant{[]} [id K]
     |rng_copy [id L]


#+end_SRC

From Listing [[basic-dlm-dimshuffle-example-dprint]], we can see
two src_python[:eval never]{DimShuffle}s applied to both random variable terms
in the src_python[:eval never]{scan} graph for \(F_t^\top \theta_t + \epsilon_t\).
The first term is just labeled as src_python[:eval never]{<TensorType...>} and
isn't truly a src_python[:eval never]{RandomVariable}, although we know it
should be (i.e. after applying the rules to the src_python[:eval never]{scan}
producing those \(\theta_t\) values).
However, the \(\epsilon_t\) src_python[:eval never]{RandomVariable} term is
present as the second argument to src_python[:eval never]{Elemwise{add...}}, but
it's wrapped by a src_python[:eval never]{DimShuffle}.

#+NAME: basic-dlm-dimshuffle-example
#+BEGIN_SRC python :wrap "SRC python :eval never"
dmshf_tt = Y_t_rv.owner.op.fn.outputs[0].variable.owner.inputs[1].owner

tt_dprint(dmshf_tt)
#+END_SRC

#+RESULTS: basic-dlm-dimshuffle-example
#+begin_SRC python :eval never
InplaceDimShuffle{x} [id A] ''
 |normal_rv.1 [id B] 'eps'
   |TensorConstant{0} [id C]
   |eps_scale_copy [id D]
   |TensorConstant{[]} [id E]
   |rng_copy [id F]


#+end_SRC

Listing [[basic-dlm-dimshuffle-params]] shows the parameters of
the src_python[:eval never]{DimShuffle} operator, or essentially what it's
supposed to do to the src_python[:eval never]{RandomVariable}: add an extra
broadcastable dimension.

#+NAME: basic-dlm-dimshuffle-params
#+BEGIN_SRC python :wrap "SRC python :eval never"
print(f"DimShuffle.inplace={dmshf_tt.op.inplace},\t"
      f"DimShuffle.new_order={dmshf_tt.op.new_order},\t"
      f"DimShuffle.input_broadcastable={dmshf_tt.op.input_broadcastable}")
#+END_SRC

#+RESULTS: basic-dlm-dimshuffle-params
#+begin_SRC python :eval never
DimShuffle.inplace=True,	DimShuffle.new_order=('x',),	DimShuffle.input_broadcastable=()


#+end_SRC

Given the parameterization of our src_python[:eval never]{RandomVariable}
operators, this can be accomplished by simply making
the src_python[:eval never]{RandomVariable}'s inputs broadcastable
(i.e. "lifting" the src_python[:eval never]{DimShuffle} to the inputs) or by
applying the src_python[:eval never]{DimShuffle} to the size parameter
of src_python[:eval never]{RandomVariable}.

For example, Listing shows that both approaches produce the same result.

#+NAME: rv-dimshuffle-lift-example
#+BEGIN_SRC python :wrap "SRC python :eval never"
test_rng_state = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1234)))
test_rng = theano.shared(test_rng_state, name='rng')

norm_lifted_inputs_sample = NormalRV(tt.as_tensor_variable(0.0).dimshuffle('x'),
                                 tt.as_tensor_variable(1.0).dimshuffle('x'),
                                 rng=test_rng).eval()

test_rng.set_value(np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1234))))

norm_altered_size_sample = NormalRV(0, 1, size=[1], rng=test_rng).eval()

print(f"lift inputs: {norm_lifted_inputs_sample}, \talter size: {norm_altered_size_sample}")

#+END_SRC

#+RESULTS: rv-dimshuffle-lift-example
#+begin_SRC python :eval never
lift inputs: [-0.9581156], 	alter size: [-0.9581156]


#+end_SRC

Given the existing design of Theano, we could consider univariate normals
as src_python[:eval never]{Elemwise} operators and then add special
considerations for sampling "batches".  This is reasonable, because normal
random variable mapping is linear in the same way that
most src_python[:eval never]{Elemwise} tensor operations are.  This approach
would confer many of the expected properties and optimizations/canonicalization
to univariate normal random variables; however, it would not address
multivariate normals.

Here we will address only some cases of src_python[:eval never]{DimShuffle}
and src_python[:eval never]{RandomVariable} interactions.
We start by naively lifting src_python[:eval never]{DimShuffle} operations
through all normal src_python[:eval never]{RandomVariable}s.

#+NAME: sympymc-lift-dimshuffle-normalrv
#+BEGIN_SRC python :exports none :eval never
normal_rv_dimshuffle_patterns = [
    tt.gof.opt.PatternSub(
        ((tt.DimShuffle, 'input_broadcastable', 'new_order', 'inplace'),
         (NormalRV, 'mu', 'sd', 'size', 'rng', 'name')),
        (NormalRV,
         ((tt.DimShuffle, 'input_broadcastable', 'new_order', 'inplace'), 'mu'),
         ((tt.DimShuffle, 'input_broadcastable', 'new_order', 'inplace'), 'sd'),
          'size', 'rng', 'name'),
        allow_multiple_clients=True,
        name='normal_dimshuffle_lift'
    )]

normal_rv_dimshuffle_canon = tt.gof.opt.EquilibriumOptimizer(normal_rv_dimshuffle_patterns,
                                                             max_use_ratio=10)
tt_dprint(Y_t_opt)

Y_t_canon_rv = optimize_graph(Y_t_opt, normal_rv_dimshuffle_canon)

tt_dprint(Y_t_canon_rv)

Y_t_canon_rv = optimize_graph(Y_t_canon_rv, normal_rv_canonicalize)

tt_dprint(Y_t_canon_rv)
#+END_SRC

#+NAME: testing-3
#+BEGIN_SRC python :exports none :eval never
# @register_canonicalize
@local_optimizer([DimShuffle])
def local_lift_DimShuffle_through_NormalRV(node):
    """
    These optimizations "lift" (i.e. propagate towards the inputs) `DimShuffle`
    through normal distributions.  It puts the graph into a more standard
    shape, and later allows us to merge consecutive `DimShuffle`s.

    In general, this prevents `DimShuffle`s from getting in the way of more
    general math-level considerations.  For example, the `DimShuffle` in
    `tt.dot(A, DimShuffle(NormalRV(...)))` prevents us from easily spotting the
    underlying `tt.dot(A, NormalRV(...))`.

    """
    if not isinstance(node.op, tt.basic.DimShuffle):
        return False

    owner = node.inputs[0].owner

    if not (owner and isinstance(owner.op, RandomVariable)):
        return False

    import pdb; pdb.set_trace()

    rv_smpl = node.inputs[0]

    new_order = node.op.new_order
    # owner.inputs

    # shape = tuple(shape_reps) + tuple(shape_ind) + tuple(shape_supp)

    new_dist_params = None
    new_size = None

    ret = [owner.op(*new_dist_params, size=new_size, rng=rng, name=rv_smpl.name)]

    copy_stack_trace(rv_smpl, ret)

    return ret


# Let's inspect the kind of `[Inplace]DimShuffle`s we're dealing with
tt_dprint(Y_t_rv.owner.op.fn, depth=5)

dmshf_op = Y_t_rv.owner.op.fn.outputs[0].variable.owner.inputs[1].owner.op
dmshf_op.inplace
dmshf_op.new_order
dmshf_op.input_broadcastable

test_mvn_tt = MvNormalRV(np.r_[0, 10, 100], np.diag([0.1, 0.01, 0.001]), size=3)

dmshf_mvn_tt = test_mvn_tt.dimshuffle([0, 1, 'x'])

# dmshf_mvn_tt.owner.op.new_order
# dmshf_mvn_tt.owner.op.input_broadcastable

test_mvn.eval()
dmshf_mvn_tt.eval()

# Now, how does the `DimShuffle`d result translate to a `MvNomalRV`?
# We might be able to generalize if we can simplify the action of the `size` parameter.

test_mvn_base_tt = MvNormalRV(np.r_[0, 10, 100], np.diag([0.1, 0.01, 0.001]))

# test_mvn_rpt_tt = tt.tile(test_mvn_base_tt, [3, 1])

test_mvn_rpt_tt.eval()

test_mvn_2_tt = MvNormalRV(np.r_[0, 10, 100], np.diag([0.1, 0.01, 0.001]), size=3)

normal_rv_canonicalize = tt.gof.opt.EquilibriumOptimizer(normal_rv_canonicalize_patterns,
                                                         max_use_ratio=10)
opt_res = optimize_graph(test_mvn_2_tt, local_lift_DimShuffle_through_NormalRV)
tt_dprint(opt_res, depth=5)
#+END_SRC

* Discussion

So far, we've only shown how to perform FFBS for DLMs in Theano...

#+BIBLIOGRAPHYSTYLE: plainnat
#+BIBLIOGRAPHY: ../tex/dlm-optimizations.bib
