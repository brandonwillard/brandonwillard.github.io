#+TITLE: Random Variables in Theano
#+AUTHOR: Brandon T. Willard
#+DATE: 2018-12-28
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+INCLUDE: org-setup.org
#+BEGIN_SRC elisp :eval t :exports none :results none
(org-babel-lob-ingest "org-babel-extensions.org")
#+END_SRC

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session symbolic-math-pymc3-2

#+BEGIN_abstract
Continuing from [[citet:WillardSymbolicMathPyMC32018]], we'll attempt to improve
upon src_python[]{RandomFunction} and make a case for a similar src_python[]{Op} in
PyMC3.
#+END_abstract

* Introduction


We'll call the new src_python{Op} developed here src_python{RandomVariable}, since random
variables are the abstraction we're primarily targeting.
src_python{RandomVariable} will provide the functionality
of src_python{Distribution}, src_python{FreeRV} and src_python{ObservedRV}, and,
by working at the src_python{Op} level, it will be much more capable of leveraging
existing Theano functionality.

Specifically, by using the src_python[]{Op} interface, we're able to do the
following:

1. Remove the need for an explicitly specified shape parameter.
   :EXAMPLE:
   For example, definitions like
   #+BEGIN_SRC python
   with pm.Model():
       X_rv = pm.Normal('X_rv', mu_X, sd=sd_X, shape=(1,))
   #+END_SRC
   reduce to
   #+BEGIN_SRC python
   with pm.Model():
       X_rv = pm.Normal('X_rv', mu_X, sd=sd_X)
   #+END_SRC
   :END:
1. Random variable nodes created by an src_python[]{Op} automatically implement
   src_python[]{Distribution.default}/src_python[]{Distribution.get_test_val}
   functionality and remove the reliance on initial values during random
   variable instantiation.  src_python[]{Op} automatically
   uses src_python[]{Op.perform}, which will draw a sample as a test value *and*
   propagate it throughout the graph to down-stream tensor variables.
2. Log-densities can be generated as secondary outputs of
   src_python[]{Op.make_node}, which removes the need
   for src_python[]{Distribution.logp*} methods.
3. src_python[]{pymc.distribution.draw_values} and related methods are no longer necessary;
   their functionality is already covered within Theano's existing graph
   machinery--in the same way
   as src_python{pymc.distribution.Distribution.default/get_test_val}.

The main points of entry in our src_python[]{Op}, are src_python[]{Op.make_node}
and src_python[]{Op.perform}.  src_python[]{Op.make_node} is used during symbolic
graph creation and provides immediate access to the src_python[]{Op}'s
symbolic inputs--serving a purpose similar to src_python[]{Distribution.__init__}.
src_python[]{Op.make_node} is where shape inference tasks (e.g. [[https://github.com/pymc-devs/pymc3/pull/1125][PyMC3 PR 1125]]) are more
suitably addressed; however, src_python[]{Op} provides additional means of shape inference
and management (e.g. src_python[]{Op.infer_shape}) occurring at different phases of
graph compilation that aren't readily accessible outside of the src_python[]{Op} framework.

* A *new* Random Variable src_python[]{Op}
#+ATTR_LATEX: :float t :placement h!
#+NAME: import_theano_pymc3
#+BEGIN_SRC python :exports code :results silent :noweb-ref new-op-code
import sys
import os

from pprint import pprint

import numpy as np

os.environ['MKL_THREADING_LAYER'] = 'GNU'

import theano
import theano.tensor as tt

theano.config.mode = 'FAST_COMPILE'
theano.config.exception_verbosity = 'high'
# NOTE: pymc3 requires test values
theano.config.compute_test_value = 'warn'

import pymc3 as pm
#+END_SRC

Most of the work involved in generalizing src_python{RandomFunction} has to do with
symbolic shape handling and inference.  We need to bridge the gaps between symbolic
array/tensor broadcasting parameters and the way Numpy random variable functions
allow distribution parameters to be specified.

:EXAMPLE:
Scalar normal random variates have a support and parameters with dimension zero.
In Listing [[np_normal_scalar_exa]] we create a scalar normal random variate in Numpy and
inspect its shape.  The length of the shape corresponds to the dimension of
the distribution's support (i.e. zero).
#+NAME: np_normal_scalar_exa
#+BEGIN_SRC python :exports both :results value :wrap "SRC python :eval never"
np.shape(np.random.normal(loc=0, scale=1, size=None))
#+END_SRC

#+RESULTS: np_normal_scalar_exa
#+begin_SRC python :eval never
()
#+end_SRC

Numpy also allows one to specify *independent* normal variates using one function
call with each variate's parameters spanning dimensions higher than the variate's.
In [[np_normal_ind_exa]] we specify three independent scalar normal variates, each with
a different mean and scale parameter.  This time, the result's shape reflects
*the number of independent random variates*, and not the dimension of the
underlying distribution's support.
#+NAME: np_normal_ind_exa
#+BEGIN_SRC python :exports both :results value :wrap "SRC python :eval never"
np.shape(np.random.normal(loc=[0, 1, 2], scale=[1, 2, 3], size=None))
#+END_SRC

#+RESULTS: np_normal_ind_exa
#+begin_SRC python :eval never
(3,)
#+end_SRC

Distribution parameters can also be broadcasted, as in [[np_normal_ind_bcast_exa]].
Now, each independent variate has the same scale value.
#+NAME: np_normal_ind_bcast_exa
#+BEGIN_SRC python :exports both :results none
np.shape(np.random.normal(loc=[0, 1, 2], scale=1, size=None))
#+END_SRC

The src_python{size} parameter effectively replicates variates, in-line with
the--potentially broadcasted--distribution parameters.


When bridging these Numpy functions and Theano, we have to adapt the
underlying parameter/shape logic of functions like src_python{np.random.normal}
to a scenario involving symbolic parameters and their symbolic shapes.

For instance, in Theano a *symbolic* scalar's shape is represented in nearly the
same way.
#+NAME: tt_scalar_shape_exa
#+BEGIN_SRC python :exports both :results value :wrap "SRC python :eval never"
test_scalar = tt.scalar()
test_scalar.shape.eval({test_scalar: 1})
#+END_SRC

#+RESULTS: tt_scalar_shape_exa
#+begin_SRC python :eval never
[]
#+end_SRC

This means that our proposed Theano adaptation of src_python{np.random.normal},
let's call it src_python{tt_normal}, should return the same result as Numpy
in the case of scalars.

What about src_python{tt_normal(loc=tt.vector(), scale=tt.vector(), size=None)}?
Since the inputs are purely symbolic, the resulting symbolic object's shape
should be, too, but we should also know that the symbolic shape should have
dimension equal to one.  Just as in [[np_normal_ind_exa]], each corresponding
element in the vector arguments of src_python{tt_normal} is an independent
variate; in the symbolic case, we might not know exactly how many of them there
are, yet, but we know that there's a vector's worth of them.

How exactly do we get that information from Theano, though?
The type produced by src_python{tt.vector} has an src_python{ndim} parameter that
provides this.  Furthermore, there is some (intermittent) functionality that
allows one to iterate over shapes.  Listing [[tt_matrix_shape_iter_exa]]
demonstrates this.
#+NAME: tt_matrix_shape_iter_exa
#+BEGIN_SRC python :exports both :results value :wrap "SRC python :eval never"
test_matrix = tt.matrix()
shape_parts = tuple(test_matrix.shape)
shape_parts
#+END_SRC

#+RESULTS: tt_matrix_shape_iter_exa
#+begin_SRC python :eval never
(Subtensor{int64}.0, Subtensor{int64}.0)
#+end_SRC

When the matrix in [[tt_matrix_shape_iter_exa]] is "materialized" (i.e. given a value),
its corresponding shape object--and its components--will take their respective
values.

#+NAME: tt_matrix_shape_iter_exa_2
#+BEGIN_SRC python :exports both :results value :wrap "SRC python :eval never"
tuple(p.eval({test_matrix: np.diag([1, 2])}) for p in shape_parts)
#+END_SRC

#+RESULTS: tt_matrix_shape_iter_exa_2
#+begin_SRC python :eval never
(array(2), array(2))
#+end_SRC

If we knew that the support of this distribution was a scalar/vector/matrix,
then these src_python{ndim}-related results--obtained from the symbolic
parameters--would tell us that we have multiple, independent variates and we
could reliably extract the symbolic variables corresponding to those actual
dimension sizes.
:END:

To determine the shape parts (i.e. support, number of independent and replicated
variates) of the symbolic random variables, we mimic the corresponding Numpy
logic and use the Theano src_python{ndim} shape information described above.
The following function generalizes that work for many simple distributions.

#+ATTR_LATEX: :float nil
#+CAPTION: Helper function used to determine a random variable's shape based on the shape of its parameters.
#+NAME: supp_shape_fn
#+BEGIN_SRC python :exports code :results silent :noweb-ref new-op-code
from collections.abc import Iterable, ByteString
from warnings import warn
from copy import copy

from theano.tensor.raw_random import (RandomFunction, RandomStateType,
                                      _infer_ndim_bcast)


def param_supp_shape_fn(ndim_supp, ndims_params, dist_params,
                        rep_param_idx=0, param_shapes=None):
    """A function for deriving a random variable's support shape/dimensions
    from one of its parameters.

    XXX: It's not always possible to determine a random variable's support
    shape from its parameters, so this function has fundamentally limited
    applicability.

    XXX: This function is not expected to handle `ndim_supp = 0` (i.e.
    scalars), since that is already definitively handled in the `Op` that
    calls this.

    TODO: Consider using `theano.compile.ops.shape_i` alongside `ShapeFeature`.

    Parameters
    ==========
    ndim_supp: int
        Total number of dimensions in the support (assumedly > 0).
    ndims_params: list of int
        Number of dimensions for each distribution parameter.
    dist_params: list of `theano.gof.graph.Variable`
        The distribution parameters.
    param_shapes: list of `theano.compile.ops.Shape` (optional)
        Symbolic shapes for each distribution parameter.
        Providing this value prevents us from reproducing the requisite
        `theano.compile.ops.Shape` object (e.g. when it's already available to
        the caller).
    rep_param_idx: int (optional)
        The index of the distribution parameter to use as a reference
        In other words, a parameter in `dist_param` with a shape corresponding
        to the support's shape.
        The default is the first parameter (i.e. the value 0).

    Results
    =======
    out: a tuple representing the support shape for a distribution with the
    given `dist_params`.
    """
    # XXX: Gotta be careful slicing Theano variables, the `Subtensor` Op isn't
    # handled by `tensor.get_scalar_constant_value`!
    # E.g.
    #     test_val = tt.as_tensor_variable([[1], [4]])
    #     tt.get_scalar_constant_value(test_val.shape[-1]) # works
    #     tt.get_scalar_constant_value(test_val.shape[0]) # doesn't
    #     tt.get_scalar_constant_value(test_val.shape[:-1]) # doesn't
    if param_shapes is not None:
        # return param_shapes[0][-self.ndim_supp:]
        return (param_shapes[rep_param_idx][-ndim_supp],)
    else:
        # return dist_params[rep_param_idx].shape[-ndim_supp]
        ref_shape = tt.shape(dist_params[rep_param_idx])
        return (ref_shape[-ndim_supp],)
#+END_SRC

Finally, we put everything together in a new random variable src_python{Op}
called src_python{RandomVariable}.

#+ATTR_LATEX: :float nil
#+CAPTION: A new random variable src_python[]{Op}.
#+NAME: new_rv_op
#+BEGIN_SRC python :exports code :results none :noweb-ref new-op-code
class RandomVariable(tt.gof.Op):
    """This is essentially `RandomFunction`, except that it removes the `outtype`
    dependency and handles shape dimension information more directly.
    """
    __props__ = ('name', 'dtype', 'ndim_supp', 'inplace', 'ndims_params')

    def __init__(self, name, dtype, ndim_supp, ndims_params, rng_fn,
                 *args,
                 supp_shape_fn=param_supp_shape_fn,
                 inplace=False,
                 **kwargs):
        """Create a random variable `Op`.

        Parameters
        ==========
        name: str
            The `Op`'s display name.
        dtype: Theano dtype
            The underlying dtype.
        ndim_supp: int
            Dimension of the support.  This value is used to infer the exact
            shape of the support and independent terms from ``dist_params``.
        ndims_params: tuple (int)
            Number of dimensions of each parameter in ``dist_params``.
        rng_fn: function or str
            The non-symbolic random variate sampling function.
            Can be the string name of a method provided by
            `numpy.random.RandomState`.
        supp_shape_fn: callable (optional)
            Function used to determine the exact shape of the distribution's
            support.

            It must take arguments ndim_supp, ndims_params, dist_params
            (i.e. an collection of the distribution parameters) and an
            optional param_shapes (i.e. tuples containing the size of each
            dimension for each distribution parameter).

            Defaults to `param_supp_shape_fn`.
        inplace: boolean
            Determine whether or not the underlying rng state is updated in-place or
            not (i.e. copied).
        """
        super().__init__(*args, **kwargs)

        self.name = name
        self.ndim_supp = ndim_supp
        self.dtype = dtype
        self.supp_shape_fn = supp_shape_fn
        self.inplace = inplace

        if not isinstance(ndims_params, Iterable):
            raise ValueError('Parameter ndims_params must be iterable.')

        self.ndims_params = tuple(ndims_params)

        self.default_output = 1

        if isinstance(rng_fn, (str, ByteString)):
            self.rng_fn = getattr(np.random.RandomState, rng_fn)
        else:
            self.rng_fn = rng_fn

    def __str__(self):
        return '{}_rv'.format(self.name)

    def _infer_shape(self, size, dist_params, param_shapes=None):
        """Compute shapes and broadcasts properties.

        Inspired by `tt.add.get_output_info`.
        """

        size_len = tt.get_vector_length(size)

        dummy_params = tuple(p if n == 0 else tt.ones(tuple(p.shape)[:-n])
                             for p, n in zip(dist_params, self.ndims_params))

        _, out_bcasts, bcastd_inputs = tt.add.get_output_info(
            tt.DimShuffle, *dummy_params)

        # _, out_bcasts, bcastd_inputs = tt.add.get_output_info(tt.DimShuffle, *dist_params)

        bcast_ind, = out_bcasts
        ndim_ind = len(bcast_ind)
        shape_ind = bcastd_inputs[0].shape

        if self.ndim_supp == 0:
            shape_supp = tuple()

            # In the scalar case, `size` corresponds to the entire result's
            # shape. This implies the following:
            #     shape_ind[-ndim_ind] == size[:ndim_ind]
            # TODO: How do we add this constraint/check symbolically?

            ndim_reps = max(size_len - ndim_ind, 0)
            shape_reps = tuple(size)[ndim_ind:]
        else:
            shape_supp = self.supp_shape_fn(self.ndim_supp,
                                            self.ndims_params,
                                            dist_params,
                                            param_shapes=param_shapes)

            ndim_reps = size_len
            shape_reps = size

        ndim_shape = self.ndim_supp + ndim_ind + ndim_reps

        if ndim_shape == 0:
            shape = tt.constant([], dtype='int64')
        else:
            shape = tuple(shape_reps) + tuple(shape_ind) + tuple(shape_supp)

        # if shape is None:
        #     raise tt.ShapeError()

        return shape

    def compute_bcast(self, dist_params, size):
        """Compute the broadcast array for this distribution's `TensorType`.

        Parameters
        ==========
        dist_params: list
            Distribution parameters.
        size: int or Iterable (optional)
            Numpy-like size of the output (i.e. replications).
        """
        shape = self._infer_shape(size, dist_params)

        # Let's try to do a better job than `_infer_ndim_bcast` when
        # dimension sizes are symbolic.
        bcast = []
        for s in shape:
            try:
                if isinstance(s.owner.op, tt.Subtensor) and \
                   s.owner.inputs[0].owner is not None:
                    # Handle a special case in which
                    # `tensor.get_scalar_constant_value` doesn't really work.
                    s_x, s_idx = s.owner.inputs
                    s_idx = tt.get_scalar_constant_value(s_idx)
                    if isinstance(s_x.owner.op, tt.Shape):
                        x_obj, = s_x.owner.inputs
                        s_val = x_obj.type.broadcastable[s_idx]
                    else:
                        # TODO: Could go for an existing broadcastable here, too, no?
                        s_val = False
                else:
                    s_val = tt.get_scalar_constant_value(s)
            except tt.NotScalarConstantError:
                s_val = False

            bcast += [s_val == 1]
        return bcast

    def infer_shape(self, node, input_shapes):
        size = node.inputs[1]
        dist_params = tuple(node.inputs[2:])
        shape = self._infer_shape(size, dist_params,
                                  param_shapes=input_shapes[2:])

        return [None, [s for s in shape]]

    def make_node(self, *dist_params, size=None, rng=None, name=None):
        """This will be the "constructor" called by users.

        Parameters
        ==========
        dist_params: list
            Distribution parameters.
        size: int or Iterable (optional)
            Numpy-like size of the output (i.e. replications).
        rng: RandomState (optional)
            Existing Theano `RandomState` object to be used.  Creates a
            new one, if `None`.
        name: str (optional)
            Label for the resulting node.

        Results
        =======
        An `Apply` node with rng state and sample tensor outputs.
        """
        if size is None:
            size = tt.constant([], dtype='int64')
        elif isinstance(size, int):
            size = tt.as_tensor_variable([size], ndim=1)
        elif not isinstance(size, Iterable):
            raise ValueError('Parameter size must be None, int, or an iterable with ints.')
        else:
            size = tt.as_tensor_variable(size, ndim=1)

        assert size.dtype in tt.int_dtypes

        dist_params = tuple(tt.as_tensor_variable(p)
                            for p in dist_params)

        if rng is None:
            rng = theano.shared(np.random.RandomState())
        elif not isinstance(rng.type, RandomStateType):
            warn('The type of rng should be an instance of RandomStateType')

        bcast = self.compute_bcast(dist_params, size)

        # dtype = tt.scal.upcast(self.dtype, *[p.dtype for p in dist_params])

        outtype = tt.TensorType(dtype=self.dtype, broadcastable=bcast)
        out_var = outtype(name=name)
        inputs = (rng, size) + dist_params
        outputs = (rng.type(), out_var)

        return theano.gof.Apply(self, inputs, outputs)

    def perform(self, node, inputs, outputs):
        """Uses `self.rng_fn` to draw random numbers."""
        rng_out, smpl_out = outputs

        # Draw from `rng` if `self.inplace` is `True`, and from a
        # copy of `rng` otherwise.
        rng, size, args = inputs[0], inputs[1], inputs[2:]

        assert isinstance(rng, np.random.RandomState), (type(rng), rng)

        rng_out[0] = rng

        # The symbolic output variable corresponding to value produced here.
        out_var = node.outputs[1]

        # If `size == []`, that means no size is enforced, and NumPy is
        # trusted to draw the appropriate number of samples, NumPy uses
        # `size=None` to represent that.  Otherwise, NumPy expects a tuple.
        if np.size(size) == 0:
            size = None
        else:
            size = tuple(size)

        if not self.inplace:
            rng = copy(rng)

        smpl_val = self.rng_fn(rng, *(args + [size]))

        if (not isinstance(smpl_val, np.ndarray) or
            str(smpl_val.dtype) != out_var.type.dtype):
            smpl_val = theano._asarray(smpl_val, dtype=out_var.type.dtype)

        # When `size` is `None`, NumPy has a tendency to unexpectedly
        # return a scalar instead of a higher-dimension array containing
        # only one element. This value should be reshaped
        # TODO: Really?  Why shouldn't the output correctly correspond to
        # the returned NumPy value?  Sounds more like a mis-specification of
        # the symbolic output variable.
        if size is None and smpl_val.ndim == 0 and out_var.ndim > 0:
            smpl_val = smpl_val.reshape([1] * out_var.ndim)

        smpl_out[0] = smpl_val

    def grad(self, inputs, outputs):
        return [theano.gradient.grad_undefined(self, k, inp,
                                               'No gradient defined through raw random numbers op')
                for k, inp in enumerate(inputs)]

    def R_op(self, inputs, eval_points):
        return [None for i in eval_points]
#+END_SRC

* Using src_python{RandomVariable}
In Listing [[create_random_variables]] we create
some src_python[]{RandomVariable} src_python{Op}s.

#+NAME: create_random_variables
#+BEGIN_SRC python :results none :noweb-ref new-op-code
import scipy
from functools import partial


# Continuous Numpy-generated variates
UniformRV = RandomVariable('uniform', theano.config.floatX, 0, [0, 0], 'uniform', inplace=True)
NormalRV = RandomVariable('normal', theano.config.floatX, 0, [0, 0], 'normal', inplace=True)
GammaRV = RandomVariable('gamma', theano.config.floatX, 0, [0, 0], 'gamma', inplace=True)
ExponentialRV = RandomVariable('exponential', theano.config.floatX, 0, [0], 'exponential', inplace=True)

# One with multivariate support
MvNormalRV = RandomVariable('multivariate_normal', theano.config.floatX, 1, [1, 2], 'multivariate_normal', inplace=True)
DirichletRV = RandomVariable('dirichlet', theano.config.floatX, 1, [1], 'dirichlet', inplace=True)

# A discrete Numpy-generated variate
PoissonRV = RandomVariable('poisson', 'int64', 0, [0], 'poisson', inplace=True)

# A SciPy-generated variate
CauchyRV = RandomVariable('cauchy', theano.config.floatX, 0, [0, 0],
                          lambda rng, *args: scipy.stats.cauchy.rvs(*args, random_state=rng),
                          inplace=True)

# Support shape is determined by the first dimension in the *second* parameter (i.e.
# the probabilities vector)
MultinomialRV = RandomVariable('multinomial', 'int64', 1, [0, 1], 'multinomial',
                               supp_shape_fn=partial(param_supp_shape_fn, rep_param_idx=1),
                               inplace=True)
#+END_SRC

:EXAMPLE:
In Listing [[random_variable_example]] we draw samples from instances
of src_python{RandomVariable}s.
#+NAME: random_variable_example
#+BEGIN_SRC python :wrap "SRC python :eval never"
print("UniformRV(0., 30., size=[10]):\n{}\n".format(
    UniformRV(0., 30., size=[10]).eval()
))

print("NormalRV([0., 100.], 30, size=[4, 2]):\n{}\n".format(
    NormalRV([0., 100.], 30, size=[4, 2]).eval()))

print("GammaRV([2., 1.], 2., size=[4, 2]):\n{}\n".format(
    GammaRV([2., 1.], 2., size=[4, 2]).eval()))

print("ExponentialRV([2., 50.], size=[4, 2]):\n{}\n".format(
    ExponentialRV([2., 50.], size=[4, 2]).eval()))

print("MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[3, 2, 3]):\n{}\n".format(
    MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[2, 3]).eval()))

print("DirichletRV([0.1, 10, 0.5], size=[3, 2, 3]):\n{}\n".format(
    DirichletRV([0.1, 10, 0.5], size=[2, 3]).eval()))

print("PoissonRV([2., 1.], size=[4, 2]):\n{}\n".format(
    PoissonRV([2., 15.], size=[4, 2]).eval()))

print("CauchyRV([1., 100.], 30, size=[4, 2]):\n{}\n".format(
    CauchyRV([1., 100.], 30, size=[4, 2]).eval()))

print("MultinomialRV(20, [1/6.]*6, size=[6, 2]):\n{}".format(
    MultinomialRV(20, [1 / 6.] * 6, size=[3, 2]).eval()))
#+END_SRC

#+RESULTS: random_variable_example
#+begin_SRC python :eval never
UniformRV(0., 30., size=[10]):
[16.89369343 13.21988788 10.26918728  1.93879999 17.27410447  5.3058197
 22.15578126 15.66637894 14.23980245 25.18733049]

NormalRV([0., 100.], 30, size=[4, 2]):
[[ -9.21735205 155.64498511]
 [ 28.4339851  105.18399789]
 [-44.42055013  97.90586798]
 [-26.55693327 115.39711945]]

GammaRV([2., 1.], 2., size=[4, 2]):
[[3.49908312 1.91088775]
 [1.13233478 0.48482779]
 [2.25680547 0.92411033]
 [1.78299465 1.19593451]]

ExponentialRV([2., 50.], size=[4, 2]):
[[  3.22763867   3.78135685]
 [  3.56648308   3.60405088]
 [  0.65872498  88.7571588 ]
 [  4.67451316 120.24080966]]

MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[3, 2, 3]):
[[[-4.75463905e-01  1.00402403e+02  2.00079552e+03]
  [-2.59426266e-01  1.00970588e+02  1.99989382e+03]
  [ 3.39161352e-01  1.00233588e+02  1.99935490e+03]]

 [[-9.90453947e-01  9.99983419e+01  1.99971327e+03]
  [-6.82615151e-01  1.00704686e+02  1.99968969e+03]
  [-2.21678226e+00  1.01142950e+02  2.00043834e+03]]]

DirichletRV([0.1, 10, 0.5], size=[3, 2, 3]):
[[[4.30717537e-03 9.94067987e-01 1.62483727e-03]
  [1.01466452e-06 9.78964356e-01 2.10346294e-02]
  [3.30762555e-05 9.72405682e-01 2.75612413e-02]]

 [[8.30072104e-06 9.99191757e-01 7.99941999e-04]
  [2.03443491e-19 8.74258555e-01 1.25741445e-01]
  [1.34671133e-07 9.99550242e-01 4.49623671e-04]]]

PoissonRV([2., 1.], size=[4, 2]):
[[ 2 22]
 [ 2 17]
 [ 0 17]
 [ 0 10]]

CauchyRV([1., 100.], 30, size=[4, 2]):
[[-123.6213883   199.96370344]
 [ -24.53254923   90.83035835]
 [  22.09067095   75.31186146]
 [-323.54712455 -595.69091703]]

MultinomialRV(20, [1/6.]*6, size=[6, 2]):
[[[3 5 1 5 2 4]
  [1 6 4 5 0 4]]

 [[4 2 4 3 5 2]
  [6 4 2 2 4 2]]

 [[3 6 2 2 2 5]
  [2 4 2 7 3 2]]]


#+end_SRC
:END:

As noted, there are a few long-standing difficulties surrounding the use and
determination of shape information in PyMC3.  src_python[]{RandomVariable}
doesn't suffer the same limitations.

:EXAMPLE:
In Listing [[mvnormal-pymc3-error]], we see that a multivariate normal random variable
cannot be created in PyMC3 without explicit shape information.

#+NAME: mvnormal-pymc3-error
#+BEGIN_SRC python :wrap "SRC python :eval never"
import traceback

test_mean = tt.vector('test_mean')
test_cov = tt.matrix('test_cov', dtype='int64')

test_mean.tag.test_value = np.asarray([1])
test_cov.tag.test_value = np.asarray([[1]])

try:
  with pm.Model():
    test_rv = pm.MvNormal('test_rv', test_mean, test_cov)
except Exception as e:
  print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS: mvnormal-pymc3-error
#+begin_SRC python :eval never
ValueError: Invalid dimension for value: 0


#+end_SRC

As Listing [[mvnormal-theano-no-error]] demonstrates, the same construction
is possible when one specifies an explicit size/shape.

#+NAME: mvnormal-theano-no-error
#+BEGIN_SRC python :wrap "SRC python :eval never"
try:
  with pm.Model():
    test_rv = pm.MvNormal('test_rv', test_mean, test_cov, shape=1)
    print("test_rv.distribution.shape = {}".format(test_rv.distribution.shape))
    print("test_rv.tag.test_value = {}".format(test_rv.tag.test_value))
except Exception as e:
  print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS: mvnormal-theano-no-error
#+begin_SRC python :eval never
test_rv.distribution.shape = [1]
test_rv.tag.test_value = [1.]


#+end_SRC
:END:

Using src_python[]{RandomVariable}, we do not have to specify a shape, nor
implement any sampling code outside of src_python[]{RandomVariable.perform}
to draw random variables and generate valid test values.

:EXAMPLE:
Listings [[dependent-multivariate-eval-exa]] and [[dependent-multivariate-testvals-exa]]
demonstrate how easy it is to create dependencies between random variates
using src_python{RandomVariable}, and how sampling and test values are
automatic.  It uses a multivariate normal as the mean of another multivariate
normal.

#+NAME: dependent-multivariate-eval-exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
theano.config.compute_test_value = 'ignore'

mu_tt = tt.vector('mu')
C_tt = tt.matrix('C')
D_tt = tt.matrix('D')

X_rv = MvNormalRV(mu_tt, C_tt)
Y_rv = MvNormalRV(X_rv, D_tt)

# Sample some values under specific parameter values
print("{} ~ X\n{} ~ Y".format(
    X_rv.eval({mu_tt: [1, 2], C_tt: np.diag([1, 2])}),
    Y_rv.eval({mu_tt: [1, 2], C_tt: np.diag([1, 2]), D_tt: np.diag([10, 20])})))
#+END_SRC

#+RESULTS: dependent-multivariate-eval-exa
#+begin_SRC python :eval never
[-1.25047147  4.87459955] ~ X
[ 2.15486205 -3.3066946 ] ~ Y


#+end_SRC

#+NAME: dependent-multivariate-testvals-exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
theano.config.compute_test_value = 'warn'

mu_tt.tag.test_value = np.array([0, 30, 40])
C_tt.tag.test_value = np.diag([100, 10, 1])
D_tt.tag.test_value = np.diag([100, 10, 1])

X_rv = MvNormalRV(mu_tt, C_tt)
Y_rv = MvNormalRV(X_rv, D_tt)

# Observe the automatically generated test values
print("X test value: {}\nY test value: {}".format(
    X_rv.tag.test_value,
    Y_rv.tag.test_value))

#+END_SRC

#+RESULTS: dependent-multivariate-testvals-exa
#+begin_SRC python :eval never
X test value: [ 1.78826967 28.73266332 38.57297111]
Y test value: [33.93703352 27.48925582 38.21563854]


#+end_SRC

:END:

:EXAMPLE:
In Listing [[dependent-poisson-multinomial-exa]], we specify the following
hierarchical model:
\begin{equation*}
  \begin{aligned}
    M &\sim \text{Poisson}\left(10\right)
    \\
    \alpha_i &\sim \text{Uniform}\left(0, 1\right),
    \quad i \in \left\{0, \dots, M\right\}
    \\
    \pi &\sim \text{Dirichlet}\left(\alpha\right)
    \\
    Y &\sim \text{Multinomial}\left(M, \pi\right)
  \end{aligned}
  \;.
\end{equation*}
This toy model is particularly interesting in how it specifies symbolic
dependencies between continuous and discrete distributions and uses random
variables to determine the shapes of other random variables.

#+NAME: dependent-poisson-multinomial-exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
theano.config.compute_test_value = 'ignore'
pois_rate = tt.dscalar('rate')
test_pois_rv = PoissonRV(pois_rate)
test_alpha = UniformRV(0, 1, size=test_pois_rv)
test_dirichlet_rv = DirichletRV(test_uniform_rv)
test_multinom_rv = MultinomialRV(test_pois_rv, test_dirichlet_rv)

test_multinom_draw = theano.function(inputs=[], outputs=test_multinom_rv,
                                     givens={pois_rate: 10.})

print("test_multinom_rv draw 1: {}\ntest_multinom_rv draw 2: {}".format(
    test_multinom_draw(), test_multinom_draw()))
#+END_SRC

#+RESULTS: dependent-poisson-multinomial-exa
#+begin_SRC python :eval never
test_multinom_rv draw 1: [0 2 0 0 1 0 2 1 0 0]
test_multinom_rv draw 2: [5 2 1 0 0 0 1 0 1 1 0 1 0]


#+end_SRC
:END:
** Random Variable Pretty Printing

In Listing [[rv-pprinter-imp]], we implement a pretty printer that produces more readable
forms of Theano graphs containing src_python{RandomVariable} nodes.

#+NAME: rv-pprinter-imp
#+BEGIN_SRC python :exports code :results none :noweb-ref new-op-code
class RandomVariablePrinter:
    """Pretty print random variables

    NOTE: When parsing LaTeX output (i.e. `self.latex=True`) in `self.process`,
    the `pstate` object is checked for a boolean `latex` and `latex_aligned`.
    The former enables LaTeX formatted output, and the latter, when `True`,
    adds a "&" before the "\sim" in the LaTeX output.
    """
    def __init__(self, name=None, latex=False):
        """
        Parameters
        ==========
        name: str (optional)
            A fixed name to use for the random variables printed by this
            printer.  If not specified, use `RandomVariable.name`.
        latex: boolean (optional)
            Whether or not to print LaTeX strings.
        """
        self.name = name
        self.latex = latex

    def process(self, output, pstate):
        if output in pstate.memo:
            return pstate.memo[output]

        pprinter = pstate.pprinter
        node = output.owner

        if node is None or not isinstance(node.op, RandomVariable):
            raise TypeError("function %s cannot represent a variable that is "
                            "not the result of a RandomVariable operation" % self.name)

        new_precedence = -1000
        out_name = output.name
        if not out_name:
            if hasattr(pstate, 'tag_generator'):
                out_name = pstate.tag_generator.get_tag()
            else:
                out_name = output.auto_name

        try:
            old_precedence = getattr(pstate, 'precedence', None)
            pstate.precedence = new_precedence

            op_name = self.name or node.op.name

            if self.latex or getattr(pstate, 'latex', False):
                sep = '&' if getattr(pstate, 'latex_aligned', False) else ''
                dist_format = "{} %s\\sim \\text{{{}}}\\left({}\\right)" % sep
            else:
                dist_format = "{} ~ {}({})"

            r = dist_format.format(
                out_name, op_name,
                ", ".join([pprinter.process(input, pstate)
                           # Skip the rng type and size parameters.
                           for input in node.inputs[2:]]))
        finally:
            pstate.precedence = old_precedence

        pstate.preamble_lines += [r]
        pstate.memo[output] = out_name

        return out_name
#+END_SRC

#+NAME: variable-tag-printer-class
#+BEGIN_SRC python :exports code :results none :noweb-ref new-op-code
from sympy import Array as SympyArray
from sympy.printing import latex as sympy_latex


class VariableTagPrinter:
    """Use readable character names for unamed variables (that aren't constant values).

    TODO: Avoid naming collisions.
    """
    @staticmethod
    def process(output, pstate):
        if output in pstate.memo:
            return pstate.memo[output]

        if isinstance(output, tt.gof.Constant):
            if output.ndim > 0 and getattr(pstate, 'latex', False):
                out_name = sympy_latex(SympyArray(output.data))
            else:
                out_name = str(output.data)
        elif output.name:
            out_name = output.name
        elif isinstance(output, tt.TensorVariable):
            out_name = "{}_u".format(pstate.tag_generator.get_tag())
        else:
            out_name = theano.printing.default_printer.process(output, pstate)

        pstate.memo[output] = out_name

        return out_name
#+END_SRC

#+NAME: preamble-pprinter-class
#+BEGIN_SRC python :exports code :results none :noweb-ref new-op-code
class PreamblePPrinter(theano.printing.PPrinter):
    """Pretty printer that displays a preamble.

    For example,

        X ~ N(\mu, \sigma)
        (b * X)

    XXX: Not thread-safe!
    """
    def __init__(self, *args, pstate_defaults=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.pstate_defaults = pstate_defaults or {}
        self.printers_dict = dict(tt.pprint.printers_dict)
        self.printers = copy(tt.pprint.printers)
        self._pstate = None

    def create_state(self, pstate):
        # FIXME: Find all the user-defined node names and make the tag
        # generator aware of them.
        if pstate is None:
            pstate = theano.printing.PrinterState(pprinter=self,
                                                  preamble_lines=[],
                                                  tag_generator=theano.printing._TagGenerator(),
                                                  ,**self.pstate_defaults)
        elif isinstance(pstate, dict):
            pstate.setdefault('preamble_lines', [])
            pstate.setdefault('tag_generator', theano.printing._TagGenerator())
            pstate.update(self.pstate_defaults)
            pstate = theano.printing.PrinterState(pprinter=self, **pstate)

        # FIXME: Good old fashioned circular references...
        # We're doing this so that `self.process` will be called correctly
        # accross all code.  (I'm lookin' about you, `DimShufflePrinter`; get
        # your act together.)
        pstate.pprinter._pstate = pstate

        return pstate

    def process(self, r, pstate=None):
        pstate = self._pstate
        assert pstate
        return super().process(r, pstate)

    def __call__(self, *args):
        if len(args) == 2 and isinstance(args[1], (theano.printing.PrinterState, dict)):
            pstate = self.create_state(args[1])
            args = (args[0], pstate)
        elif len(args) == 1:
            pstate = self.create_state(None)
            args += (pstate,)
        else:
            # XXX: The graph processing doesn't pass around the printer state!
            # TODO: We'll have to copy the code and fix it...
            raise NotImplemented('No preambles for graph printing, yet.')

        body_str = super().__call__(*args)
        if pstate.preamble_lines and getattr(pstate, 'latex', False):
            preamble_str = "\n\\\\\n".join(pstate.preamble_lines)
            if getattr(pstate, 'latex_aligned', False):
                preamble_str = "\\begin{{aligned}}\n{}\n\\end{{aligned}}".format(preamble_str)
            return "\n\\\\\n".join([preamble_str, body_str])
        else:
            return "\n".join(pstate.preamble_lines + [body_str])
#+END_SRC

#+NAME: instantiate-pprinter-classes
#+BEGIN_SRC python :exports code :results none :noweb-ref new-op-code
tt_pprint = PreamblePPrinter()
tt_pprint.assign(lambda pstate, r: True, VariableTagPrinter)
tt_pprint.assign(UniformRV, RandomVariablePrinter('U'))
tt_pprint.assign(NormalRV, RandomVariablePrinter('N'))
tt_pprint.assign(GammaRV, RandomVariablePrinter('Gamma'))
tt_pprint.assign(ExponentialRV, RandomVariablePrinter('Exp'))
tt_pprint.assign(MvNormalRV, RandomVariablePrinter('N'))
tt_pprint.assign(DirichletRV, RandomVariablePrinter('Dir'))
tt_pprint.assign(PoissonRV, RandomVariablePrinter('Pois'))
tt_pprint.assign(CauchyRV, RandomVariablePrinter('C'))
tt_pprint.assign(MultinomialRV, RandomVariablePrinter('MN'))

tt_tex_pprint = PreamblePPrinter(pstate_defaults={'latex': True, 'latex_aligned': True})
tt_tex_pprint.printers = copy(tt_pprint.printers)
tt_tex_pprint.printers_dict = dict(tt_pprint.printers_dict)
tt_tex_pprint.assign(tt.mul, theano.printing.OperatorPrinter('\\circ', -1, 'either'))
tt_tex_pprint.assign(tt.true_div, theano.printing.PatternPrinter(('\\frac{%(0)s}{%(1)s}', -1000)))
tt_tex_pprint.assign(tt.pow, theano.printing.PatternPrinter(('{%(0)s}^{%(1)s}', -1000)))
#+END_SRC

#+NAME: rv-pprinter-tests
#+BEGIN_SRC python :eval never-export :exports none :results silent
tt_normalrv_name_expr = tt.scalar('b') * NormalRV(tt.scalar('\mu'), tt.scalar('\sigma'), name='X')
tt_normalrv_noname_expr = tt.scalar('b') * NormalRV(tt.scalar('\mu'), tt.scalar('\sigma'))
tt_2_normalrv_noname_expr = NormalRV(tt.scalar('\mu_2'), tt.scalar('\sigma_2')) * (tt.scalar('b') * NormalRV(tt.scalar('\mu'), tt.scalar('\sigma')) + tt.scalar('c'))

assert tt_pprint(tt_normalrv_name_expr) == "X ~ N(\mu, \sigma)\n(b * X)"
assert tt_pprint(tt_normalrv_noname_expr) == "A ~ N(\mu, \sigma)\n(b * A)"
assert tt_pprint(tt_2_normalrv_noname_expr) == "A ~ N(\mu_2, \sigma_2)\nB ~ N(\mu, \sigma)\n(A * ((b * B) + c))"
#+END_SRC

:EXAMPLE:
Listing [[rv-pprinter-exa]], creates a graph with two random variables and prints the
results with the default Theano pretty printer.
#+NAME: rv-theano-pprinter-exa
#+BEGIN_SRC python :eval never-export :results output :wrap "SRC text :eval never"
Z_tt = UniformRV(tt.scalar('l_0'), tt.scalar('l_1'), name='Z')
X_tt = NormalRV(Z_tt, tt.scalar('\sigma_1'), name='X')
Y_tt = MvNormalRV(tt.vector('\mu'), tt.abs_(X_tt) * tt.constant(np.diag([1, 2])), name='Y')

W_tt = X_tt * (tt.scalar('b') * Y_tt + tt.scalar('c'))

print(tt.pprint(W_tt))
#+END_SRC

#+RESULTS: rv-theano-pprinter-exa
#+begin_SRC text :eval never
(normal_rv(<RandomStateType>, TensorConstant{[]}, uniform_rv(<RandomStateType>, TensorConstant{[]}, l_0, l_1), \sigma_1) * ((b * multivariate_normal_rv(<RandomStateType>, TensorConstant{[]}, \mu, (|normal_rv(<RandomStateType>, TensorConstant{[]}, uniform_rv(<RandomStateType>, TensorConstant{[]}, l_0, l_1), \sigma_1)| * TensorConstant{[[1 0]
 [0 2]]}))) + c))


#+end_SRC


#+NAME: rv-pprinter-exa
#+BEGIN_SRC python :eval never-export :results output scalar raw replace
print("\\begin{{equation*}}\n{}\n\\end{{equation*}}".format(
    tt_tex_pprint(W_tt, {'latex': True, 'latex_aligned': True})))
#+END_SRC

#+RESULTS: rv-pprinter-exa
\begin{equation*}
\begin{aligned}
Z &\sim \text{U}\left(l_0, l_1\right)
\\
X &\sim \text{N}\left(Z, \sigma_1\right)
\\
Y &\sim \text{N}\left(\mu, (|X| \circ \left[\begin{matrix}1 & 0\\0 & 2\end{matrix}\right])\right)
\end{aligned}
\\
(X \circ ((b \circ Y) + c))
\end{equation*}


:END:

* Testing src_python[]{RandomVariable}                             :noexport:
In the following we implement some unit-like tests
for src_python[]{RandomVariable}.  They confirm expected sample dimensions and
broadcast properties.

#+BEGIN_SRC python
def rv_numpy_test(rv, *params, size=None):
    """Test for correspondence between `RandomVariable` and NumPy shape and
    broadcast dimensions.
    """
    test_rv = rv(*params, size=size)
    param_vals = [tt.gof.op.get_test_value(p) for p in params]
    size_val = None if size is None else tt.gof.op.get_test_value(size)
    test_val = getattr(np.random, rv.name)(*param_vals, size=size_val)
    test_shp = np.shape(test_val)

    # This might be a little too harsh, since purely symbolic `tensor.vector` inputs
    # have no broadcastable information, yet, they can take broadcastable values.
    # E.g.
    #     x_tt = tt.vector('x')
    #     x_tt.tag.test_value = np.array([5]) # non-symbolic value is broadcastable!
    #     x_tt.tag.test_value = np.array([5, 4]) # non-symbolic value is not broadcastable.
    #
    # In the end, there's really no clear way to determine this without full
    # evaluation of a symbolic node, and that mostly defeats the purpose.
    # Unfortunately, this is what PyMC3 resorts to when constructing its
    # `TensorType`s (and shapes).
    test_bcast = [s == 1 for s in test_shp]
    np.testing.assert_array_equal(test_rv.type.broadcastable, test_bcast)

    eval_args = {p: v for p, v in zip(params, param_vals)
                 if isinstance(p, tt.Variable) and not isinstance(p, tt.Constant)}
    np.testing.assert_array_equal(test_rv.shape.eval(eval_args), test_shp)
    np.testing.assert_array_equal(np.shape(test_rv.eval(eval_args)), test_shp)


tt.config.on_opt_error = 'raise'

rv_numpy_test(NormalRV, 0., 1.)
rv_numpy_test(NormalRV, 0., 1., size=[3])
# Broadcast sd over independent means...
rv_numpy_test(NormalRV, [0., 1., 2.], 1.)
rv_numpy_test(NormalRV, [0., 1., 2.], 1., size=[3, 3])
rv_numpy_test(NormalRV, [0], [1], size=[1])

rv_numpy_test(NormalRV, tt.as_tensor_variable([0]), [1], size=[1])
rv_numpy_test(NormalRV, tt.as_tensor_variable([0]), [1], size=tt.as_tensor_variable([1]))


# XXX: Shouldn't work due to broadcastable comments in `rv_numpy_test`.
# test_mean = tt.vector('test_mean')
# test_mean.tag.test_value = np.r_[1]
# rv_numpy_test(NormalRV, test_mean, [1], size=tt.as_tensor_variable([1]))

# with pm.Model():
#     test_rv = pm.MvNormal('test_rv', [0], np.diag([1]), shape=1)
#
# test_rv.broadcastable

rv_numpy_test(MvNormalRV, [0], np.diag([1]))
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4, 1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4, 1, 1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[1, 5, 8])
rv_numpy_test(MvNormalRV, [0, 1, 2], np.diag([1, 1, 1]))
# Broadcast cov matrix across independent means?
# Looks like NumPy doesn't support that (and are probably better off for it).
# rv_numpy_test(MvNormalRV, [[0, 1, 2], [4, 5, 6]], np.diag([1, 1, 1]))
#+END_SRC

* Algebraic Manipulations
With our new src_python[]{RandomVariable}, we can alter the replacement patterns
used by src_python[]{tt.gof.opt.PatternSub} in
[[citet:WillardSymbolicMathPyMC32018]] and implement a slightly better parameter
lifting for affine transforms of scalar normal random variables in
[[scalar_norm_affine_rv_opt]].

#+NAME: algebra-requirements
#+BEGIN_SRC python :eval never-export :exports none :results none :noweb strip-export
<<new-op-code>>
#+END_SRC

#+NAME: scalar_norm_affine_rv_opt
#+BEGIN_SRC python :exports code :results silent
# We use the following to handle keyword arguments.
construct_norm_rv = lambda rng, size, mu, sd: NormalRV(mu, sd, size=size, rng=rng)

norm_lift_pats = [
    # Lift element-wise multiplication
    tt.gof.opt.PatternSub(
        (tt.mul,
         'a_x',
         (NormalRV, 'rs_x', 'size_x', 'mu_x', 'sd_x')),
        (construct_norm_rv,
         'rs_x',
         # XXX: Is this really consistent?  How will it handle broadcasting?
         'size_x',
         (tt.mul, 'a_x', 'mu_x'),
         (tt.mul, 'a_x', 'sd_x'),
        )),
    # Lift element-wise addition
    tt.gof.opt.PatternSub(
        (tt.add,
         (NormalRV, 'rs_x', 'size_x', 'mu_x', 'sd_x'),
         'b_x'),
        (construct_norm_rv,
         'rs_x',
         # XXX: Is this really consistent?  How will it handle broadcasting?
         'size_x',
         (tt.add, 'mu_x', 'b_x'),
         'sd_x',
        )),
]

norm_lift_opts = tt.gof.opt.EquilibriumOptimizer(
    norm_lift_pats, max_use_ratio=10)
#+END_SRC

#+NAME: graph-manipulation-setup
#+BEGIN_SRC python :eval never :exports none :results silent
from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv

theano.config.compute_test_value = 'ignore'
#+END_SRC

:EXAMPLE:
#+ATTR_LATEX: :float nil
#+CAPTION: Scaled normal random variable example using src_python[]{RandomVariable}.
#+NAME: mat_mul_scaling_rv_exa
#+BEGIN_SRC python :results none :noweb yes
<<graph-manipulation-setup>>

mu_X = tt.vector('\mu')
sd_X = tt.vector('\sigma')

a_tt = tt.fscalar('a')
b_tt = tt.fscalar('b')

X_rv = NormalRV(mu_X, sd_X, name='X')
trans_X_rv = a_tt * X_rv + b_tt

trans_X_graph = FunctionGraph(tt_inputs([trans_X_rv]), [trans_X_rv])

# Create a copy and optimize that
trans_X_graph_opt = trans_X_graph.clone()

_ = norm_lift_opts.optimize(trans_X_graph_opt)
#+END_SRC

#+NAME: before_mat_mul_scaling_rv_exa
#+BEGIN_SRC python :eval never-export :results output scalar raw replace
print("\\begin{{equation*}}\n{}\n\\end{{equation*}}".format(
    tt_tex_pprint(trans_X_graph.outputs[0])))
#+END_SRC

Before applying the optimization:
#+RESULTS: before_mat_mul_scaling_rv_exa
\begin{equation*}
\begin{aligned}
X &\sim \text{N}\left(\mu, \sigma\right)
\end{aligned}
\\
((a \circ X) + b)
\end{equation*}


#+NAME: after_mat_mul_scaling_rv_exa
#+BEGIN_SRC python :eval never-export :results output scalar raw replace
print("\\begin{{equation*}}\n{}\n\\end{{equation*}}".format(
    tt_tex_pprint(trans_X_graph_opt.outputs[0])))
#+END_SRC

After applying the optimization:
#+RESULTS: after_mat_mul_scaling_rv_exa
\begin{equation*}
\begin{aligned}
A &\sim \text{N}\left(((a \circ \mu) + b), (a \circ \sigma)\right)
\end{aligned}
\\
A
\end{equation*}


:END:

:TODO:
- What about division and subtraction?  These can be addressed using canonicalization?
:END:

Now, what if we wanted to handle affine transformations of a multivariate normal
random variable?  Specifically, consider the following:
\begin{equation*}
  X \sim N\left(\mu, \Sigma \right), \quad
  A X \sim N\left(A \mu, A \Sigma A^\top \right)
 \;.
\end{equation*}

At first, the substitution pattern in Listing [[multi_norm_affine_rv_opt]] might
seem reasonable.
#+ATTR_LATEX: :float t :placement h
#+NAME: multi_norm_affine_rv_opt
#+BEGIN_SRC python :eval never :output code :results none
# Vector multiplication
tt.gof.opt.PatternSub(
    (tt.dot,
     'A_x',
     (MvNormalRV, 'rs_x', 'size_x', 'mu_x', 'cov_x')),
    (construct_rv,
     MvNormalRV,
     'rs_x',
     'size_x',
     (tt.dot, 'A_x', 'mu_x'),
     (tt.dot,
      (tt.dot, 'A_x', 'cov_x')
      (tt.transpose, 'A_x')),
    ))
#+END_SRC

Unfortunately, the combination of size parameter and broadcasting complicates
the scenario.  Both parameters indirectly affect the distribution parameters,
making the un-lifted dot-product consistent, but not necessarily the lifted products.

The following example demonstrates the lifting issues brought on by
broadcasting.

:EXAMPLE:
We create a simple multivariate normal in Listing [[simple_mvnorm_exa]].
#+ATTR_LATEX: :float t :placement h
#+NAME: simple_mvnorm_exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
mu_X = [0, 10]
cov_X = np.diag([1, 1e-2])
size_X_rv = [2, 3]
X_rv = MvNormalRV(mu_X, cov_X, size=size_X_rv)

print('X_rv sample:\n{}\n'.format(X_rv.tag.test_value))
#+END_SRC

#+RESULTS: simple_mvnorm_exa
#+begin_SRC python :eval never
X_rv sample:
[[[ 1.73535673 10.14604812]
  [ 0.81111665 10.07573636]
  [-0.79263131  9.95495907]]

 [[-1.45442983 10.17411569]
  [-0.13669398 10.02984376]
  [ 0.58279226 10.02003365]]]


#+end_SRC

Next, we create a simple matrix operator to apply to the multivariate normal.
#+NAME: simple_mvnorm_op_exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
A_tt = tt.as_tensor_variable([[2, 5, 8], [3, 4, 9]])
# or A_tt = tt.as_tensor_variable([[2, 5, 8]])

# It's really just `mu_X`...
E_X_rv = X_rv.owner.inputs[2]

print('A * X_rv =\n{}\n'.format(tt.dot(A_tt, X_rv).tag.test_value))
#+END_SRC

#+RESULTS: simple_mvnorm_op_exa
#+begin_SRC python :eval never
A * X_rv =
[[[  1.18524621 150.31045062]
  [  1.07000851 150.65771936]]

 [[  1.31685497 160.33572146]
  [  0.33506491 160.82202495]]]


#+end_SRC

As we can see, the multivariate normal's test/sampled value has the correct
shape for our matrix operator.

#+NAME: simple_mvnorm_op_err_exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
import traceback
try:
    print('A * E[X_rv] =\n{}\n'.format(tt.dot(A_tt, E_X_rv).tag.test_value))
except ValueError as e:
    print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS: simple_mvnorm_op_err_exa
#+begin_SRC python :eval never
ValueError: shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)


#+end_SRC

However, we see that the multivariate normal's inputs (i.e. the src_python[]{Op}
inputs)--specifically the mean parameter--do not directly reflect the support's
shape, as one might expect.

#+NAME: simple_mvnorm_op_no_err_exa
#+BEGIN_SRC python :wrap "SRC python :eval never"
size_tile = tuple(size_X_rv) + (1,)
E_X_rv_ = tt.tile(E_X_rv, size_tile, X_rv.ndim)

print('A * E[X_rv] =\n{}\n'.format(tt.dot(A_tt, E_X_rv_).tag.test_value))
#+END_SRC

#+RESULTS: simple_mvnorm_op_no_err_exa
#+begin_SRC python :eval never
A * E[X_rv] =
[[[  0 150]
  [  0 150]]

 [[  0 160]
  [  0 160]]]


#+end_SRC

We can manually replicate the inputs so that they match the output shape, but
a solution to the general problem requires a more organized response.
:END:

:TESTING:
#+BEGIN_SRC python :eval never :exports none :results none
def replicate_expr(param, rep_size, dist_op, param_idx):
    return tt.tile(param, tuple(rep_size) + (1,), dist_op.ndims_params[param_idx] + len(rep_size))

mvnorm_lift_pats = tt.gof.opt.PatternSub(
    (tt.dot,
     'A_x',
     (MvNormalRV, 'rs_x', 'size_x', 'mu_x', 'cov_x')),
    (construct_rv,
     MvNormalRV,
     'rs_x',
     'size_x',
     (tt.dot, 'A_x',
      (replicate_expr, 'mu_x', 'size_x', MvNormalRV, 0)),
     (tt.dot,
      (tt.dot, 'A_x',
       (replicate_expr, 'cov_x', 'size_x', MvNormalRV, 1)),
      (tt.transpose, 'A_x'))),
)

mvnorm_lift_opts = tt.gof.opt.EquilibriumOptimizer(
    [ mvnorm_lift_pats ], max_use_ratio=10)

mu_X = [0, 10]
cov_X = np.diag([1, 1e-2])
X_rv = MvNormalRV(mu_X, cov_X, size=[2, 3])

A_tt = tt.as_tensor_variable([[2, 5, 8], [3, 4, 9]])

Z_rv = tt.dot(A_tt, X_rv)

Z_graph = FunctionGraph(tt_inputs([Z_rv]), [Z_rv])

tt.printing.debugprint(Z_graph)

Z_graph_opt = Z_graph.clone()

# This won't work because `tt.dot` uses `tensor_dot`, which does some
# reshaping and dim-shuffling.
# We need to account for those, as well.
# TODO: We need to implement something like `local_dimshuffle_lift` for some RVs.
_ = mvnorm_lift_opts.optimize(Z_graph_opt)

tt.printing.debugprint(Z_graph_opt)
#+END_SRC
:END:

* A Problem with Conversion from PyMC3

#+NAME: pymc3-conversion-requirements
#+BEGIN_SRC python :eval never-export :exports none :results none :noweb strip-export
<<new-op-code>>
#+END_SRC

As in [[citet:WillardSymbolicMathPyMC32018]], we can create mappings between
existing PyMC3 random variables and their new src_python[]{RandomVariable}
equivalents.

:EXAMPLE:
#+NAME: pymc_theano_rv_equivs
#+BEGIN_SRC python :results none
pymc_theano_rv_equivs = {
    pm.Normal:
    lambda dist, rand_state:
    (None,
     # PyMC3 shapes aren't NumPy-like size parameters, so we attempt to
     # adjust for that.
     NormalRV(dist.mu, dist.sd, size=dist.shape[1:], rng=rand_state)),
    pm.MvNormal:
    lambda dist, rand_state:
    (None, NormalRV(dist.mu, dist.cov, size=dist.shape[1:], rng=rand_state)),
}
#+END_SRC
:END:

However, if we attempt the same PymC3 graph conversion approach as before
(i.e. convert a PyMC3 model to a Theano src_python[]{FunctionGraph}
using src_python[]{model_graph}, then replace PyMC3 random variable nodes with
our new random variable types using src_python[]{create_theano_rvs}), we're
likely to run into a problem involving mismatching broadcastable dimensions.

The problem arises because *PyMC3 "knows" more broadcast information than it
should*, since it uses the Theano variables' test values in order to obtain
concrete shapes for the random variables it creates.  Using concrete,
non-symbolic shapes, it can exactly determine what would otherwise be ambiguous
[[http://deeplearning.net/software/theano/library/tensor/basic.html?highlight=broadcastable#theano.tensor.TensorType.broadcastable][broadcastable dimensions]] at the symbolic level.

More specifically, broadcast information is required during the construction of a
Theano src_python[]{TensorType}, so PyMC3 random variable types can be
inconsistent (unnecessarily restrictive, really) causing Theano to complain when
we try to construct a src_python[]{FunctionGraph}.

:EXAMPLE:
Consider the following example; it constructs two purely symbolic
Theano vectors: one with broadcasting and one without.
#+ATTR_LATEX: :float t :placement h
#+NAME: y-x-broadcast-setup
#+BEGIN_SRC python :wrap "SRC python :eval never"
y_tt = tt.row('y')
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))

x_tt = tt.matrix('x')
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))
#+END_SRC

#+RESULTS: y-x-broadcast-setup
#+begin_SRC python :eval never
y_tt.broadcastable = (True, False)
x_tt.broadcastable = (False, False)


#+end_SRC
Notice that it--by default--signifies no broadcasting on its first and only
dimension.

If we wish--or if [[http://deeplearning.net/software/theano/library/config.html#config.compute_test_value][Theano's configuration demands]] it--we can assign the
symbolic vector arbitrary test values, as long as they're consistent with its
type (i.e. a vector, or 1-dimensional array).

In the following, we assign both a broadcastable (i.e. first--and only--dimension has
size 1) and non-broadcastable test value.

Test value is broadcastable:
#+NAME: x-broadcast-noerror
#+BEGIN_SRC python :wrap "SRC python :eval never"
from contextlib import contextmanager


x_tt.tag.test_value = np.array([[5]])

print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(x_tt.tag.test_value).broadcastable))
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))

@contextmanager
def short_exception_msg(exc_type):
    _verbosity = theano.config.exception_verbosity
    theano.config.exception_verbosity = 'low'
    try:
        yield
    except exc_type as e:
        import traceback
        print("".join(traceback.format_exception_only(type(e), e)))
    finally:
        theano.config.exception_verbosity = _verbosity


with short_exception_msg(TypeError):
    x_tt.shape
    print("shape checks out!")
#+END_SRC

#+RESULTS: x-broadcast-noerror
#+begin_SRC python :eval never
test_value.broadcastable = (True, True)
x_tt.broadcastable = (False, False)
shape checks out!


#+end_SRC

#+NAME: y-broadcast-noerror
#+BEGIN_SRC python :wrap "SRC python :eval never"
y_tt.tag.test_value = np.array([[5]])

print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(y_tt.tag.test_value).broadcastable))
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))

with short_exception_msg(TypeError):
    y_tt.shape
    print("shape checks out!")
#+END_SRC

#+RESULTS: y-broadcast-noerror
#+begin_SRC python :eval never
test_value.broadcastable = (True, True)
y_tt.broadcastable = (True, False)
shape checks out!


#+end_SRC

Test value is *not* broadcastable:
#+NAME: x-broadcast-error
#+BEGIN_SRC python :wrap "SRC python :eval never"
x_tt.tag.test_value = np.array([[5, 4]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(x_tt.tag.test_value).broadcastable))
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))

with short_exception_msg(TypeError):
    x_tt.shape
    print("shape checks out!")
#+END_SRC

#+RESULTS: x-broadcast-error
#+begin_SRC python :eval never
test_value.broadcastable = (True, False)
x_tt.broadcastable = (False, False)
shape checks out!


#+end_SRC

#+NAME: y-broadcast-error
#+BEGIN_SRC python :wrap "SRC python :eval never"
y_tt.tag.test_value = np.array([[5, 4], [3, 2]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(y_tt.tag.test_value).broadcastable))
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))

with short_exception_msg(TypeError):
    y_tt.shape
    print("shape checks out!")
#+END_SRC

#+RESULTS: y-broadcast-error
#+begin_SRC python :eval never
test_value.broadcastable = (False, False)
y_tt.broadcastable = (True, False)
TypeError: For compute_test_value, one input test value does not have the requested type.

Backtrace when that variable is created:

  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py", line 485, in mainloop
    self.interact()
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py", line 476, in interact
    self.run_cell(code, store_history=True)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2909, in run_ast_nodes
    if self.run_code(code, result):
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-19-7427b1688530>", line 1, in <module>
    __org_babel_python_fname = '/tmp/user/1000/babel-fsZXPU/python-cZypXi'; __org_babel_python_fh = open(__org_babel_python_fname); exec(compile(__org_babel_python_fh.read(), __org_babel_python_fname, 'exec')); __org_babel_python_fh.close()
  File "/tmp/user/1000/babel-fsZXPU/python-cZypXi", line 1, in <module>
    y_tt = tt.row('y')

The error when converting the test value to that variable type:
Non-unit value on shape on a broadcastable dimension.
(2, 2)
(True, False)


#+end_SRC

Simply put: non-broadcastable Theano tensor variable types can take
broadcastable and non-broadcastable values, while broadcastable types can only
take broadcastable values.
:END:

What we can take from the example above is that if we determine that a vector
has broadcastable dimensions using test values--as PyMC3 does--we unnecessarily
introduce restrictions and potential inconsistencies down the line.
One point of origin for such issues is *shared variables*.

* Discussion

In follow-ups to this series, we'll address a few loose ends, such as
- the inclusion of density functions and likelihoods,
- decompositions/reductions of overlapping multivariate types
  (e.g. transforms between tensors of univariate normals and equivalent
  multivariate normals),
- canonicalization of graphs containing src_python{RandomVariable} terms,
- and more optimizations that specifically target MCMC schemes (e.g. automatic conversion to scale
  mixture decompositions).

:TODO:
Talk about src_python[]{supp_shape_fn}.
:END:

#+BIBLIOGRAPHY: ../tex/symbolic-pymc3.bib
#+BIBLIOGRAPHYSTYLE: plainnat
