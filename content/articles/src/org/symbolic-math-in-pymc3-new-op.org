#+TITLE: Random Variables in Theano
#+AUTHOR: Brandon T. Willard
#+DATE: 2018-12-28
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(results) html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+INCLUDE: org-setup.org
#+BEGIN_SRC elisp :eval t :exports none :results none
(org-babel-lob-ingest "org-babel-extensions.org")
#+END_SRC

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session symbolic-math-pymc3-2

#+BEGIN_abstract
Continuing from [[cite:WillardSymbolicMathPyMC32018]], we'll attempt to improve
upon src_python[]{RandomFunction} and make a case for a similar src_python[]{Op} in
PyMC3.
#+END_abstract

* A *new* Random Variable src_python[]{Op}

We'll call this new src_python[]{Op} src_python[]{RandomVariable}, since random
variables are the abstraction we're primarily targeting.
src_python[]{RandomVariable} will provide the functionality
of src_python[]{Distribution}, src_python[]{FreeRV} and src_python[]{ObservedRV}, and,
by working at the src_python[]{Op} level, it will be much more capable of leveraging
existing Theano functionality.

Specifically, by using the src_python[]{Op} interface, we're able to do the
following:

1. Reduce/remove the need for an explicitly specified shape parameter.
   #+HTML: <div class="example" markdown="">
   For example, definitions like
   #+BEGIN_SRC python
   with pm.Model():
       X_rv = pm.Normal('X_rv', mu_X, sd=sd_X, shape=(1,))
   #+END_SRC
   reduce to
   #+BEGIN_SRC python
   with pm.Model():
       X_rv = pm.Normal('X_rv', mu_X, sd=sd_X)
   #+END_SRC
   #+HTML: </div>
2. Random variable nodes created by an src_python[]{Op} automatically implement
   src_python[]{Distribution.default}/src_python[]{Distribution.get_test_val}
   functionality and remove the reliance on initial values during random
   variable instantiation.  src_python[]{Op} automatically
   uses src_python[]{Op.perform}, which will draw a sample as a test value *and*
   propagate it throughout the graph to derived/down-stream tensor variables.
3. Log-densities can be generated as secondary outputs of
   src_python[]{Op.make_node}, which removes the need
   for src_python[]{Distribution.logp*} methods.
4. src_python[]{pymc.distribution.draw_values} and related methods are no longer necessary;
   their functionality is already covered within Theano's existing graph machinery--in the
   same way as src_python[]{pymc.distribution.Distribution.default/get_test_val}.

The main points of entry in our src_python[]{Op}, are src_python[]{Op.make_node}
and src_python[]{Op.perform}.  src_python[]{Op.make_node} is used during symbolic
graph creation and provides immediate access to the src_python[]{Op}'s
symbolic inputs--serving a purpose similar to src_python[]{Distribution.__init__}.
src_python[]{Op.make_node} is where shape inference tasks (e.g. [[https://github.com/pymc-devs/pymc3/pull/1125][PyMC3 PR 1125]]) are more
suitably addressed; however, src_python[]{Op} provides additional means of shape inference
and management (e.g. src_python[]{Op.infer_shape}) occurring at different phases of
graph compilation that aren't readily accessible outside of the src_python[]{Op} framework.

** Implementation
#+ATTR_LATEX: :float t :placement h!
#+NAME: import_theano_pymc3
#+BEGIN_SRC python
import sys
import os

from pprint import pprint

import numpy as np

os.environ['MKL_THREADING_LAYER'] = 'GNU'

import theano
import theano.tensor as tt

theano.config.mode = 'FAST_COMPILE'
theano.config.exception_verbosity = 'high'
theano.config.compute_test_value = 'raise'

import pymc3 as pm
#+END_SRC

#+ATTR_LATEX: :float nil
#+CAPTION: Helper function for src_python[]{RandomVariable}
#+NAME: supp_shape_fn
#+BEGIN_SRC python
from collections.abc import Iterable, ByteString
from warnings import warn
from copy import copy

from theano.tensor.raw_random import (RandomFunction, RandomStateType,
                                      _infer_ndim_bcast)


def matched_supp_shape_fn(ndim_supp, ndims_params, dist_params,
                          param_shapes=None):
    """A function for extracting a random variable's support shape/dimensions
    from other (e.g. distribution parameters) shape information.

    This default implementation uses the first non-independent/replicated
    dimension of the first distribution parameter.

    For example, with a normal random variable, the shape of the mean parameter
    along the first dimension will be used; dimensions after that, if any,
    determine the mean parameters for other *independent* normal variables.
    """
    # XXX: Gotta be careful slicing Theano variables, the `Subtensor` Op isn't
    # handled by `tensor.get_scalar_constant_value`!
    # E.g.
    #     test_val = tt.as_tensor_variable([[1], [4]])
    #     tt.get_scalar_constant_value(test_val.shape[-1]) # works
    #     tt.get_scalar_constant_value(test_val.shape[0]) # doesn't
    #     tt.get_scalar_constant_value(test_val.shape[:-1]) # doesn't
    if param_shapes is not None:
        # return param_shapes[0][-self.ndim_supp:]
        return (param_shapes[0][-ndim_supp],)
    else:
        ref_shape = tt.shape(dist_params[0])
        # return ref_shape[-self.ndim_supp:]
        return (ref_shape[-ndim_supp],)
#+END_SRC

#+ATTR_LATEX: :float nil
#+CAPTION: A new random variable src_python[]{Op}.
#+NAME: new_rv_op
#+BEGIN_SRC python
class RandomVariable(tt.gof.Op):
    """This is essentially `RandomFunction`, except that it removes the `outtype`
    dependency and handles shape dimension information more directly.
    """
    __props__ = ('name', 'dtype', 'ndim_supp', 'inplace', 'ndims_params')

    def __init__(self, name, ndim_supp, ndims_params, rng_fn, *args,
                 supp_shape_fn=None, dtype=theano.config.floatX, inplace=False,
                 ,**kwargs):
        """Create a random variable `Op`.

        Parameters
        ==========
        ndim_supp: int
            Dimension of the support.  This value is used to infer the exact
            shape of the support and independent terms from ``dist_params``.
        ndims_params: tuple (int)
            Number of dimensions for each parameter in ``dist_params``
            for a single variate.  Used to determine the shape of the
            independent variate space.
        rng_fn: function or str
            Sampler function.  Can be the string name of a method provided by
            `numpy.random.RandomState`.
        supp_shape_fn: callable
            Function used to determine the exact shape of the distribution's
            support. It must take arguments ndim_supp, ndims_params,
            dist_params (i.e. an collection of the distribution parameters) and an
            optional param_shapes (i.e. tuples containing the size of each
            dimension for each distribution parameter).

            Defaults to `supp_shape_fn`
        """
        super().__init__(*args, **kwargs)

        self.name = name
        self.inplace = inplace
        self.dtype = dtype

        self.supp_shape_fn = supp_shape_fn or matched_supp_shape_fn

        self.ndim_supp = ndim_supp

        if not isinstance(ndims_params, Iterable):
            raise ValueError('Parameter ndims_params must be iterable.')

        self.ndims_params = tuple(ndims_params)

        self.default_output = 1

        if isinstance(rng_fn, (str, ByteString)):
            self.rng_fn = getattr(np.random.RandomState, rng_fn)
        else:
            self.rng_fn = rng_fn

    def __str__(self):
        return '{}_rv'.format(self.name)

    def _infer_shape(self, size, dist_params, param_shapes=None):
        """Compute shapes and broadcasts properties.

        Inspired by `tt.add.get_output_info`.
        """

        size_len = tt.get_vector_length(size)

        dummy_params = tuple(p if n == 0 else tt.ones(tuple(p.shape)[:-n])
                             for p, n in zip(dist_params, self.ndims_params))

        _, out_bcasts, bcastd_inputs = tt.add.get_output_info(
            tt.DimShuffle, *dummy_params)

        # _, out_bcasts, bcastd_inputs = tt.add.get_output_info(tt.DimShuffle, *dist_params)
        # .tag.test_value

        bcast_ind, = out_bcasts
        ndim_ind = len(bcast_ind)
        shape_ind = bcastd_inputs[0].shape

        if self.ndim_supp == 0:
            shape_supp = tuple()

            # In the scalar case, `size` corresponds to the entire result's
            # shape. This implies the following:
            #     shape_ind[-ndim_ind] == size[:ndim_ind]
            # TODO: How do we add this constraint/check symbolically?

            ndim_reps = max(size_len - ndim_ind, 0)
            shape_reps = tuple(size)[ndim_ind:]
        else:
            shape_supp = self.supp_shape_fn(self.ndim_supp,
                                            self.ndims_params,
                                            dist_params,
                                            param_shapes=param_shapes)

            ndim_reps = size_len
            shape_reps = size

        ndim_shape = self.ndim_supp + ndim_ind + ndim_reps

        if ndim_shape == 0:
            shape = tt.constant([], dtype='int64')
        else:
            shape = tuple(shape_reps) + tuple(shape_ind) + tuple(shape_supp)

        # if shape is None:
        #     raise tt.ShapeError()

        return shape

    def make_node(self, *dist_params, size=None, rng=None, name=None):
        """This will be the "constructor" called by users.

        Parameters
        ==========
        dist_params: list
            Distribution parameters.
        size: Iterable or None
            Numpy-like size of the output (i.e. replications).
        rng: RandomState or None
            Existing Theano `RandomState` object to be used.  Creates a
            new one, if `None`.

        Results
        =======
        An `Apply` node with rng state and sample tensor outputs.
        """
        if not (size is None or isinstance(size, Iterable)):
            raise ValueError('Parameter size must be None or iterable')

        dist_params = tuple(tt.as_tensor_variable(p)
                            for p in dist_params)

        dtype = tt.scal.upcast(self.dtype, *[p.dtype for p in dist_params])

        if rng is None:
            rng = theano.shared(np.random.RandomState())
        elif not isinstance(rng.type, RandomStateType):
            warn('The type of rng should be an instance of RandomStateType')

        if size is None:
            size = tt.constant([], dtype='int64')
        else:
            size = tt.as_tensor_variable(size, ndim=1)

        assert size.dtype == 'int64'

        shape = self._infer_shape(size, dist_params)

        # Let's try to do a better job than `_infer_ndim_bcast` when
        # dimension sizes are symbolic.
        bcast = []
        for s in shape:
            try:
                if isinstance(s.owner.op, tt.Subtensor) and \
                   s.owner.inputs[0].owner is not None:
                    # Handle a special case in which
                    # `tensor.get_scalar_constant_value` doesn't really work.
                    s_x, s_idx = s.owner.inputs
                    s_idx = tt.get_scalar_constant_value(s_idx)
                    if isinstance(s_x.owner.op, tt.Shape):
                        x_obj, = s_x.owner.inputs
                        s_val = x_obj.type.broadcastable[s_idx]
                    else:
                        # TODO: Could go for an existing broadcastable here, too, no?
                        s_val = False
                else:
                    s_val = tt.get_scalar_constant_value(s)
            except tt.NotScalarConstantError:
                s_val = False

            bcast += [s_val == 1]

        outtype = tt.TensorType(dtype=dtype, broadcastable=bcast)

        out_var = outtype(name=name)

        inputs = (rng, size) + dist_params
        outputs = (rng.type(), out_var)

        return theano.gof.Apply(self, inputs, outputs)

    def infer_shape(self, node, input_shapes):
        size = node.inputs[1]
        dist_params = tuple(node.inputs[2:])
        shape = self._infer_shape(size, dist_params,
                                  param_shapes=input_shapes[2:])

        return [None, [s for s in shape]]

    def perform(self, node, inputs, outputs):
        """Uses `self.rng_fn` to draw random numbers."""
        rng_out, smpl_out = outputs

        # Draw from `rng` if `self.inplace` is `True`, and from a
        # copy of `rng` otherwise.
        rng, size, args = inputs[0], inputs[1], inputs[2:]

        assert type(rng) == np.random.RandomState, (type(rng), rng)

        rng_out[0] = rng

        # The symbolic output variable corresponding to value produced here.
        out_var = node.outputs[1]

        # If `size == []`, that means no size is enforced, and NumPy is
        # trusted to draw the appropriate number of samples, NumPy uses
        # `size=None` to represent that.  Otherwise, NumPy expects a tuple.
        if np.size(size) == 0:
            size = None
        else:
            size = tuple(size)

        if not self.inplace:
            rng = copy(rng)

        smpl_val = self.rng_fn(rng, *(args + [size]))

        if (not isinstance(smpl_val, np.ndarray) or
            str(smpl_val.dtype) != out_var.type.dtype):
            smpl_val = theano._asarray(smpl_val, dtype=out_var.type.dtype)

        # When `size` is `None`, NumPy has a tendency to unexpectedly
        # return a scalar instead of a higher-dimension array containing
        # only one element. This value should be reshaped
        # TODO: Really?  Why shouldn't the output correctly correspond to
        # the returned NumPy value?  Sounds more like a mis-specification of
        # the symbolic output variable.
        if size is None and smpl_val.ndim == 0 and out_var.ndim > 0:
            smpl_val = smpl_val.reshape([1] * out_var.ndim)

        smpl_out[0] = smpl_val

    def grad(self, inputs, outputs):
        return [theano.gradient.grad_undefined(self, k, inp,
                                               'No gradient defined through raw random numbers op')
                for k, inp in enumerate(inputs)]

    def R_op(self, inputs, eval_points):
        return [None for i in eval_points]
#+END_SRC

#+HTML: <div class="example" markdown="">
Here are some examples of src_python[]{RandomVariable} in action.
#+NAME: random_variable_example
#+BEGIN_SRC python
NormalRV = RandomVariable('normal', 0, [0, 0], 'normal')
MvNormalRV = RandomVariable('multivariate_normal', 1, [1, 2], 'multivariate_normal')

print("NormalRV([0., 100.], 30, size=[4, 2]):\n{}\n".format(
    NormalRV([0., 100.], 30, size=[4, 2]).eval()))

print("MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[3, 2, 3]):\n{}".format(
    MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[3, 2, 3]).eval()))
#+END_SRC

#+RESULTS: random_variable_example
#+BEGIN_SRC python
NormalRV([0., 100.], 30, size=[4, 2]):
[[ 27.43632901  93.39441318]
 [ 15.85959218 133.70107347]
 [ -9.36097104  68.13953575]
 [ 13.41389074 102.36908493]]

MvNormalRV([0, 1e2, 2e3], np.diag([1, 1, 1]), size=[3, 2, 3]):
[[[[-1.95386446e+00  1.00395353e+02  2.00061885e+03]
   [ 1.05345467e-02  1.00488854e+02  2.00075391e+03]
   [-2.96094820e-01  9.97791115e+01  2.00039472e+03]]

  [[ 1.09725296e+00  1.00481658e+02  2.00168402e+03]
   [-5.97706814e-01  9.84007224e+01  1.99998250e+03]
   [ 6.05551705e-01  1.00905256e+02  1.99904533e+03]]]


 [[[ 3.13777265e-01  9.73407662e+01  1.99854117e+03]
   [-8.54227150e-01  1.01332597e+02  1.99945623e+03]
   [-4.71493054e-01  9.98134334e+01  2.00121111e+03]]

  [[-3.61516595e-01  9.99772061e+01  1.99853933e+03]
   [ 5.17271791e-01  9.74970520e+01  2.00052296e+03]
   [ 2.15904894e-01  9.99694385e+01  2.00021462e+03]]]


 [[[-9.61903721e-01  1.00569645e+02  1.99858826e+03]
   [-6.72526622e-01  1.00277177e+02  2.00036283e+03]
   [ 2.58608351e-01  1.01020520e+02  1.99866678e+03]]

  [[ 8.70002780e-02  1.01020608e+02  1.99975983e+03]
   [ 8.56671014e-01  9.97595247e+01  2.00094824e+03]
   [ 6.17426596e-01  1.01919972e+02  1.99914348e+03]]]]


#+END_SRC

#+HTML: </div>

As we've mentioned, there are a few difficulties surrounding the use and
determination of shape information in PyMC3.  src_python[]{RandomVariable} doesn't
suffer the same limitations.

#+HTML: <div class="example" markdown="" title-name="">
A multivariate normal random variable cannot be created without
explicit shape information.
#+BEGIN_SRC python
import traceback

test_mean = tt.vector('test_mean')
test_mean.tag.test_value = np.asarray([1])

test_cov = tt.matrix('test_cov')
test_cov.tag.test_value = np.asarray([[1]])

try:
  with pm.Model():
    test_rv = pm.MvNormal('test_rv', test_mean, test_cov)
except Exception as e:
  print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
ValueError: Invalid dimension for value: 0


#+END_SRC

#+BEGIN_SRC python
try:
  with pm.Model():
    test_rv = pm.MvNormal('test_rv', test_mean, test_cov, shape=1)
    print("test_rv.distribution.shape = {}".format(test_rv.distribution.shape))
    print("test_rv.tag.test_value = {}".format(test_rv.tag.test_value))
except Exception as e:
  print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_rv.distribution.shape = [1]
test_rv.tag.test_value = [1.]


#+END_SRC

Using src_python[]{RandomVariable}, we do not have to specify a shape, nor
implement any sampling code outside of src_python[]{RandomVariable.perform}
to draw random variables and generate valid test values.
#+BEGIN_SRC python
test_mv_rv = MvNormalRV(test_mean, test_cov)
test_mv_rv_2 = MvNormalRV(test_mv_rv, test_cov)

# Observe the automatically generated test values
print("test_mv_rv.tag.test_value = {}".format(test_mv_rv.tag.test_value))
print("test_mv_rv_2.tag.test_value = {}".format(test_mv_rv_2.tag.test_value))

# Sample some values under specific parameter values
print("test_mv_rv.eval() = {}".format(test_mv_rv.eval(
    {test_mean: [1, 2], test_cov: np.diag([1, 2])})))
print("test_mv_rv_2.eval() = {}".format(test_mv_rv_2.eval(
    {test_mean: [1, 2, 3], test_cov: np.diag([1, 2, 70])})))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_mv_rv.tag.test_value = [0.90661799]
test_mv_rv_2.tag.test_value = [1.10201953]
test_mv_rv.eval() = [-1.90184227  1.8679379 ]
test_mv_rv_2.eval() = [ 2.05659828 -1.33638943  3.85355663]


#+END_SRC
#+HTML: </div>



** Testing src_python[]{RandomVariable}                             :noexport:
In the following we implement some unit-like tests
for src_python[]{RandomVariable}.  They confirm expected sample dimensions and
broadcast properties.

#+BEGIN_SRC python
def rv_numpy_test(rv, *params, size=None):
    """Test for correspondence between `RandomVariable` and NumPy shape and
    broadcast dimensions.
    """
    test_rv = rv(*params, size=size)
    param_vals = [tt.gof.op.get_test_value(p) for p in params]
    size_val = None if size is None else tt.gof.op.get_test_value(size)
    test_val = getattr(np.random, rv.name)(*param_vals, size=size_val)
    test_shp = np.shape(test_val)

    # This might be a little too harsh, since purely symbolic `tensor.vector` inputs
    # have no broadcastable information, yet, they can take broadcastable values.
    # E.g.
    #     x_tt = tt.vector('x')
    #     x_tt.tag.test_value = np.array([5]) # non-symbolic value is broadcastable!
    #     x_tt.tag.test_value = np.array([5, 4]) # non-symbolic value is not broadcastable.
    #
    # In the end, there's really no clear way to determine this without full
    # evaluation of a symbolic node, and that mostly defeats the purpose.
    # Unfortunately, this is what PyMC3 resorts to when constructing its
    # `TensorType`s (and shapes).
    test_bcast = [s == 1 for s in test_shp]
    np.testing.assert_array_equal(test_rv.type.broadcastable, test_bcast)

    eval_args = {p: v for p, v in zip(params, param_vals)
                 if isinstance(p, tt.Variable) and not isinstance(p, tt.Constant)}
    np.testing.assert_array_equal(test_rv.shape.eval(eval_args), test_shp)
    np.testing.assert_array_equal(np.shape(test_rv.eval(eval_args)), test_shp)


tt.config.on_opt_error = 'raise'

rv_numpy_test(NormalRV, 0., 1.)
rv_numpy_test(NormalRV, 0., 1., size=[3])
# Broadcast sd over independent means...
rv_numpy_test(NormalRV, [0., 1., 2.], 1.)
rv_numpy_test(NormalRV, [0., 1., 2.], 1., size=[3, 3])
rv_numpy_test(NormalRV, [0], [1], size=[1])

rv_numpy_test(NormalRV, tt.as_tensor_variable([0]), [1], size=[1])
rv_numpy_test(NormalRV, tt.as_tensor_variable([0]), [1], size=tt.as_tensor_variable([1]))


# XXX: Shouldn't work due to broadcastable comments in `rv_numpy_test`.
# test_mean = tt.vector('test_mean')
# test_mean.tag.test_value = np.r_[1]
# rv_numpy_test(NormalRV, test_mean, [1], size=tt.as_tensor_variable([1]))

# with pm.Model():
#     test_rv = pm.MvNormal('test_rv', [0], np.diag([1]), shape=1)
#
# test_rv.broadcastable

rv_numpy_test(MvNormalRV, [0], np.diag([1]))
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4, 1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[4, 1, 1])
rv_numpy_test(MvNormalRV, [0], np.diag([1]), size=[1, 5, 8])
rv_numpy_test(MvNormalRV, [0, 1, 2], np.diag([1, 1, 1]))
# Broadcast cov matrix across independent means?
# Looks like NumPy doesn't support that (and are probably better off for it).
# rv_numpy_test(MvNormalRV, [[0, 1, 2], [4, 5, 6]], np.diag([1, 1, 1]))
#+END_SRC

* A Problem with PyMC3 Broadcast Dimensions
As in [[cite:WillardSymbolicMathPyMC32018]], we can create mappings between
existing PyMC3 random variables and their new src_python[]{RandomVariable}
equivalents.

#+NAME: pymc_theano_rv_equivs
#+BEGIN_SRC python
pymc_theano_rv_equivs = {
    pm.Normal:
    lambda dist, rand_state:
    (None,
     # PyMC3 shapes aren't NumPy-like size parameters, so we attempt to
     # adjust for that.
     NormalRV(dist.mu, dist.sd, size=dist.shape[1:], rng=rand_state)),
    pm.MvNormal:
    lambda dist, rand_state:
    (None, NormalRV(dist.mu, dist.cov, size=dist.shape[1:], rng=rand_state)),
}
#+END_SRC

However, if we attempt the same PymC3 graph conversion approach as before (i.e. convert a
PyMC3 model to a Theano src_python[]{FunctionGraph}
using src_python[]{model_graph}, then replace PyMC3 random variable
nodes with our new random variable types
using src_python[]{create_theano_rvs}), we're likely to run into a
problem involving mismatching broadcastable dimensions.

The problem arises because *PyMC3 "knows" more broadcast information than it
should*, since it uses the Theano variables' test values in order to obtain
concrete shapes for the random variables it creates.  Using concrete,
non-symbolic shapes, it can exactly determine what would otherwise be ambiguous
[[http://deeplearning.net/software/theano/library/tensor/basic.html?highlight=broadcastable#theano.tensor.TensorType.broadcastable][broadcastable dimensions]] at the symbolic level.

More specifically, broadcast information is required during the construction of a
Theano src_python[]{TensorType}, so PyMC3 random variable types can be
inconsistent (unnecessarily restrictive, really) causing Theano to complain when
we try to construct a src_python[]{FunctionGraph}.

#+HTML: <div class="example" markdown="">
Consider the following example; it constructs two purely symbolic
Theano vectors: one with broadcasting and one without.
#+ATTR_LATEX: :float t :placement h
#+BEGIN_SRC python
y_tt = tt.row('y')
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))
x_tt = tt.matrix('x')
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
y_tt.broadcastable = (True, False)
x_tt.broadcastable = (False, False)


#+END_SRC
Notice that it--by default--signifies no broadcasting on its first and only
dimension.

If we wish--or if [[http://deeplearning.net/software/theano/library/config.html#config.compute_test_value][Theano's configuration demands]] it--we can assign the
symbolic vector arbitrary test values, as long as they're consistent with its
type (i.e. a vector, or 1-dimensional array).

In the following, we assign both a broadcastable (i.e. first--and only--dimension has
size 1) and non-broadcastable test value.

Test value is broadcastable:
#+BEGIN_SRC python
x_tt.tag.test_value = np.array([[5]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(x_tt.tag.test_value).broadcastable))
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))

# Compute this to run internal checks
try:
    x_tt.shape
    print("shape checks out!")
except TypeError as e:
    print(str(e))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_value.broadcastable = (True, True)
x_tt.broadcastable = (False, False)
shape checks out!


#+END_SRC

#+BEGIN_SRC python
y_tt.tag.test_value = np.array([[5]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(y_tt.tag.test_value).broadcastable))
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))

# Compute this to run internal checks
try:
    y_tt.shape
    print("shape checks out!")
except TypeError as e:
    print(str(e))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_value.broadcastable = (True, True)
y_tt.broadcastable = (True, False)
shape checks out!


#+END_SRC


Test value is *not* broadcastable:
#+BEGIN_SRC python
x_tt.tag.test_value = np.array([[5, 4]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(x_tt.tag.test_value).broadcastable))
print("x_tt.broadcastable = {}".format(x_tt.broadcastable))

# Compute this to run internal checks
try:
    x_tt.shape
    print("shape checks out!")
except TypeError as e:
    print(str(e))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_value.broadcastable = (True, False)
x_tt.broadcastable = (False, False)
shape checks out!


#+END_SRC

#+BEGIN_SRC python
y_tt.tag.test_value = np.array([[5, 4], [3, 2]])
print("test_value.broadcastable = {}".format(
    tt.as_tensor_variable(y_tt.tag.test_value).broadcastable))
print("y_tt.broadcastable = {}".format(y_tt.broadcastable))

# Compute this to run internal checks
try:
    y_tt.shape
    print("shape checks out!")
except TypeError as e:
    print(str(e))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
test_value.broadcastable = (False, False)
y_tt.broadcastable = (True, False)
For compute_test_value, one input test value does not have the requested type.

Backtrace when that variable is created:

  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py", line 485, in mainloop
    self.interact()
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py", line 476, in interact
    self.run_cell(code, store_history=True)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2909, in run_ast_nodes
    if self.run_code(code, result):
  File "/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-163-7eec6ac09cb0>", line 1, in <module>
    __org_babel_python_fname = '/tmp/user/1000/babel-f5w2XO/python-Cjuz3L'; __org_babel_python_fh = open(__org_babel_python_fname); exec(compile(__org_babel_python_fh.read(), __org_babel_python_fname, 'exec')); __org_babel_python_fh.close()
  File "/tmp/user/1000/babel-f5w2XO/python-Cjuz3L", line 1, in <module>
    y_tt = tt.row('y')

The error when converting the test value to that variable type:
Non-unit value on shape on a broadcastable dimension.
(2, 2)
(True, False)


#+END_SRC

Simply put: non-broadcastable Theano tensor variable types can take
broadcastable and non-broadcastable values, while broadcastable types can only
take broadcastable values.

#+HTML: </div>

What we can take from the example above is that if we determine that a vector
has broadcastable dimensions using test values--as PyMC3 does--we unnecessarily
introduce restrictions and potential inconsistencies down the line.
One point of origin for such issues is *shared variables*.

* Optimizations Using src_python[]{RandomVariable}

With our new src_python[]{RandomVariable}, we can alter the replacement patterns
used by src_python[]{tt.gof.opt.PatternSub} in
[[cite:WillardSymbolicMathPyMC32018]] and implement a slightly better parameter
lifting for affine transforms of scalar normal random variables.

#+NAME: rv_optimizations
#+BEGIN_SRC python
# Create random variable constructors.
NormalRV = RandomVariable('normal', 0, [0, 0], 'normal')
MvNormalRV = RandomVariable('multivariate_normal', 1, [1, 2], 'multivariate_normal')

# We use the following to handle keyword arguments.
construct_rv = lambda rng, size, mu, sd: NormalRV(mu, sd, size=size, rng=rng)

norm_lift_pats = [
    # Lift element-wise multiplication
    tt.gof.opt.PatternSub(
        (tt.mul,
         'a_x',
         (NormalRV, 'rs_x', 'size_x', 'mu_x', 'sd_x')),
        (construct_rv,
         'rs_x',
         # XXX: Is this really consistent?  How will it handle broadcasting?
         'size_x',
         (tt.mul, 'a_x', 'mu_x'),
         (tt.mul, 'a_x', 'sd_x'),
        )),
    # Lift element-wise addition
    tt.gof.opt.PatternSub(
        (tt.add,
         (NormalRV, 'rs_x', 'size_x', 'mu_x', 'sd_x'),
         'b_x'),
        (construct_rv,
         'rs_x',
         # XXX: Is this really consistent?  How will it handle broadcasting?
         'size_x',
         (tt.add, 'mu_x', 'b_x'),
         'sd_x',
        )),
]

norm_lift_opts = tt.gof.opt.EquilibriumOptimizer(
    norm_lift_pats, max_use_ratio=10)
#+END_SRC

#+HTML: <div class="example" markdown="">

#+ATTR_LATEX: :float nil
#+CAPTION: Scaled normal random variable example using src_python[]{RandomVariable}.
#+NAME: mat_mul_scaling_rv_exa
#+BEGIN_SRC python
from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv

mu_X = tt.vector('mu_X')
sd_X = tt.vector('sd_X')

mu_X.tag.test_value = np.array([0], dtype=tt.config.floatX)
sd_X.tag.test_value = np.array([1, 2], dtype=tt.config.floatX)

# TODO: Defining the offset, `b_tt`, using `tt.vector` will err-out because of
# non-matching dimensions and no default broadcasting.
# This is another good reason for broadcasting inputs in `make_node`.
# E.g.
# b_tt = tt.vector('b')
# b_tt.tag.test_value = np.array([1, 2], dtype=tt.config.floatX)
# or
# b_tt.tag.test_value = np.array([1], dtype=tt.config.floatX)

b_tt = tt.as_tensor_variable([5.], name='b')

X_rv = NormalRV(mu_X, sd_X, name='~X_rv')
Z_rv = 5 * X_rv + b_tt

Z_graph = FunctionGraph(tt_inputs([Z_rv]), [Z_rv])

Z_graph_opt = Z_graph.clone()

_ = norm_lift_opts.optimize(Z_graph_opt)

print('Before: {}'.format(tt.pprint(Z_graph.outputs[0])))
print('After: {}'.format(tt.pprint(Z_graph_opt.outputs[0])))
#+END_SRC

#+RESULTS: mat_mul_scaling_rv_exa
#+BEGIN_SRC text
Before: ((TensorConstant{5} * normal_rv(<RandomStateType>, TensorConstant{[]}, mu_X, sd_X)) + TensorConstant{(1,) of 5.0})
After: normal_rv(<RandomStateType>, TensorConstant{[]}, ((TensorConstant{5} * mu_X) + TensorConstant{(1,) of 5.0}), (TensorConstant{5} * sd_X))


#+END_SRC

#+HTML: </div>

:TODO:
- What about division and subtraction?  These can be addressed using
canonicalization?
:END:

Now, what if we wanted to handle affine transformations of a multivariate normal
random variable?  Specifically, consider implementing the following:
\begin{equation*}
  X \sim N\left(\mu, \Sigma \right), \quad
  A X \sim N\left(A \mu, A \Sigma A^\top \right)
 \;.
\end{equation*}

At first, the following substitution pattern might seem reasonable:
#+ATTR_LATEX: :float t :placement h
#+BEGIN_SRC python
# Vector multiplication
tt.gof.opt.PatternSub(
    (tt.dot,
     'A_x',
     (MvNormalRV, 'rs_x', 'size_x', 'mu_x', 'cov_x')),
    (construct_rv,
     MvNormalRV,
     'rs_x',
     'size_x',
     (tt.dot, 'A_x', 'mu_x'),
     (tt.dot,
      (tt.dot, 'A_x', 'cov_x')
      (tt.transpose, 'A_x')),
    ))
#+END_SRC

Unfortunately, the combination of size parameter and broadcasting complicates
the scenario.  Both parameters indirectly affect the distribution parameters,
making the un-lifted dot-product consistent, but not necessarily the lifted products.

The following example demonstrates the lifting issues brought on by
broadcasting.

#+HTML: <div class="example" markdown="">
First, we create a simple multivariate normal.
#+ATTR_LATEX: :float t :placement h
#+BEGIN_SRC python
mu_X = [0, 10]
cov_X = np.diag([1, 1e-2])
size_X_rv = [2, 3]
X_rv = MvNormalRV(mu_X, cov_X, size=size_X_rv)

print('X_rv sample:\n{}\n'.format(X_rv.tag.test_value))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
X_rv sample:
[[[-0.49226543  9.98771301]
  [-0.22713441 10.00124952]
  [ 1.21604812 10.0386737 ]]

 [[ 1.61758857  9.98456418]
  [ 1.26945358  9.9205853 ]
  [ 1.25295917 10.09953858]]]


#+END_SRC

Next, we create a simple matrix operator to apply to the multivariate normal.
#+BEGIN_SRC python
A_tt = tt.as_tensor_variable([[2, 5, 8], [3, 4, 9]])
# or A_tt = tt.as_tensor_variable([[2, 5, 8]])

# It's really just `mu_X`...
E_X_rv = X_rv.owner.inputs[2]

print('A * X_rv =\n{}\n'.format(tt.dot(A_tt, X_rv).tag.test_value))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
A * X_rv =
[[[  7.60818207 150.2910632 ]
  [ 19.60611837 150.36836345]]

 [[  8.55909917 160.31620039]
  [ 21.20721252 160.53188091]]]


#+END_SRC

As we can see, the multivariate normal's test/sampled value has the correct
shape for our matrix operator.

#+BEGIN_SRC python
import traceback
try:
    print('A * E[X_rv] =\n{}\n'.format(tt.dot(A_tt, E_X_rv).tag.test_value))
except ValueError as e:
    print("".join(traceback.format_exception_only(type(e), e)))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
ValueError: shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)


#+END_SRC

However, we see that the multivariate normal's inputs (i.e. the src_python[]{Op}
inputs)--specifically the mean parameter--do not directly reflect the support's
shape, as intuitively would suggest.

#+BEGIN_SRC python
size_tile = tuple(size_X_rv) + (1,)
E_X_rv_ = tt.tile(E_X_rv, size_tile, X_rv.ndim)

print('A * E[X_rv] =\n{}\n'.format(tt.dot(A_tt, E_X_rv_).tag.test_value))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC python
A * E[X_rv] =
[[[  0 150]
  [  0 150]]

 [[  0 160]
  [  0 160]]]


#+END_SRC

We can manually replicate the inputs so that they match the output shape, but
a solution to the general problem requires a more organized response.

#+HTML: </div>

:TESTING:
#+BEGIN_SRC python
def replicate_expr(param, rep_size, dist_op, param_idx):
    return tt.tile(param, tuple(rep_size) + (1,), dist_op.ndims_params[param_idx] + len(rep_size))

mvnorm_lift_pats = tt.gof.opt.PatternSub(
    (tt.dot,
     'A_x',
     (MvNormalRV, 'rs_x', 'size_x', 'mu_x', 'cov_x')),
    (construct_rv,
     MvNormalRV,
     'rs_x',
     'size_x',
     (tt.dot, 'A_x',
      (replicate_expr, 'mu_x', 'size_x', MvNormalRV, 0)),
     (tt.dot,
      (tt.dot, 'A_x',
       (replicate_expr, 'cov_x', 'size_x', MvNormalRV, 1)),
      (tt.transpose, 'A_x'))),
)

mvnorm_lift_opts = tt.gof.opt.EquilibriumOptimizer(
    [ mvnorm_lift_pats ], max_use_ratio=10)

mu_X = [0, 10]
cov_X = np.diag([1, 1e-2])
X_rv = MvNormalRV(mu_X, cov_X, size=[2, 3])

A_tt = tt.as_tensor_variable([[2, 5, 8], [3, 4, 9]])

Z_rv = tt.dot(A_tt, X_rv)

Z_graph = FunctionGraph(tt_inputs([Z_rv]), [Z_rv])

tt.printing.debugprint(Z_graph)

Z_graph_opt = Z_graph.clone()

# This won't work because `tt.dot` uses `tensor_dot`, which does some
# reshaping and dim-shuffling.
# We need to account for those, as well.
# TODO: We need to implement something like `local_dimshuffle_lift` for some RVs.
_ = mvnorm_lift_opts.optimize(Z_graph_opt)

tt.printing.debugprint(Z_graph_opt)
#+END_SRC
:END:

* Discussion

In a follow-up, we'll address a few loose ends, such as
- the inclusion of density functions and likelihoods,
- decompositions/reductions of overlapping multivariate types
  (e.g. transforms between tensors of univariate normals and equivalent
  multivariate normals),
- canonicalization of graphs containing src_python{RandomVariable} terms,
- and optimizations that specifically benefit MCMC schemes (e.g. automatic conversion to scale
  mixture decompositions that improve sampling/covariance structure).

:TODO:
Talk about src_python[]{supp_shape_fn}.
:END:

#+BIBLIOGRAPHY: ../tex/symbolic-pymc3.bib
#+BIBLIOGRAPHYSTYLE: plainnat
