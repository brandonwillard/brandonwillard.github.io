#+TITLE: Graph Manipulation and MCMC
#+AUTHOR: Brandon T. Willard
#+DATE: 2019-01-15
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+BEGIN_SRC elisp :eval t :exports none :results none
(org-babel-load-file "org-setup.org")
(org-babel-lob-ingest "org-babel-extensions.org")
#+END_SRC

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session symbolic-math-pymc3-mcmc

#+NAME: set-pelican-preamble
#+BEGIN_SRC elisp :eval export-only :exports results :results value raw
(org-pelican-create-yaml)
#+END_SRC

#+RESULTS: set-pelican-preamble
#+BEGIN_EXPORT html
---
bibliography:
- 'tex/symbolic-pymc3.bib'
modified: '2019-1-28'
tags: 'pymc3,theano,statistics,symbolic computation,python,probability theory'
title: Graph Manipulation and MCMC
date: '2019-01-15'
author: 'Brandon T. Willard'
figure_dir: '{attach}/articles/figures/'
figure_ext: png
---
#+END_EXPORT

#+BEGIN_abstract
Continuing from [[citet:WillardRandomVariablesTheano2018]], ...
#+END_abstract

* Introduction

With a set of distributions defined more completely within a graph, we can much
more easily produce MCMC samplers that use distribution-level domain knowledge.

In the examples we'll use here, the model of interest will be the Horseshoe
model [[citep:carvalho_horseshoe_2010]] given by
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \mathop{\text{N}}\nolimits\left(0, \tau^2\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:hs_model}
\end{equation}

The Horseshoe prior quickly decays, so many generic sampling methods tend
produce poor estimates for models using it.  However, under different parameter
expansions, the prior can be sampled more efficiently.  Those expansions
usually depend domain knowledge in the areas of probability theory and the
integral calculus that is not easily accessible to primarily derivative-based
approaches.

As well, these reformulations are often very simplistic when formulated in terms
of random variables, and that simplicity can translate to simpler
implementations and lower computational costs.

#+NAME: theano-random-function-load
#+BEGIN_SRC python :exports none :results none :var src=(org-babel-eval-read-file "theano-random-variable.py")
exec(src)
#+END_SRC

#+NAME: mcmc-requirements
#+BEGIN_SRC python :exports none :results none :noweb strip-export
# <<theano-random-function-load()>>

from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv

theano.config.compute_test_value = 'ignore'
#+END_SRC

* Understanding the Problem(s)

To start, we need to identify sub-graphs that match our considered
reformulations, reason about them, and--when appropriate--replace them.

For the first step, which involves searching a graph for specific Theano
expressions, we'll consider two common approaches:
- directly--by walking "walking" Python functions through a graph--and
- pattern-matching--via some form of domain-specific language.

Using a direct approach, it can be very straight-forward to implement simple
search-and-replace objectives.  However, for modeling non-trivial systems
of logic and mathematical frameworks--like probability theory, different
algebras, and properties of function spaces--a direct approach will quickly
become unscalable, error-prone, and impossibly difficult to maintain.

The work we intend to do makes implicit use of nominal logic, field axioms,
and aspects of the typed lambda calculus--among other things.

Let's illustrate some of these points using an example.

:EXAMPLE:
We demonstrate automatic model reformulation using the variance expansion in
\eqref{eq:hs_model} in [[citet:scott_parameter_2010]].

This expansion works pushing the half-Cauchy variance term, $\tau$, out of the
normal, $\beta$:
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \tau \cdot \mathop{\text{N}}\nolimits\left(0, 1\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:norm_var_sink}
\end{equation}

An applicable reformulation rule might look like the following:
\begin{equation}
  \begin{aligned}
    \mathop{\text{N}}\nolimits\left(a m, a^2 C\right)
    &\to a \mathop{\text{N}}\nolimits\left(m, C\right)
  \end{aligned}
\label{eq:norm_replacement_exa}
\;.
\end{equation}
:END:

Now, what about an expression like $\mathop{\text{N}}\nolimits\left(0,
a^2\right)$; should our replacement in \eqref{eq:norm_replacement_exa} produce
$a \mathop{\text{N}}\nolimits\left(0, 1\right)$?

Conventional mathematical reasoning might say so, but a term rewriting system
will need to be designed to account for this explicitly.  The important
questions have more to do with *where* and *how*: by requiring the addition of all
necessary replacements, or by automatic reasoning using a relatively small set of
axioms from which they can be derived?

For instance, we could provide some rules that represent group/ring/field axioms
(e.g. identity and zero elements) and allow such a system to consider them in
tandem.

For ease-of-use--especially with respect to developers working at the
mathematics-level--it's more desirable to formulate rules at a high level
(e.g. set and measure theory) and use a system with a generalized means
of verifying and deriving these expectations.
Even for general scalability, testing, and development, it can be much better to
focus on well compartmentalized pieces of such a system that have direct
mappings to well understood subjects.

While such solutions are possible, they're not trivial to implement *correctly*.
It shouldn't be surprising that there are very deep bodies of research on such
systems--usually under the subjects *term rewriting* and *symbolic computation*.
A good part of classical AI focused on the challenges induced by these automation
objectives and their implementations.

This doesn't mean--however--that they're prohibitively difficult to use or develop.
As demonstrated in [[citet:WillardRandomVariablesTheano2018]], Theano provides
some pattern-based graph manipulation using a form of unification.  This functionality
shares some of the same fundamental abstraction(s) as the more sophisticated systems
alluded to earlier, but it starts to fall short right where our objectives get started.

In the following, we'll demonstrate the critical short-comings, and introduce some
steps further into the direction of modern unification and term rewriting.
# [[citet:ByrdRelationalProgrammingminiKanren2009]] [[citet:RocklinlogpyLogicProgramming2018]]
# [[citet:WillardRoleSymbolicComputation2017]]
* A Language for Graph Manipulation
In this case, graph manipulation mostly consists of term rewriting in the
context of a Theano graphs, and--as we've stated earlier--the term rewriting is
mostly driven by algebraic considerations.

The mechanical aspects of this work is largely generalizable in terms of
orchestrated unification.  In the lead-up articles
[[citet:WillardRandomVariablesTheano2018]] and
[[citet:WillardSymbolicMathPyMC32018]], we used src_python{PatternSub} {{{results(NameError: name 'PatternSub' is not defined)}}}, which
uses unification (and reification) to implement pattern matching and
substitution (i.e. rewrite rules).

:REMARK:
In the context of Python, this sort of work has some fundamental limitations and
unnecessarily confusing aspects.  Python doesn't lend itself to symbolic manipulation,
making things like expression manipulation and traversal particularly onerous.
:END:

Beyond some small technical issues, src_python{PatternSub} {{{results(NameError: name 'PatternSub' is not defined)}}} only provides a
limited form of unification, and doesn't provide a programmable context for
controlling exactly how and when the unification is performed.

:EXAMPLE:
Let's attempt to implement the replacement in \eqref{eq:norm_var_sink}
using src_python{PatternSub} {{{results(NameError: name 'PatternSub' is not defined)}}}.

#+ATTR_LATEX: :float nil
#+CAPTION: A naively specified Horseshoe model.
#+NAME: hs-model
#+BEGIN_SRC python :exports none :results silent
tau_rv = CauchyRV(0, 1, name='\\tau')
beta_stddev = tt.abs_(tau_rv)
beta_rv = NormalRV(0, beta_stddev, name='\\beta')
Y_rv = NormalRV(beta_rv, 1, name='Y')
#+END_SRC

#+NAME: hs-var-expansion-opt-setup
#+BEGIN_SRC python :exports none :results silent :noweb strip-export
<<mcmc-requirements>>
#+END_SRC

#+NAME: hs_var_expansion_opt
#+BEGIN_SRC python :results output :noweb yes :wrap "SRC python :eval never"
<<hs-model>>

construct_norm_rv = lambda rng, size, mu, sd: NormalRV(mu, sd, size=size, rng=rng)

norm_sink_pats = [
    # N(0, a^2) -> a N(0, 1)
    tt.gof.opt.PatternSub(
        (NormalRV, 'rng_x', 'size_x',
         'b_x', 'a_x'),
        (tt.mul, 'a_x',
         (construct_norm_rv, 'rng_x', 'size_x',
          'b_x', tt.constant(1.0)))),
]

norm_sink_opts = tt.gof.opt.EquilibriumOptimizer(
    norm_sink_pats, max_use_ratio=10)
hs_Y_graph = FunctionGraph(tt_inputs([Y_rv]), [Y_rv])
hs_Y_graph_opt = hs_Y_graph.clone()

_ = norm_sink_opts.optimize(hs_Y_graph_opt)
#+END_SRC

We see in [[hs_var_expansion_opt]] that moving from a node that produces two outputs
(i.e. a src_python{RandomVariable} {{{results()}}} outputs the symbolic RNG *and* a tensor from
the sample space) to one that produces only a single output (i.e. a product)
result in an error.  A work-around for this doesn't seem possible, and a fix for
src_python{PatternSub.transform} {{{results(NameError: name 'PatternSub' is not defined)}}} is most likely necessary.

#+RESULTS: hs_var_expansion_opt
#+begin_SRC python :eval never
/tmp/user/1000/babel-am345c/python-Q2wVRq in <module>()
     21 hs_Y_graph_opt = hs_Y_graph.clone()
     22
---> 23 _ = norm_sink_opts.optimize(hs_Y_graph_opt)

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in optimize(self, fgraph, *args, **kwargs)
     95             orig = theano.tensor.basic.constant.enable
     96             theano.tensor.basic.constant.enable = False
---> 97             ret = self.apply(fgraph, *args, **kwargs)
     98         finally:
     99             theano.tensor.basic.constant.enable = orig

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in apply(self, fgraph, start_from)
   2511                         nb = change_tracker.nb_imported
   2512                         t_opt = time.time()
-> 2513                         lopt_change = self.process_node(fgraph, node, lopt)
   2514                         time_opts[lopt] += time.time() - t_opt
   2515                         if not lopt_change:

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in process_node(self, fgraph, node, lopt)
   2032         lopt = lopt or self.local_opt
   2033         try:
-> 2034             replacements = lopt.transform(node)
   2035         except Exception as e:
   2036             if self.failure_callback is not None:

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in transform(self, node, get_nodes)
   1796                     return pattern.clone()
   1797             p = self.out_pattern
-> 1798             ret = build(p, u)
   1799             if self.values_eq_approx:
   1800                 ret.tag.values_eq_approx = self.values_eq_approx

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in build(pattern, u)
   1787             def build(pattern, u):
   1788                 if isinstance(pattern, (list, tuple)):
-> 1789                     args = [build(p, u) for p in pattern[1:]]
   1790                     return pattern[0](*args)
   1791                 elif isinstance(pattern, string_types):

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in <listcomp>(.0)
   1787             def build(pattern, u):
   1788                 if isinstance(pattern, (list, tuple)):
-> 1789                     args = [build(p, u) for p in pattern[1:]]
   1790                     return pattern[0](*args)
   1791                 elif isinstance(pattern, string_types):

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in build(pattern, u)
   1788                 if isinstance(pattern, (list, tuple)):
   1789                     args = [build(p, u) for p in pattern[1:]]
-> 1790                     return pattern[0](*args)
   1791                 elif isinstance(pattern, string_types):
   1792                     return u[unify.Var(pattern)]

/tmp/user/1000/babel-am345c/python-Q2wVRq in <lambda>(rng, size, mu, sd)
      4 Y_rv = NormalRV(beta_rv, 1, name='Y')
      5
----> 6 construct_norm_rv = lambda rng, size, mu, sd: NormalRV(mu, sd, size=size, rng=rng)
      7
      8 norm_sink_pats = [

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/op.py in __call__(self, *inputs, **kwargs)
    672                 thunk.outputs = [storage_map[v] for v in node.outputs]
    673
--> 674                 required = thunk()
    675                 assert not required  # We provided all inputs
    676

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/op.py in rval(p, i, o, n)
    890             # default arguments are stored in the closure of `rval`
    891             def rval(p=p, i=node_input_storage, o=node_output_storage, n=node):
--> 892                 r = p(n, [x[0] for x in i], o)
    893                 for o in node.outputs:
    894                     compute_map[o][0] = True

<string> in perform(self, node, inputs, outputs)

AssertionError: (<class 'numpy.ndarray'>, array(0.29952447))


#+end_SRC
:END:

The [[citet:miniKanrenorg]] domain-specific language (DSL) provides an abstract
platform upon which all the capabilities we seek are provided.  While most
implementations are expressed seamlessly in a Lisp-like language, there are some
for Python.  In particular, we will use src_python{kanren} {{{results(' from '/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/kanren/__init__.py'>)}}}
[[citep:RocklinlogpyLogicProgramming2018]] .

To get started, we'll create a new src_python{theano.gof.opt.LocalOptimizer} {{{results()}}} that uses
src_python{kanren} {{{results(' from '/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/kanren/__init__.py'>)}}} in a limited capacity (e.g. only for implementing rewrite rules).

* A miniKanren Theano Optimizer
:PROPERTIES:
# :header-args: :noweb-ref theano-minikanren-opt
:END:

As in the example Hy compiler from
[[citet:WillardReadableStringsRelational2018a]], we need to specify how
unification occurs in the context of Theano objects.

Throughout, we'll make extensive use of multiple-dispatch (implemented by the
Python library src_python{multipledispatch} {{{results(NameError: name 'multipledispatch' is not defined)}}}
[[citep:RocklinMultipledispatchContribute2019]]).

#+NAME: minikanren-opt-imports
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
from collections import Callable
from warnings import warn

import numpy as np

import theano
import theano.tensor as tt

from theano.printing import debugprint as tt_dprint

from kanren import var, run, eq, conde, lall, fact, Relation, isvar
from kanren.core import success, fail

from kanren.term import term, operator, arguments
from kanren.assoccomm import eq_assoccomm, eq_assoc, eq_comm
from kanren.assoccomm import commutative, associative

from unification import variables
from unification.core import unify, reify, _unify, _reify
from unification.more import unify_object

from theano.tensor import Elemwise
from theano.scalar.basic import mul, add

from multipledispatch import dispatch
#+END_SRC

** Theano Graph Unification and Reification

In the following, we implement unification using src_python{unification} {{{results(NameError: name 'unification' is not defined)}}}--itself
using src_python{multipledispatch} {{{results(NameError: name 'multipledispatch' is not defined)}}}.

We start by defining meta objects that wrap the existing Theano graph objects.
Using these meta objects, we can create graphs containing partially constructed
objects--or logic variables--as well as define our own graph orderings and
normal/canonical forms.

#+NAME: theano-meta-objects
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import abc

# TODO: Replace `from_obj` with a dispatched function?
# from multipledispatch import dispatch


def meta_reify_all(rands):
    # We want as many of the rands reified as possible,
    any_unreified = False
    reified_rands = []
    for s in rands:
        if isinstance(s, MetaSymbol):
            rrand = s.reify()
            reified_rands += [rrand]
            any_unreified |= isinstance(rrand, MetaSymbol)
            any_unreified |= isvar(rrand)
        elif MetaSymbol.is_meta(s):
            reified_rands += [s]
            any_unreified |= True
        else:
            reified_rands += [s]

    return reified_rands, any_unreified


class MetaSymbol(abc.ABC):
    """Meta objects for unification and such.
    """
    # TODO: Consider automatically registering base types.
    # Might need to make this a `type`.
    #
    # def __new__(cls, base, **kwargs):
    #     cls.register(base)
    #     res = object.__new__(cls)
    @property
    @abc.abstractmethod
    def base(self):
        """The base type/rator for this meta object."""
        pass

    @classmethod
    def is_meta(cls, obj):
        return isinstance(obj, MetaSymbol) or isvar(obj)

    @classmethod
    def from_obj(cls, obj):
        """Create a meta object for a given base object.

        XXX: Be careful when overriding this: `isvar` checks are necessary!
        """
        if cls.is_meta(obj) or obj is None:
            return obj

        if isinstance(obj, (list, tuple)):
            return type(obj)([cls.from_obj(o) for o in obj])

        try:
            obj_cls = next(oo for oo in cls.__subclasses__()
                           if isinstance(obj, oo.base))
        except StopIteration:
            if hasattr(cls, 'base') and isinstance(obj, cls.base):
                res = cls(*[getattr(obj, s)
                            for s in getattr(cls, '__slots__', [])],
                          obj=obj)
            else:
                raise ValueError(
                    'Could not find a MetaSymbol class for {}'.format(obj))
        else:
            res = obj_cls.from_obj(obj)

        return res

    def __init__(self, obj=None):
        self.obj = obj

    def rators(self):
        """Create a tuple of the meta object's operator parameters (i.e. "rators").
        """
        return tuple(getattr(self, s)
                     for s in getattr(self, '__slots__', []))

    def reify(self):
        """Create a concrete base object from this meta object (and its
        rators).
        """
        if self.obj is not None:
            return self.obj
        else:
            reified_rands, any_unreified = meta_reify_all(self.rators())

            # If not all the rators reified, then create another meta
            # object--albeit one with potentially more non-`None` `obj` fields.
            rator = self.base if not any_unreified else type(self)
            res = rator(*reified_rands)

            if not any_unreified:
                self.obj = res

            return res

    def __eq__(self, other):
        """Syntactic equality between meta objects and their bases.
        """
        res = False
        if ((type(self) == type(other) and
             self.base == other.base) or
                # Compare against base objects, as well
                self.base == type(other)):
            if hasattr(self, '__slots__') and self.__slots__:
                # Are all the object rators equal?
                res = all(getattr(self, attr) == getattr(other, attr)
                          for attr in self.__slots__)
            # TODO: Do we want these?  They're a bit limiting, since
            # reified objects can construct their `obj`s and those
            # won't be equal to other--potentially equivalent--base
            # objects.
            # elif self.base == type(other) and hasattr(self, 'obj'):
            #     # Is our associated concrete object equal to the base object?
            #     res = self.obj == other
            elif hasattr(self, 'obj') and hasattr(other, 'obj'):
                res = self.obj == other.obj
            else:
                # Are the objects identical?
                res = self is other
        return res

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        def _make_hashable(x):
            if isinstance(x, list):
                return tuple(x)
            elif isinstance(x, np.ndarray):
                return x.data.tobytes()
            else:
                return x
        return hash(tuple(_make_hashable(p) for p in self.rators()))

    def __str__(self):
        if self.obj is None:
            params = self.rators()
            args = ', '.join([str(p) for p in params])
            res = '{}({})'.format(self.__class__.__name__, args)
        else:
            res = str(self.obj)
        return res

    def __repr__(self):
        args = ', '.join([repr(p) for p in self.rators()] +
                         ['obj={}'.format(repr(self.obj))])
        return '{}({})'.format(
            self.__class__.__name__, args)


class MetaType(MetaSymbol):
    base = theano.Type


class MetaRandomStateType(MetaType):
    base = tt.raw_random.RandomStateType


class MetaTensorType(MetaType):
    base = tt.TensorType
    __slots__ = ['dtype', 'broadcastable', 'name']

    def __init__(self, dtype, broadcastable, name, obj=None):
        super().__init__(obj=obj)
        self.dtype = dtype
        self.broadcastable = broadcastable
        self.name = name


class MetaOp(MetaSymbol):
    base = tt.Op

    def __call__(self, *args):
        res_apply = MetaApply(self, args)
        tt_apply = res_apply.reify()
        if not self.is_meta(tt_apply):
            return MetaVariable.from_obj(tt_apply.default_output())
        res_var = MetaVariable(var(), tt_apply, var(), var())
        return res_var


class MetaApply(MetaSymbol):
    base = tt.Apply
    __slots__ = ['op', 'inputs']

    def __init__(self, op, inputs, outputs=None, obj=None):
        super().__init__(obj=obj)
        self.op = MetaOp.from_obj(op)
        self.inputs = tuple(MetaSymbol.from_obj(i) for i in inputs)
        self.outputs = outputs

    def reify(self):
        if getattr(self, 'obj', None):
            return self.obj
        else:
            tt_op = self.op.reify()
            if not self.is_meta(tt_op):
                reified_rands, any_unreified = meta_reify_all(self.inputs)
                if not any_unreified:
                    tt_var = tt_op(*reified_rands)
                    self.obj = tt_var.owner
                    return tt_var.owner
            return self


class MetaVariable(MetaSymbol):
    base = theano.Variable
    __slots__ = ['type', 'owner', 'index', 'name']

    def __init__(self, type, owner, index, name, obj=None):
        super().__init__(obj=obj)
        self.type = MetaType.from_obj(type)
        self.owner = MetaApply.from_obj(owner)
        self.index = index
        self.name = name

    def reify(self):
        # Having an `owner` causes issues (e.g. other, unrelated outputs).
        # This function lets the owning `Apply` (well, it's `Op`, really)
        # create the base object.
        if getattr(self, 'obj', None):
            return self.obj
        elif getattr(self, 'owner', None):
            tt_apply = self.owner.reify()
            if not self.is_meta(tt_apply):
                # If `tt_apply.nout == 1`, then `self.index` shouldn't matter.
                tt_index = 0 if tt_apply.nout == 1 else self.index
                if self.is_meta(tt_index):
                    return self
                tt_var = tt_apply.outputs[tt_index]
                self.obj = tt_var
                return tt_var
        return super().reify()


class MetaTensorVariable(MetaVariable):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.TensorVariable


class MetaConstant(MetaVariable):
    base = theano.Constant
    __slots__ = ['type', 'data']

    def __init__(self, type, data, name=None, obj=None):
        super().__init__(type, None, None, name, obj=obj)
        self.data = data


class MetaTensorConstant(MetaConstant):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.TensorConstant
    __slots__ = ['type', 'data', 'name']

    def __init__(self, type, data, name=None, obj=None):
        super().__init__(type, data, name, obj=obj)


class MetaSharedVariable(MetaVariable):
    base = tt.sharedvar.SharedVariable
    __slots__ = ['name', 'type', 'data', 'strict']

    @classmethod
    def from_obj(cls, obj):
        if isvar(obj):
            return obj
        res = cls(obj.name, obj.type, obj.container.data, obj.container.strict,
                  obj=obj)
        return res

    def __init__(self, name, type, data, strict, obj=None):
        super().__init__(type, None, None, name, obj=obj)
        self.data = data
        self.strict = strict


class MetaTensorSharedVariable(MetaSharedVariable):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.sharedvar.TensorSharedVariable


class MetaScalarSharedVariable(MetaSharedVariable):
    base = tt.sharedvar.ScalarSharedVariable
#+END_SRC

In Listing [[theano-object-unify]] we create dispatch functions so that unification
and reification works with our Theano meta object classes and ordinary Theano
objects themselves.

#+NAME: theano-object-unify
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
tt_class_abstractions = tuple(c.base for c in MetaSymbol.__subclasses__())


def unify_MetaSymbol(u, v, s):
    # We need this, because `unify_object` only checks the object
    # types and unifies the `__slots__` (or `__dict__`) attributes.
    # Those steps miss the case when objects are equal (and unifiable)
    # based on identity or other object-level criteria (e.g. other non-
    # `__slots__` attributes).
    if u == v:
        return s
    if hasattr(u, '__slots__'):
        return unify([getattr(u, slot) for slot in u.__slots__],
                     [getattr(v, slot) for slot in v.__slots__],
                     s)
    return False


_unify.add((MetaSymbol, MetaSymbol, dict), unify_object)
_unify.add((MetaSymbol, tt_class_abstractions, dict),
           lambda u, v, s: unify_MetaSymbol(u, MetaSymbol.from_obj(v), s))
_unify.add((tt_class_abstractions, MetaSymbol, dict),
           lambda u, v, s: unify_MetaSymbol(MetaSymbol.from_obj(u), v, s))
_unify.add((tt_class_abstractions, tt_class_abstractions, dict),
           lambda u, v, s: unify_MetaSymbol(MetaSymbol.from_obj(u),
                                            MetaSymbol.from_obj(v), s))


def _reify_MetaSymbol(o, s):
    # `o.obj` could be a Theano object, but it could also be a logic variable,
    # in which case the `rands` should not be the same.
    # TODO: Seems like we could short-circuit some of the reification when
    # `o.obj` is present.
    rands = o.rators()
    new_rands = reify(rands, s)
    if rands == new_rands:
        return o
    else:
        newobj = type(o)(*new_rands)
        return newobj


_reify.add((MetaSymbol, dict), _reify_MetaSymbol)


def _reify_TheanoClasses(o, s):
    meta_obj = MetaSymbol.from_obj(o)
    return reify(meta_obj, s)


_reify.add((tt_class_abstractions, dict), _reify_TheanoClasses)


_isvar = isvar.resolve((object,))

isvar.add((MetaSymbol,), lambda x: _isvar(x) or isvar(x.obj))
#+END_SRC

The additions in Listing [[theano-object-terms]] create dispatch functions
for src_python{kanren.term.operator} {{{results(AttributeError: 'Dispatcher' object has no attribute 'operator')}}} and src_python{kanren.term.arguments} {{{results(AttributeError: 'Dispatcher' object has no attribute 'arguments')}}},
which allow us to use some algebraically aware forms of
unification--like src_python{kanren.assoccomm.eq_assoccomm} {{{results((u\, v)>)}}} (i.e. associative
and commutative equality/unification).
#+NAME: theano-object-terms
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
def operator_MetaVariable(x):
    # Get an apply node, if any
    x_owner = getattr(x, 'owner', None)
    if x_owner and hasattr(x_owner, 'op'):
        return x_owner.op
    return None


operator.add((MetaVariable,), operator_MetaVariable)
operator.add((tt.Variable,), lambda x: operator(MetaVariable.from_obj(x)))


def arguments_MetaVariable(x):
    # Get an apply node, if any
    x_owner = getattr(x, 'owner', None)
    if x_owner and hasattr(x_owner, 'op'):
        return x_owner.inputs
    return None


arguments.add((MetaVariable,), arguments_MetaVariable)
arguments.add((tt.Variable,), lambda x: arguments(MetaVariable.from_obj(x)))

# Enable [re]construction of terms
term.add((tt.Op, (list, tuple)), lambda op, args: term(MetaOp.from_obj(op), args))
term.add((MetaOp, (list, tuple)), lambda op, args: op(*args))

fact(commutative, tt.add)
fact(commutative, tt.mul)
fact(associative, tt.add)
fact(associative, tt.mul)


def meta_term(op, *args, ttype=None, index=None, name=None, var_prefix='r'):
    """Construct a `MetaTensorVariable` object corresponding to the application
    of an `Op` to some operands--with unspecified parameters replaced by
    `kanren` logic variables.

    Parameters
    ==========
    op: `Op`
        The `Op` to be applied.
    args: list of `TensorVariable`
        The operands for `Op`
    ttype: `TensorType` (optional)
        The resulting `MetaTensorVariable`'s Theano tensor type.
    index: int (optional)
        The resulting `MetaTensorVariable`'s index value.
    name: str (optional)
        The resulting `MetaTensorVariable`'s name.
    var_prefix: str (optional)
        Prefix to use for the `kanren` logic variable names/IDs.

    Results
    =======
    out: `MetaTensorVariable`
    """
    def _fix_args(x):
        if MetaSymbol.is_meta(x):
            return x
        else:
            return tt.as_tensor_variable(x)

    meta_apply = MetaApply(MetaSymbol.from_obj(op),
                           tuple(_fix_args(a) for a in args))

    return MetaTensorVariable(
        ttype or var('{}_type_lvar'.format(var_prefix)),
        meta_apply,
        index or var('{}_index_lvar'.format(var_prefix)),
        name or var('{}_name_lvar'.format(var_prefix)))

#+END_SRC

** Testing                                                        :noexport:

Listing [[theano-object-tools]] provides a high-level form of graph object comparison
(i.e. one that isn't point-equality-like).  This is especially useful during testing,
and whenever we aren't concerned with objects being strictly identical.

#+NAME: theano-object-tools
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
from collections import OrderedDict


to_meta = MetaSymbol.from_obj

def expand_meta(x, tt_print=tt.pprint):
    if isinstance(x, MetaSymbol):
        return OrderedDict([('rator', x.base),
                            ('rands', tuple(expand_meta(p)
                                            for p in x.rators())),
                            ('obj', expand_meta(getattr(x, 'obj', None)))])
    elif tt_print and isinstance(x, theano.gof.op.Op):
        return x.name
    elif tt_print and isinstance(x, theano.gof.graph.Variable):
        return tt_print(x)
    else:
        return x


def graph_equal(x, y):
    """Compare elements in a Theano graph using their object properties and not
    just identity.
    """
    try:
        if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):
            return (len(x) == len(y) and
                    all(MetaSymbol.from_obj(xx) == MetaSymbol.from_obj(yy)
                        for xx, yy in zip(x, y)))
        return MetaSymbol.from_obj(x) == MetaSymbol.from_obj(y)
    except ValueError:
        return False

#+END_SRC

#+NAME: theano-meta-classes-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_meta_classes():
    vec_tt = tt.vector('vec')
    vec_m = MetaSymbol.from_obj(vec_tt)
    assert vec_m.obj == vec_tt
    assert type(vec_m) == MetaTensorVariable
    vec_type_m = vec_m.type
    assert type(vec_type_m) == MetaTensorType
    assert vec_type_m.dtype == vec_tt.dtype
    assert vec_type_m.broadcastable == vec_tt.type.broadcastable
    assert vec_type_m.name == vec_tt.type.name

    assert graph_equal(tt.add(1, 2), meta_term(tt.add, 1, 2).reify())

    meta_var = meta_term(tt.add, 1, var()).reify()
    assert isinstance(meta_var, MetaSymbol)
    assert isinstance(meta_var.owner.op.obj, theano.Op)
    assert isinstance(meta_var.owner.inputs[0].obj, tt.TensorConstant)


test_meta_classes()
#+END_SRC

#+NAME: theano-unification-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_unification():
    x, y, a, b = tt.dvectors('xyab')
    x_s = tt.scalar('x_s')
    y_s = tt.scalar('y_s')
    c = tt.constant(1, 'c')
    d = tt.constant(2, 'd')
    x_l = tt.vector('x_l')
    y_l = tt.vector('y_l')
    z_l = tt.vector('z_l')

    with variables(x_l):
        assert a == reify(x_l, {x_l: a})
        test_expr = 1 + 2 * x_l
        test_reify_res = reify(test_expr, {x_l: a})
        assert graph_equal(test_reify_res, 1 + 2*a)

    with variables(x_l):
        z = tt.add(b, a)
        assert {x_l: z} == unify(x_l, z)
        assert {x_l: b} == unify(tt.add(x_l, a), tt.add(b, a))

    with variables(x_l, y_l):
        assert {x_l: b, y_l: a} == unify(1/tt.add(x_l, a), 1/tt.add(b, y_l))

    with variables(x):
        assert unify(x, b)[x] == b
        assert unify([x], [b])[x] == b
        assert unify((x,), (b,))[x] == b
        assert unify(x + 1, b + 1)[x] == b
        assert unify(x + a, b + a)[x] == b

    with variables(x):
        assert unify(a + b, a + x)[x] == b

    with variables(x):
        assert b == next(eq(a + b, a + x)({}))[x]

    # Generalize unification for an `Op` over `TensorTypes`
    x_lvar = var('x_lvar')
    y_lvar = var('y_lvar')
    type_lvar = var('type_lvar')
    index_lvar = var('index_lvar')
    name_lvar = var('name_lvar')
    meta_add = meta_term(tt.add, x_lvar, y_lvar,
                         ttype=type_lvar,
                         index=index_lvar,
                         name=name_lvar)

    # The parameters are vectors
    tt_expr_add_1 = tt.add(x, y)
    assert graph_equal(tt_expr_add_1, reify(meta_add, unify(meta_add, tt_expr_add_1)).reify())

    # The parameters are scalars
    tt_expr_add_2 = tt.add(x_s, y_s)
    assert graph_equal(tt_expr_add_2, reify(meta_add, unify(meta_add, tt_expr_add_2)).reify())

    # The parameters are constants
    tt_expr_add_3 = tt.add(c, d)
    assert graph_equal(tt_expr_add_3,
                       reify(meta_add, unify(meta_add, tt_expr_add_3)).reify())


test_unification()
#+END_SRC

#+NAME: theano-term-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_terms():
    x, a, b = tt.dvectors('xab')
    test_expr = x + a * b

    assert test_expr.owner.op == operator(test_expr)
    assert test_expr.owner.inputs == arguments(test_expr)
    assert graph_equal(test_expr, term(operator(test_expr), arguments(test_expr)))
#+END_SRC

#+NAME: theano-kanren-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_kanren():
    x, a, b = tt.dvectors('xab')

    with variables(x):
        assert b == run(1, x, eq(a + b, a + x))[0]
        assert b == run(1, x, eq(a * b, a * x))[0]


test_kanren()
#+END_SRC

#+HEADER: :noweb-ref theano-minikaren-opt
#+NAME: theano-assoccomm-tests
#+BEGIN_SRC python :exports none :results silent
def test_assoccomm():
    from kanren.assoccomm import buildo

    x, a, b, c = tt.dvectors('xabc')
    test_expr = x + 1
    q = var('q')

    assert q == run(1, q, buildo(tt.add, test_expr.owner.inputs, test_expr))[0]
    assert tt.add == run(1, q, buildo(q, test_expr.owner.inputs, test_expr))[0].reify()
    assert graph_equal(tuple(test_expr.owner.inputs), run(1, q, buildo(tt.add, q, test_expr))[0])

    with variables(x):
        assert (to_meta(a),) == run(0, x, (eq_comm, to_meta(a * b), to_meta(b * x)))
        assert (to_meta(a),) == run(0, x, (eq_comm, to_meta(a + b), to_meta(b + x)))

    # XXX: This only works when the nested `Op`s have been collapsed
    # (i.e. after canonization--and a `+ 0`/`* 1` for the currently broken
    # Theano) See https://github.com/Theano/Theano/pull/6686
    with variables(x):
        res = run(0, x, (eq_assoc, to_meta(tt.add(a, b, c)), to_meta(tt.add(a, x))))
        assert graph_equal(res[0], b + c)
        res = run(0, x, (eq_assoc, to_meta(tt.mul(a, b, c)), to_meta(tt.mul(a, x))))
        assert graph_equal(res[0], b * c)


test_assoccomm()
#+END_SRC

** miniKanren Relations

Now that we're able to unify objects, src_python{kanren} {{{results(' from '/home/bwillard/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/kanren/__init__.py'>)}}} relations should work
on Theano graphs.  We'll start with an example of some simple algebraic
simplifications and a miniKanren goal that applies them to a Theano graph object.

Listing [[kanren-reduces-relation]] creates a set of relations in miniKanren that
succinctly generalize a few algebraic and arithmetic properties.
In this instance, the relations--expressed as miniKanren goals--are indirectly
applied through the use of a src_python{Relation} {{{results()}}} object, which serves as a
more efficient means of defining and applying simple replacement rules.

#+NAME: kanren-reduces-relation
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
reduces = Relation('reduces')

x_lvar = var('x_lvar')
y_lvar = var('y_lvar')
z_lvar = var('z_lvar')


fact(reduces,
     meta_term(tt.add, x_lvar, x_lvar),
     meta_term(tt.mul, tt.constant(2), x_lvar))
fact(reduces,
     meta_term(tt.mul, x_lvar, x_lvar),
     meta_term(tt.pow, x_lvar, tt.constant(2)))
fact(reduces,
     meta_term(tt.neg,
               meta_term(tt.neg, x_lvar, var_prefix='s')),
     x_lvar)
fact(reduces,
     meta_term(tt.exp, meta_term(tt.log, x_lvar, var_prefix='s')),
     x_lvar)
fact(reduces,
     meta_term(tt.log, meta_term(tt.exp, x_lvar, var_prefix='s')),
     x_lvar)
fact(reduces,
     meta_term(tt.mul,
               meta_term(tt.pow, x_lvar, y_lvar, var_prefix='s'),
               meta_term(tt.pow, x_lvar, z_lvar, var_prefix='t')),
     meta_term(tt.pow, x_lvar,
               meta_term(tt.add, y_lvar, z_lvar, var_prefix='s')))
#+END_SRC

#+NAME: kanren-project-goal
#+BEGIN_SRC python :exports none :results none :noweb-ref theano-minikaren-opt
def project(vars, body_func):
    "A goal constructor for projecting logic variables."
    def goal(s):
        proj_vars = reify(vars, s)
        body_func(proj_vars)
        yield s
    return goal
#+END_SRC

A goal for the reduction process is given in Listing [[kanren-reduce-goal]].  It is
a recursive goal that evaluates a single
#+NAME: kanren-reduce-goal
#+BEGIN_SRC python :exports code :results silent
def kanren_reduce(input_expr, n=0):
    def _reduce(in_expr, out_expr):
        expr_rdcd = var()
        return (conde,
                # Attempt to apply a single reduction
                [(reduces, in_expr, expr_rdcd),
                 # If it succeeds, consider another
                 (_reduce, expr_rdcd, out_expr)],
                # Return the input unchanged
                [eq(out_expr, in_expr)])

    reduced_expression = var()
    res = run(n, reduced_expression,
              (_reduce, input_expr, reduced_expression))

    return res
#+END_SRC

#+NAME: kanren-relation-tests
#+BEGIN_SRC python :exports none :results silent
def test_kanren_relation():
    a = tt.vector('a')

    def reify_all(x):
        if isinstance(x, (tuple, list)):
            return type(x)([r.reify() for r in x])
        return x.reify()

    # XXX: Expressions like `2*a` don't actually have inputs `2` and `a`;
    # They have inputs like `InplaceDimShuffle`d `2` and `a`, which won't be
    # properly represented by an equivalent meta object with inputs `2` and
    # `a`.
    # If we reify such meta objects, then the resulting object's inputs should
    # match.
    assert graph_equal((2*a, a + a), reify_all(kanren_reduce(a + a)))
    assert graph_equal((a**2, a * a), reify_all(kanren_reduce(a * a)))
    assert graph_equal((a, tt.log(tt.exp(a))), reify_all(kanren_reduce(tt.log(tt.exp(a)))))
    assert graph_equal((a, tt.exp(tt.log(a))), reify_all(kanren_reduce(tt.exp(tt.log(a)))))


test_kanren_relation()
#+END_SRC


:EXAMPLE:
On of the advantages of miniKanren for these kinds of rewrite rules, is that
it provides a stream of all the possible replacements.

For example, \eqref{eq:kanren-reduce-example} shows all the replacement
results for $x^{2} x^{2}$, which includes the original expression, combined
powers, and a squaring.

#+NAME: kanren-reduce-example
#+BEGIN_SRC python :eval never-export :exports both :results output scalar raw replace
import textwrap


tt_tex_options = {'latex': True, 'latex_aligned': True}

x = tt.vector('x')
exa_expr = x**2 * x**2

results = '\n\\\\\n'.join([
    '&=' + tt_tex_pprint(s.reify(), tt_tex_options)
    for s in kanren_reduce(exa_expr)
    if not graph_equal(exa_expr, s)
])

print("""
\\begin{{equation*}}
\\begin{{aligned}}
    {} &=
    \\\\
{}
\\end{{aligned}}
\\label{{eq:kanren-reduce-example}}
\\end{{equation*}}
""".format(tt_tex_pprint(exa_expr, tt_tex_options).strip('()'),
           textwrap.indent(results, '\t\t')))
#+END_SRC

#+RESULTS: kanren-reduce-example
\begin{equation*}
\begin{aligned}
    {x}^{2} \circ {x}^{2} &=
    \\
		&={{x}^{2}}^{2}
		\\
		&={x}^{(2 + 2)}
\end{aligned}
\label{eq:kanren-reduce-example}
\end{equation*}


:END:
** A miniKanren src_python{LocalOptimizer}

#+NAME: kanren-theano-opt-imports
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import theano
from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv
from theano.gof.opt import LocalOptimizer, EquilibriumOptimizer
#+END_SRC

Listing [[kanren-theano-opt-class]] provides a src_python{LocalOptimizer} wrapper around
the src_python{kanren} functionality.

#+NAME: kanren-theano-opt-class
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
class KanrenRelationSub(LocalOptimizer):
    reentrant = True

    def __init__(self, kanren_relation, relation_lvars=None):
        """
        Parameters
        ==========
        kanren_relation: kanren.Relation or goal
            The miniKanren relation store or goal (taking input and output
            terms) to use.
        relation_lvars: Iterable
            A collection of term to be considered logic variables by miniKanren
            (e.g. Theano terms used in `kanren_relation`).
        """
        self.kanren_relation = kanren_relation
        self.relation_lvars = relation_lvars or []
        super().__init__()

    def transform(self, node):
        """
        TODO: Only uses *one* `run` result.
        """
        # TODO: Could do this with `self.tracks`?
        if not isinstance(node, tt.Apply):
            return False

        input_expr = node.default_output()

        with variables(*self.relation_lvars):
            q = var()
            res = run(1, q, (self.kanren_relation, input_expr, q))

        if len(res) > 0:
            new_node = res[0].reify()

            if MetaSymbol.is_meta(new_node):
                raise ValueError(
                    "Kanren results not fully reifiable: {}".format(new_node))

            # Handle (some) nodes with multiple outputs
            res = list(node.outputs)
            res[getattr(node.op, 'default_output', 0) or 0] = new_node
            return res
        else:
            return False

#+END_SRC

:EXAMPLE:
In Listing [[theano-optimize-helper]] we create a helper function that returns an
optimized version of its Theano tensor argument.

#+NAME: theano-optimize-helper
#+BEGIN_SRC python :exports code :results silent
def optimize_graph(x, optimization):
    if not isinstance(x, FunctionGraph):
        inputs = tt_inputs([x])
        outputs = [x]
        model_memo = clone_get_equiv(inputs, outputs,
                                     copy_orphans=False)
        cloned_inputs = [model_memo[i] for i in inputs]
        cloned_outputs = [model_memo[i] for i in outputs]

        x_graph = FunctionGraph(cloned_inputs, cloned_outputs, clone=False)
        x_graph.memo = model_memo
    else:
        x_graph = x

    x_graph_opt = x_graph.clone()
    optimization.optimize(x_graph_opt)
    return x_graph_opt.outputs[0]
#+END_SRC

Applying the reductions from [[kanren-reduces-relation]], we see the rules applied in
succession--as expected.
#+NAME: theano-optimize-example
#+BEGIN_SRC python :exports code :results silent
reduces_opt = EquilibriumOptimizer([KanrenRelationSub(reduces)],
                                   max_use_ratio=10)

test_opt = optimize_graph(tt.log(tt.exp(a)), reduces_opt)
assert graph_equal(a, test_opt)

test_opt = optimize_graph(-tt.log(tt.exp(-a)), reduces_opt)
assert graph_equal(a, test_opt)
#+END_SRC
:END:
* MCMC Optimizations
With the full capabilities of miniKanren, we're better prepared to implement
general rewrite rules for MCMC models.

** Simple Parameter Expansion
Let's re-attempt the replacement in \eqref{eq:norm_var_sink}.

#+NAME: kanren-normal-reduce-setup
#+BEGIN_SRC python :eval never-export :exports none :results silent :noweb strip-export
<<mcmc-requirements>>
<<theano-minikaren-opt>>
<<theano-optimize-helper>>
#+END_SRC

#+NAME: kanren-normal-reduce-rule
#+BEGIN_SRC python :exports code :results silent :noweb yes
from unification.utils import transitive_get as walk


<<hs-model>>

mcmc_transforms = Relation('mcmc_transforms')

C_lvar = var('C_lvar')
shape_lvar = var('shape_lvar')
rng_lvar = var('rng_lvar')
zero_const_lvar = MetaTensorConstant(var('zero_type'), 0, var('zero_name'))
one_const_lvar = MetaTensorConstant(var('zero_type'), 1)

fact(mcmc_transforms,
     meta_term(NormalRV,
               zero_const_lvar, C_lvar,
               shape_lvar, rng_lvar),
     meta_term(tt.mul, C_lvar,
               meta_term(NormalRV,
                         zero_const_lvar, one_const_lvar,
                         shape_lvar, rng_lvar)))


def not_eq(lvar, val):
    def _goal(s):
        lvar_val = walk(lvar, s)
        if isinstance(lvar_val, (tt.Constant, MetaConstant)):
            if lvar_val.data != val:
                yield s
        else:
            yield s
    return _goal


mcmc_goals = lambda x, y: (conde, ((mcmc_transforms, x, y),
                                   (not_eq, C_lvar, 1)))
#+END_SRC

#+NAME: kanren-normal-reduce-example
#+BEGIN_SRC python :exports code :results none
mcmc_opt = EquilibriumOptimizer([KanrenRelationSub(mcmc_goals)],
                                max_use_ratio=1)

Y_rv_opt = optimize_graph(Y_rv, mcmc_opt)
#+END_SRC

#+NAME: kanren-normal-reduce-example-print
#+BEGIN_SRC python :exports code :results output scalar raw replace
print("\\begin{{equation*}}\n{}\n\\end{{equation*}}".format(
    tt_tex_pprint(Y_rv_opt, {'latex': True, 'latex_aligned': True})))
#+END_SRC

#+RESULTS: kanren-normal-reduce-example-print
\begin{equation*}
\begin{aligned}
\tau &\sim \text{C}\left(0, 1\right), \quad \mathbb{R}
\\
A &\sim \text{N}\left(0, 1\right), \quad \mathbb{R}
\\
Y &\sim \text{N}\left((|\tau| \circ A), 1\right), \quad \mathbb{R}
\end{aligned}
\\
Y
\end{equation*}


** Normal-Gamma Gibbs Sampling
[[citet:ZhangTraceclassMarkov2019]] provides a prescription for more efficient
Gibbs block sampling based on Normal-Gamma family parameters.
This is exactly the kind of high-level theoretical work that can be implemented
in a sufficiently sophisticated, algebraically aware term rewriting context.

:REMARK:
One of the "sophistications" missing here is *constraint relations* in our miniKanren
implementation.
:END:
* Discussion

#+BIBLIOGRAPHY: ../tex/symbolic-pymc3.bib
#+BIBLIOGRAPHYSTYLE: plainnat
