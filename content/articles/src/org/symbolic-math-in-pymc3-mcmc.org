#+TITLE: Graph Manipulation and MCMC
#+AUTHOR: Brandon T. Willard
#+DATE: 2019-01-15
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+BEGIN_SRC elisp :eval t :exports none :results none
(org-babel-load-file "org-setup.org")
(org-babel-lob-ingest "org-babel-extensions.org")
#+END_SRC

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session symbolic-math-pymc3-mcmc

#+NAME: set-pelican-preamble
#+BEGIN_SRC elisp :eval export-only :exports results :results value raw
(org-pelican-create-yaml)
#+END_SRC

#+BEGIN_abstract
Continuing from [[citet:WillardRandomVariablesTheano2018]], ...
#+END_abstract

* Introduction

With a set of distributions defined more completely within a graph, we can much
more easily produce MCMC samplers that use distribution-level domain knowledge.

In the examples we'll use here, the model of interest will be the Horseshoe
model [[citep:carvalho_horseshoe_2010]] given by
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \mathop{\text{N}}\nolimits\left(0, \tau^2\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:hs_model}
\end{equation}

The Horseshoe prior quickly decays, so many generic sampling methods tend
produce poor estimates for models using it.  However, under different parameter
expansions, the prior can be sampled more efficiently.  Those expansions
usually depend domain knowledge in the areas of probability theory and the
integral calculus that is not easily accessible to primarily derivative-based
approaches.

As well, these reformulations are often very simplistic when formulated in terms
of random variables, and that simplicity can translate to simpler
implementations and lower computational costs.

#+NAME: theano-random-function-load
#+BEGIN_SRC python :exports none :results none :var src=(org-babel-eval-read-file "theano-random-variable.py")
exec(src)
#+END_SRC

#+NAME: mcmc-requirements
#+BEGIN_SRC python :exports none :results none :noweb strip-export
# <<theano-random-function-load()>>

from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv

theano.config.compute_test_value = 'ignore'
#+END_SRC

* Understanding the Problem(s)

To start, we need to identify sub-graphs that match our considered
reformulations, reason about them, and--when appropriate--replace them.

For the first step, which involves searching a graph for specific Theano
expressions, we'll consider two common approaches:
- directly--by walking "walking" Python functions through a graph--and
- pattern-matching--via some form of domain-specific language.

Using a direct approach, it can be very straight-forward to implement simple
search-and-replace objectives.  However, for modeling non-trivial systems
of logic and mathematical frameworks--like probability theory, different
algebras, and properties of function spaces--a direct approach will quickly
become unscalable, error-prone, and impossibly difficult to maintain.

The work we intend to do makes implicit use of nominal logic, field axioms,
and aspects of the typed lambda calculus--among other things.

Let's illustrate some of these points using an example.

:EXAMPLE:
We demonstrate automatic model reformulation using the variance expansion in
\eqref{eq:hs_model} in [[citet:scott_parameter_2010]].

This expansion works pushing the half-Cauchy variance term, $\tau$, out of the
normal, $\beta$:
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \tau \cdot \mathop{\text{N}}\nolimits\left(0, 1\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:norm_var_sink}
\end{equation}

An applicable reformulation rule might look like the following:
\begin{equation}
  \begin{aligned}
    \mathop{\text{N}}\nolimits\left(a m, a^2 C\right)
    &\to a \mathop{\text{N}}\nolimits\left(m, C\right)
  \end{aligned}
\label{eq:norm_replacement_exa}
\;.
\end{equation}
:END:

Now, what about an expression like $\mathop{\text{N}}\nolimits\left(0,
a^2\right)$; should our replacement in \eqref{eq:norm_replacement_exa} produce
$a \mathop{\text{N}}\nolimits\left(0, 1\right)$?

Conventional mathematical reasoning might say so, but a term rewriting system
will need to be designed to account for this explicitly.  The important
questions have more to do with *where* and *how*: by requiring the addition of all
necessary replacements, or by automatic reasoning using a relatively small set of
axioms from which they can be derived?

For instance, we could provide some rules that represent group/ring/field axioms
(e.g. identity and zero elements) and allow such a system to consider them in
tandem.

For ease-of-use--especially with respect to developers working at the
mathematics-level--it's more desirable to formulate rules at a high level
(e.g. set and measure theory) and use a system with a generalized means
of verifying and deriving these expectations.
Even for general scalability, testing, and development, it can be much better to
focus on well compartmentalized pieces of such a system that have direct
mappings to well understood subjects.

While such solutions are possible, they're not trivial to implement *correctly*.
It shouldn't be surprising that there are very deep bodies of research on such
systems--usually under the subjects *term rewriting* and *symbolic computation*.
A good part of classical AI focused on the challenges induced by these automation
objectives and their implementations.

This doesn't mean--however--that they're prohibitively difficult to use or develop.
As demonstrated in [[citet:WillardRandomVariablesTheano2018]], Theano provides
some pattern-based graph manipulation using a form of unification.  This functionality
shares some of the same fundamental abstraction(s) as the more sophisticated systems
alluded to earlier, but it starts to fall short right where our objectives get started.

In the following, we'll demonstrate the critical short-comings, and introduce some
steps further into the direction of modern unification and term rewriting.
# [[citet:ByrdRelationalProgrammingminiKanren2009]] [[citet:RocklinlogpyLogicProgramming2018]]
# [[citet:WillardRoleSymbolicComputation2017]]
* A Language for Graph Manipulation
In this case, graph manipulation mostly consists of term rewriting in the
context of a Theano graphs, and--as we've stated earlier--the term rewriting is
mostly driven by algebraic considerations.

The mechanical aspects of this work is largely generalizable in terms of
orchestrated unification.  In the lead-up articles
[[citet:WillardRandomVariablesTheano2018]] and
[[citet:WillardSymbolicMathPyMC32018]], we used src_python{PatternSub}, which
uses unification (and reification) to implement pattern matching and
substitution (i.e. rewrite rules).

:REMARK:
In the context of Python, this sort of work has some fundamental limitations and
unnecessarily confusing aspects.  Python doesn't lend itself to symbolic manipulation,
making things like expression manipulation and traversal particularly onerous.
:END:

Beyond some small technical issues, src_python{PatternSub} only provides a
limited form of unification, and doesn't provide a programmable context for
controlling exactly how and when the unification is performed.

:EXAMPLE:
Let's attempt to implement the replacement in \eqref{eq:norm_var_sink}
using src_python{PatternSub}.

#+ATTR_LATEX: :float nil
#+CAPTION: A naively specified Horseshoe model.
#+NAME: hs-model
#+BEGIN_SRC python :exports none :results silent
tau_rv = CauchyRV(0, 1, name='\\tau')
beta_stddev = tt.abs_(tau_rv)
beta_rv = NormalRV(0, beta_stddev, name='\\beta')
Y_rv = NormalRV(beta_rv, 1, name='Y')
#+END_SRC

#+NAME: hs-var-expansion-opt-setup
#+BEGIN_SRC python :exports none :results silent :noweb strip-export
<<mcmc-requirements>>
#+END_SRC

#+NAME: hs_var_expansion_opt
#+BEGIN_SRC python :results output :noweb yes :wrap "SRC python :eval never"
<<hs-model>>

construct_norm_rv = lambda rng, size, mu, sd: NormalRV(mu, sd, size=size, rng=rng)

norm_sink_pats = [
    # N(0, a^2) -> a N(0, 1)
    tt.gof.opt.PatternSub(
        (NormalRV, 'rng_x', 'size_x',
         'b_x', 'a_x'),
        (tt.mul, 'a_x',
         (construct_norm_rv, 'rng_x', 'size_x',
          'b_x', tt.constant(1.0)))),
]

norm_sink_opts = tt.gof.opt.EquilibriumOptimizer(
    norm_sink_pats, max_use_ratio=10)
hs_Y_graph = FunctionGraph(tt_inputs([Y_rv]), [Y_rv])
hs_Y_graph_opt = hs_Y_graph.clone()

_ = norm_sink_opts.optimize(hs_Y_graph_opt)
#+END_SRC

We see in [[hs_var_expansion_opt]] that moving from a node that produces two outputs
(i.e. a src_python{RandomVariable} outputs the symbolic RNG *and* a tensor from
the sample space) to one that produces only a single output (i.e. a product)
result in an error.  A work-around for this doesn't seem possible, and a fix for
src_python{PatternSub.transform} is most likely necessary.

#+RESULTS: hs_var_expansion_opt
#+begin_SRC python :eval never
/tmp/user/1000/babel-TfmXjk/python-5CWvpq in <module>()
     16 hs_Y_graph_opt = hs_Y_graph.clone()
     17
---> 18 _ = norm_sink_opts.optimize(hs_Y_graph_opt)

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in optimize(self, fgraph, *args, **kwargs)
     95             orig = theano.tensor.basic.constant.enable
     96             theano.tensor.basic.constant.enable = False
---> 97             ret = self.apply(fgraph, *args, **kwargs)
     98         finally:
     99             theano.tensor.basic.constant.enable = orig

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in apply(self, fgraph, start_from)
   2511                         nb = change_tracker.nb_imported
   2512                         t_opt = time.time()
-> 2513                         lopt_change = self.process_node(fgraph, node, lopt)
   2514                         time_opts[lopt] += time.time() - t_opt
   2515                         if not lopt_change:

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in process_node(self, fgraph, node, lopt)
   2056         if len(old_vars) != len(replacements):
   2057             raise ValueError('Optimizer %s gave wrong number of replacements'
-> 2058                              % lopt)
   2059         # None in the replacement mean that this variable isn't used
   2060         # and we want to remove it

ValueError: Optimizer normal_rv(rng_x, size_x, b_x, a_x) -> Elemwise{mul,no_inplace}(a_x, <function <lambda> at 0x7f509671d9d8>(rng_x, size_x, b_x, TensorConstant{1.0})) gave wrong number of replacements


#+end_SRC
:END:

The [[citet:miniKanrenorg]] domain-specific language (DSL) provides an abstract
platform upon which all the capabilities we seek are provided.  While most
implementations are expressed seamlessly in a Lisp-like language, there are some
for Python.  In particular, we will use src_python{kanren}
[[citep:RocklinlogpyLogicProgramming2018]] .

To get started, we'll create a new src_python{theano.gof.opt.LocalOptimizer} that uses
src_python{kanren} in a limited capacity (e.g. only for implementing rewrite rules).

* A miniKanren Theano Optimizer

As in the example Hy compiler from
[[citet:WillardReadableStringsRelational2018a]], we need to specify how
unification occurs in the context of Theano objects.

Throughout, we'll make extensive use of multiple-dispatch (implemented by the
Python library src_python{multipledispatch}
[[citep:RocklinMultipledispatchContribute2019]]).

#+NAME: minikanren-opt-imports
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
from collections import Callable
from warnings import warn

import theano
import theano.tensor as tt

from theano.printing import debugprint as tt_dprint

from kanren import var, run, eq, conde, lall, fact, Relation, isvar
from kanren.core import success, fail

from kanren.term import term, operator, arguments
from kanren.assoccomm import eq_assoccomm, eq_assoc, eq_comm
from kanren.assoccomm import commutative, associative

from unification import variables
from unification.core import unify, reify, _unify, _reify
from unification.more import unify_object

from theano.tensor import Elemwise
from theano.scalar.basic import mul, add

from multipledispatch import dispatch
#+END_SRC

** Theano Graph Unification and Reification

In the following, we implement unification using src_python{unification}--itself
using src_python{multipledispatch}.

We start by defining "expression" forms of Theano graph objects.  These consist
of tuples that represent function calls--src_python{(op, *args)}--that would
effectively re-construct a given Theano graph object.  In essence, we're
approximating a Lisp-like intermediate language.

#+NAME: theano-object-expansion
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
from collections import Callable


def _maybe_reify(s):
    if hasattr(s, 'reify') and isinstance(s.reify, Callable):
        return s.reify()
    else:
        return s


class AbstractSymbol(object):
    @classmethod
    def from_obj(cls, obj):
        if (isinstance(obj, AbstractSymbol) or
                isvar(obj) or obj is None):
            return obj

        obj_cls = [s for s in AbstractSymbol.__subclasses__()
                   if s.meta == type(obj)]

        if len(obj_cls) == 0:
            raise ValueError('Could not find a class for {}'.format(type(obj)))
        elif len(obj_cls) > 1:
            raise ValueError('More than one class for {}'.format(type(obj)))

        obj_cls = obj_cls[0]
        res = obj_cls(*[getattr(obj, s) for s in obj_cls.__slots__])
        res.obj = obj
        return res

    def parameters(self):
        return [getattr(self, s) for s in self.__slots__]

    def reify(self):
        if getattr(self, 'obj', None):
            return self.obj
        else:
            return self.meta(*[_maybe_reify(getattr(self, s))
                               for s in self.__slots__])

    def __eq__(self, other):
        if (type(self) == type(other) and
                self.meta == other.meta):
            return all(getattr(self, attr) == getattr(other, attr)
                       for attr in self.__slots__)
        elif self.meta == type(other):
            return all(getattr(self, attr) == getattr(other, attr)
                       for attr in self.__slots__)
        return False

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        def _list_to_tuple(x):
            if isinstance(x, list):
                return tuple(x)
            else:
                return x
        return hash(tuple(_list_to_tuple(p) for p in self.parameters()))


class AbstractTensorType(AbstractSymbol):
    meta = tt.TensorType
    __slots__ = ['dtype', 'broadcastable', 'name']

    def __init__(self, dtype, broadcastable, name):
        self.dtype = dtype
        self.broadcastable = broadcastable
        self.name = name


# class AbstractOp(AbstractSymbol):
#     meta = tt.Op
#     __slots__ = ['op']
#
#     def __init__(self, op):
#         self.op = op
#
#     def reify(self):
#         return self.op
#
#     def __call__(self, *args):
#         return self.op(*args)


class AbstractApply(AbstractSymbol):
    meta = tt.Apply
    __slots__ = ['op', 'inputs']

    def __init__(self, op, inputs, outputs=[]):
        self.op = op
        self.inputs = [self.from_obj(i) for i in inputs]

    def reify(self):
        return self.op(*[_maybe_reify(i) for i in self.inputs])


class AbstractTensorVariable(AbstractSymbol):
    meta = tt.TensorVariable
    __slots__ = ['type', 'owner', 'index', 'name']

    def __init__(self, type, owner, index, name):
        self.type = self.from_obj(type)
        self.owner = self.from_obj(owner)
        self.index = index
        self.name = name

    def reify(self):
        """Having an `owner` causes issues.  Let the owning `Apply` create this
        object."""
        if self.owner:
            res = self.owner.reify()
            return res.owner.outputs[self.index]
        return super().reify()


class AbstractTensorConstant(AbstractSymbol):
    meta = tt.TensorConstant
    __slots__ = ['type', 'data', 'name']

    def __init__(self, type, data, name):
        self.type = self.from_obj(type)
        self.data = data
        self.name = name

#+END_SRC

Next, we need to create dispatch functions so that unification and reification
works with our Theano meta object classes and ordinary Theano objects
themselves.
Also, by creating dispatch functions for src_python{kanren.term.operator}
and src_python{kanren.term.arguments}, we're able to use the algebraically aware
forms of unification--like src_python{kanren.assoccomm.eq_assoccomm}
(i.e. associative and commutative equality/unification).

#+NAME: theano-object-unify-and-terms
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
tt_class_abstractions = tuple(c.meta for c in AbstractSymbol.__subclasses__())

_unify.add((AbstractSymbol, AbstractSymbol, dict), unify_object)
_unify.add((AbstractSymbol, tt_class_abstractions, dict),
           lambda u, v, s: unify_object(u, AbstractSymbol.from_obj(v), s))
_unify.add((tt_class_abstractions, AbstractSymbol, dict),
           lambda u, v, s: unify_object(AbstractSymbol.from_obj(u), v, s))
_unify.add((tt_class_abstractions, tt_class_abstractions, dict),
           lambda u, v, s: unify_object(AbstractSymbol.from_obj(u),
                                        AbstractSymbol.from_obj(v), s))


def _reify_AbstractSymbol(o, s):
    attrs = [getattr(o, attr) for attr in o.__slots__]
    new_attrs = reify(attrs, s)
    if attrs == new_attrs and o.obj:
        return o.obj
    else:
        newobj = type(o)(*new_attrs)
        return newobj.reify()


_reify.add((AbstractSymbol, dict), _reify_AbstractSymbol)


def _reify_TheanoClasses(o, s):
    meta_obj = AbstractSymbol.from_obj(o)
    return reify(meta_obj, s)


_reify.add((tt_class_abstractions, dict), _reify_TheanoClasses)


#
# These allow some Theano `Op`s to be unified associatively and commutatively.
#
def operator_AbstractSymbol(x):
    # Get an apply node, if any
    res = getattr(x, 'owner', None)
    if res and hasattr(res, 'op'):
        return res.op
    return x.meta


operator.add((AbstractSymbol,), operator_AbstractSymbol)
operator.add((tt_class_abstractions,), lambda x: operator(AbstractSymbol.from_obj(x)))


def arguments_AbstractSymbol(x):
    # Get an apply node, if any
    res = getattr(x, 'owner', None)
    if res and hasattr(res, 'op'):
        return res.inputs
    return res.parameters()


arguments.add((AbstractSymbol,), arguments_AbstractSymbol)
arguments.add((tt_class_abstractions,), lambda x: arguments(AbstractSymbol.from_obj(x)))


def term_Op(op, args):
    r_args = reify(args, {})
    return op(*r_args)


term.add((tt.Op, (list, tuple)), term_Op)

fact(commutative, tt.add)
fact(commutative, tt.mul)
fact(associative, tt.add)
fact(associative, tt.mul)
#+END_SRC

** Testing                                                        :noexport:

Listing [[theano-object-equality]] provides a high-level form of graph object comparison
(i.e. one that isn't point-equality-like).  This is especially useful during testing,
and whenever we aren't concerned with objects being strictly identical.

#+NAME: theano-object-equality
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def graph_equal(x, y):
    """Compare elements in a Theano graph using their object properties and not
    just identity.
    """
    return AbstractSymbol.from_obj(x) == AbstractSymbol.from_obj(y)
#+END_SRC

#+NAME: theano-unification-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_unification():
    x = tt.vector('x')
    y = tt.vector('y')
    a = tt.vector('a')
    b = tt.vector('b')
    x_s = tt.scalar('x_s')
    y_s = tt.scalar('y_s')
    c = tt.constant(1, 'c')
    d = tt.constant(2, 'd')
    x_l = tt.vector('x_l')
    y_l = tt.vector('y_l')
    z_l = tt.vector('z_l')

    with variables(x_l):
        assert a == reify(x_l, {x_l: a})
        test_expr = 1 + 2 * x_l
        test_reify_res = reify(test_expr, {x_l: a})
        assert graph_equal(test_reify_res, 1 + 2*a)

    with variables(x_l):
        z = tt.add(b, a)
        assert {x_l: z} == unify(x_l, z)
        assert {x_l: b} == unify(tt.add(x_l, a), tt.add(b, a))

    with variables(x_l, y_l):
        assert {x_l: b, y_l: a} == unify(1/tt.add(x_l, a), 1/tt.add(b, y_l))

    with variables(x):
        assert unify(x, b)[x] == b
        assert unify([x], [b])[x] == b
        assert unify((x,), (b,))[x] == b
        assert unify(x + 1, b + 1)[x] == b
        assert unify(x + a, b + a)[x] == b

    with variables(x):
        assert unify(a + b, a + x)[x] == b

    with variables(x):
        assert b == next(eq(a + b, a + x)({}))[x]

    # Generalize unification for an `Op` over `TensorTypes`
    x_lvar = var('x_lvar')
    y_lvar = var('y_lvar')
    type_lvar = var('type_lvar')
    index_lvar = var('index_lvar')
    name_lvar = var('name_lvar')
    meta_add = AbstractTensorVariable(
        type_lvar,
        AbstractApply(tt.add, [x_lvar, y_lvar]),
        index_lvar, name_lvar)

    # The parameters are vectors
    tt_expr_add_1 = tt.add(x, y)
    expected_res = {type_lvar: tt_expr_add_1.type,
                    x_lvar: x, y_lvar: y,
                    index_lvar: 0, name_lvar: None}
    assert expected_res == reify(unify(meta_add, tt_expr_add_1), {})

    # The parameters are scalars
    tt_expr_add_2 = tt.add(x_s, y_s)
    expected_res = {type_lvar: tt_expr_add_2.type,
                    x_lvar: x_s, y_lvar: y_s,
                    index_lvar: 0, name_lvar: None}
    assert expected_res == reify(unify(meta_add, tt_expr_add_2), {})

    # The parameters are constants
    tt_expr_add_3 = tt.add(c, d)
    expected_res = {type_lvar: tt_expr_add_3.type,
                    x_lvar: c, y_lvar: d,
                    index_lvar: 0, name_lvar: None}
    assert expected_res == reify(unify(meta_add, tt_expr_add_3), {})


test_unification()
#+END_SRC

#+NAME: theano-term-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_terms():
    x = tt.vector('x')
    a = tt.vector('a')
    b = tt.vector('b')

    test_expr = x + a * b

    assert test_expr.owner.op == operator(test_expr)
    assert test_expr.owner.inputs == arguments(test_expr)
    assert graph_equal(test_expr, term(operator(test_expr), arguments(test_expr)))
#+END_SRC

#+NAME: theano-kanren-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_kanren():
    x = tt.vector('x')
    a = tt.vector('a')
    b = tt.vector('b')

    with variables(x):
        assert b == run(1, x, eq(a + b, a + x))[0]
        assert b == run(1, x, eq(a * b, a * x))[0]


test_kanren()


def test_assoccomm():
    x = tt.vector('x')
    a = tt.vector('a')
    b = tt.vector('b')
    c = tt.vector('c')

    with variables(x):
        # TODO FIXME: One of the goals is failing (maybe `buildo`?)

        # call = lambda x: x[0](*x[1:])
        # eq_assoc(a + (b + c), ((a + b) + c))
        # next(call(eq_assoc(a + (b + c), ((a + b) + c)))({}))
        # next(call(call(eq_comm(a + b, b + a)))({}))
        # run(1, x, eq_assoc(a + (b + c), ((a + x) + c)))

        assert (b,) == run(1, x, eq_comm(a + (x + c), ((a + x) + c)))
        assert (b,) == run(1, x, eq_comm(a * (x * c), ((a * x) * c)))
        assert (b,) == run(1, x, eq_comm(a + b, x + a))
        assert (b,) == run(1, x, eq_comm(a * b, x * a))


test_assoccomm()
#+END_SRC

** miniKanren Relations

Now that we're able to unify objects, src_python{kanren} relations should work
on Theano graphs.  We'll start with an example of some simple algebraic
simplifications and a miniKanren goal that applies them to a Thean graph object.

:EXAMPLE:
#+NAME: reduces-relation
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
x_lvar = tt.vector('x_lvar')
reduces_lvars = {x_lvar}
reduces = Relation('reduces')

fact(reduces, x_lvar + x_lvar, 2*x_lvar)
# See `theano.tensor.opt.local_opt_alloc`.
fact(reduces, x_lvar * x_lvar, x_lvar**2)
# See `theano.tensor.opt.local_neg_neg`.
fact(reduces, -(-x_lvar), x_lvar)
fact(reduces, tt.exp(tt.log(x_lvar)), x_lvar)
fact(reduces, tt.log(tt.exp(x_lvar)), x_lvar)
#+END_SRC

#+BEGIN_SRC python :exports none :results none :noweb-ref theano-minikaren-opt
def project(vars, body_func):
    "A goal constructor for projecting logic variables."
    def goal(s):
        proj_vars = reify(vars, s)
        body_func(proj_vars)
        yield s
    return goal
#+END_SRC

#+NAME: kanren-reduce-example
#+BEGIN_SRC python :exports code :results silent
def kanren_reduce(input_expr, n=0):
    with variables(*reduces_lvars):
        def _reduce(in_expr, out_expr):
            rdc_expr = var()
            expr_rdcd = var()

            match_goals = [(eq_assoccomm, in_expr, rdc_expr),
                           (reduces, rdc_expr, expr_rdcd),
                           # Reduces further?
                           (_reduce, expr_rdcd, out_expr),
            ]
            return (conde,
                    # It reduced...
                    match_goals,
                    # ...or it didn't, so return the expression as it was.
                    [success, eq(out_expr, in_expr)])

        reduced_expression = var()
        res = run(n, reduced_expression,
                  (_reduce, input_expr, reduced_expression))

        return res
#+END_SRC

#+NAME: kanren-reduce-example-asserts
#+BEGIN_SRC python :exports code :results silent
assert a == kanren_reduce(tt.log(tt.exp(a)))[0]
assert a == kanren_reduce(tt.exp(tt.log(a)))[0]
assert graph_equal(2*a, kanren_reduce(a + a)[0])
assert graph_equal(a**2, kanren_reduce(a * a)[0])
#+END_SRC
:END:
** A miniKanren src_python{LocalOptimizer}

#+NAME: kanren-theano-opt-imports
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import theano
from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv
from theano.gof.opt import LocalOptimizer, EquilibriumOptimizer
#+END_SRC

Listing [[kanren-theano-opt-class]] provides a src_python{LocalOptimizer} wrapper around
the src_python{kanren} functionality.

#+NAME: kanren-theano-opt-class
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
class KanrenRelationSub(LocalOptimizer):
    reentrant = True

    def __init__(self, kanren_relation, relation_lvars):
        """
        Parameters
        ==========
        kanren_relation: kanren.Relation
            The miniKanren relation store to use.
        relation_lvars: set
            A set of term to be considered logic variables by miniKanren
            (e.g. Theano terms used in `kanren_relation`).
        """
        self.kanren_relation = kanren_relation
        self.relation_lvars = relation_lvars
        super().__init__()

    def transform(self, node):
        """
        TODO: Only uses *one* `run` result.
        """
        # TODO: Could do this with `self.tracks`?
        if not isinstance(node, tt.Apply):
            return False

        input_expr = node.default_output()

        with variables(*self.relation_lvars):
            def _reduce(in_expr, out_expr):
                rdc_expr = var()
                match_goals = [(eq_assoccomm, in_expr, rdc_expr),
                               (self.kanren_relation, rdc_expr, out_expr)]
                return match_goals

            reduced_expression = var()
            res = run(1, reduced_expression,
                      ,*_reduce(input_expr, reduced_expression))

        if len(res) > 0:
            new_node = res[0]

            if isinstance(res, (tuple, list)):
                # In this case, the miniKanren results are (assumedly) unevaluated
                # Theano object expressions.  This result is given when the replacement
                # pattern is an unevaluated Theano object.
                # We need to reify them.
                new_node = reify(ExpressionTuple(new_node), {})

                assert not isinstance(new_node, (tuple, list))

            # Handle (some) nodes with multiple outputs
            res = list(node.outputs)
            res[getattr(node.op, 'default_output', 0) or 0] = new_node
            return res
        else:
            return False

#+END_SRC

:EXAMPLE:
In Listing [[theano-optimize-helper]] we create a helper function that returns an
optimized version of its Theano tensor argument.

#+NAME: theano-optimize-helper
#+BEGIN_SRC python :exports code :results silent
def optimize_graph(x, optimization):
    if not isinstance(x, FunctionGraph):
        inputs = tt_inputs([x])
        outputs = [x]
        model_memo = clone_get_equiv(inputs, outputs,
                                     copy_orphans=False)
        cloned_inputs = [model_memo[i] for i in inputs]
        cloned_outputs = [model_memo[i] for i in outputs]

        x_graph = FunctionGraph(cloned_inputs, cloned_outputs, clone=False)
        x_graph.memo = model_memo
    else:
        x_graph = x

    x_graph_opt = x_graph.clone()
    optimization.optimize(x_graph_opt)
    return x_graph_opt.outputs[0]
#+END_SRC

Applying the reductions from [[reduces-relation]], we see the rules applied in
succession--as expected.
#+NAME: theano-optimize-example
#+BEGIN_SRC python :exports code :results silent
reduces_opt = EquilibriumOptimizer([KanrenRelationSub(reduces,
                                                      reduces_lvars)],
                                   max_use_ratio=10)

test_opt = optimize_graph(tt.log(tt.exp(a)), reduces_opt)
assert graph_equal(a, test_opt)

test_opt = optimize_graph(-tt.log(tt.exp(-a)), reduces_opt)
assert graph_equal(a, test_opt)
#+END_SRC
:END:
* MCMC Optimizations
With the full capabilities of miniKanren, we're better prepared to implement
general rewrite rules for MCMC models.

** Simple Parameter Expansion
Let's re-attempt the replacement in \eqref{eq:norm_var_sink}.

#+NAME: kanren-normal-reduce-setup
#+BEGIN_SRC python :eval never-export :exports none :results silent :noweb strip-export
<<mcmc-requirements>>
<<theano-minikaren-opt>>
<<theano-optimize-helper>>
#+END_SRC

#+NAME: kanren-normal-reduce-rule
#+BEGIN_SRC python :exports code :results silent :noweb yes
<<hs-model>>

<<reduces-relation>>

C_lvar = tt.scalar('C_lvar')
shape_lvar = var('shape_lvar')
rng_lvar = var('rng_lvar')
zero_dtype_lvar = var('zero_dtype')
zero_const_lvar = (tt.TensorConstant, zero_dtype_lvar, 0)

reduces_lvars |= {rng_lvar, shape_lvar, C_lvar}
fact(reduces,
     # Use un-evaluated forms
     (NormalRV, zero_dtype_lvar, C_lvar, shape_lvar, rng_lvar),
     (Elemwise(mul), C_lvar,
      (NormalRV, zero_dtype_lvar, (tt.TensorConstant, zero_dtype_lvar, 1), shape_lvar, rng_lvar))
)
#+END_SRC

#+BEGIN_SRC python :exports none :results none
C_lvar = var('C_lvar')
shape_lvar = var('shape_lvar')
rng_lvar = var('rng_lvar')

norm_term_expanded = (term_operator(beta_rv),) + term_arguments(beta_rv)
norm_expr = (NormalRV, rng_lvar, shape_lvar, norm_term_expanded[3], C_lvar)


assert unify(norm_expr[1], norm_term_expanded[1], {}) is not False
assert unify(norm_expr[2], norm_term_expanded[2], {}) is not False
assert unify(norm_expr[3], norm_term_expanded[3], {}) is not False

term_operator(norm_expr[3]), term_arguments(norm_expr[3])
term_operator(norm_term_expanded[3]), term_arguments(norm_term_expanded[3])

term_arguments(norm_expr[3])[0] == term_arguments(norm_term_expanded[3])[0]

unify(term_operator(norm_expr[3]), term_operator(norm_term_expanded[3]))
unify(term_arguments(norm_expr[3]), term_arguments(norm_term_expanded[3]))

_unify.resolve((type(norm_expr[3]), type(norm_term_expanded[3]), dict))

_unify(norm_expr, norm_term_expanded, {})

_unify.resolve((type(norm_expr), type(beta_rv), dict))
_unify(norm_expr, beta_rv, {})
#+END_SRC

#+NAME: kanren-normal-reduce-example
#+BEGIN_SRC python :exports code :results none
reduces_opt = EquilibriumOptimizer([KanrenRelationSub(reduces,
                                                      reduces_lvars)],
                                   max_use_ratio=10)

# Y_rv_opt = optimize_graph(Y_rv, reduces_opt)
Y_rv_opt = optimize_graph(beta_rv, reduces_opt)
#+END_SRC

#+NAME: kanren-normal-reduce-example-print
#+BEGIN_SRC python :exports code :results output scalar raw replace
print("\\begin{{equation*}}\n{}\n\\end{{equation*}}".format(
    tt_tex_pprint(Y_rv_opt, {'latex': True, 'latex_aligned': True})))
#+END_SRC

** Normal-Gamma Gibbs Sampling
[[citet:ZhangTraceclassMarkov2019]] provides a prescription for more efficient
Gibbs block sampling based on Normal-Gamma family parameters.
This is exactly the kind of high-level theoretical work that can be implemented
in a sufficiently sophisticated, algebraically aware term rewriting context.

:REMARK:
One of the "sophistications" missing here is *constraint relations* in our miniKanren
implementation.
:END:
* Discussion

#+BIBLIOGRAPHY: ../tex/symbolic-pymc3.bib
#+BIBLIOGRAPHYSTYLE: plainnat
