#+TITLE: Graph Manipulation and MCMC
#+AUTHOR: Brandon T. Willard
#+DATE: 2019-01-15
#+EMAIL: brandonwillard@gmail.com
#+FILETAGS: :pymc3:theano:statistics:symbolic computation:python:probability theory:

#+STARTUP: hideblocks indent hidestars
#+OPTIONS: author:t date:t ^:nil toc:nil title:t tex:t d:(not "todo" "logbook" "note" "testing" "notes") html-preamble:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />
#+STYLE: <link rel="stylesheet" type="text/css" href="../extra/custom.css" />

#+BEGIN_SRC elisp :eval t :exports none :results none
(org-babel-load-file "org-setup.org")
(org-babel-lob-ingest "org-babel-extensions.org")
#+END_SRC

#+PROPERTY: header-args :eval never-export :exports both :results output drawer replace
#+PROPERTY: header-args+ :session symbolic-math-pymc3-mcmc

#+NAME: set-pelican-preamble
#+BEGIN_SRC elisp :eval export-only :exports results :results value raw
(org-pelican-create-yaml)
#+END_SRC

#+RESULTS: set-pelican-preamble
#+BEGIN_EXPORT html
---
bibliography:
- 'tex/symbolic-pymc3.bib'
modified: '2019-1-28'
tags: 'pymc3,theano,statistics,symbolic computation,python,probability theory'
title: Graph Manipulation and MCMC
date: '2019-01-15'
author: 'Brandon T. Willard'
figure_dir: '{attach}/articles/figures/'
figure_ext: png
---
#+END_EXPORT

#+BEGIN_abstract
Continuing from [[citet:WillardRandomVariablesTheano2018]], ...
#+END_abstract

* Introduction

With a set of distributions defined more completely within a graph, we can much
more easily produce MCMC samplers that use distribution-level domain knowledge.

In the examples we'll use here, the model of interest will be the Horseshoe
model [[citep:carvalho_horseshoe_2010]] given by
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \mathop{\text{N}}\nolimits\left(0, \tau^2\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:hs_model}
\end{equation}

The Horseshoe prior quickly decays, so many generic sampling methods tend
produce poor estimates for models using it.  However, under different parameter
expansions, the prior can be sampled more efficiently.  Those expansions
usually depend domain knowledge in the areas of probability theory and the
integral calculus that is not easily accessible to primarily derivative-based
approaches.

As well, these reformulations are often very simplistic when formulated in terms
of random variables, and that simplicity can translate to simpler
implementations and lower computational costs.

#+NAME: plotting-setup
#+BEGIN_SRC python :exports none :results none
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib import rcParams

rcParams['figure.figsize'] = (11.7, 8.27)

# plt.rc('text', usetex=True)
sns.set_style("whitegrid")
sns.set_context("paper")
#+END_SRC

#+NAME: theano-random-function-load
#+BEGIN_SRC python :exports none :results none :var src=(org-babel-eval-read-file "theano-random-variable.py")
exec(src)
#+END_SRC

#+NAME: mcmc-requirements
#+BEGIN_SRC python :exports none :results none :noweb strip-export
# <<theano-random-function-load()>>

from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv

theano.config.compute_test_value = 'ignore'
#+END_SRC

* Understanding the Problem(s)

To start, we need to identify sub-graphs that match our considered
reformulations, reason about them, and--when appropriate--replace them.

For the first step, which involves searching a graph for specific Theano
expressions, we'll consider two common approaches:
- directly--by walking "walking" Python functions through a graph--and
- pattern-matching--via some form of domain-specific language.

Using a direct approach, it can be very straight-forward to implement simple
search-and-replace objectives.  However, for modeling non-trivial systems
of logic and mathematical frameworks--like probability theory, different
algebras, and properties of function spaces--a direct approach will quickly
become unscalable, error-prone, and impossibly difficult to maintain.

The work we intend to do makes implicit use of nominal logic, field axioms,
and aspects of the typed lambda calculus--among other things.

Let's illustrate some of these points using an example.

:EXAMPLE:
We demonstrate automatic model reformulation using the variance expansion in
Equation [[eqref:eq:hs_model]] in [[citet:scott_parameter_2010]].

This expansion works pushing the half-Cauchy variance term, $\tau$, out of the
normal, $\beta$:
\begin{equation}
  \begin{aligned}
    Y &\sim \mathop{\text{N}}\nolimits\left(\beta, 1\right)
    \\
    \beta &\sim \tau \cdot \mathop{\text{N}}\nolimits\left(0, 1\right)
    \\
    \tau &\sim \mathop{\text{C}^{+}}\nolimits\left(0, 1\right)
    \;.
  \end{aligned}
\label{eq:norm_var_sink}
\end{equation}

An applicable reformulation rule might look like the following:
\begin{equation}
  \begin{aligned}
    \mathop{\text{N}}\nolimits\left(a m, a^2 C\right)
    &\to a \mathop{\text{N}}\nolimits\left(m, C\right)
  \end{aligned}
\label{eq:norm_replacement_exa}
\;.
\end{equation}
:END:

Now, what about an expression like $\mathop{\text{N}}\nolimits\left(0,
a^2\right)$; should our replacement in Equation [[eqref:eq:norm_replacement_exa]] produce
$a \mathop{\text{N}}\nolimits\left(0, 1\right)$?

Conventional mathematical reasoning might say so, but a term rewriting system
will need to be designed to account for this explicitly.  The important
questions have more to do with *where* and *how*: by requiring the addition of all
necessary replacements, or by automatic reasoning using a relatively small set of
axioms from which they can be derived?

For instance, we could provide some rules that represent group/ring/field axioms
(e.g. identity and zero elements) and allow such a system to consider them in
tandem.

For ease-of-use--especially with respect to developers working at the
mathematics-level--it's more desirable to formulate rules at a high level
(e.g. set and measure theory) and use a system with a generalized means
of verifying and deriving these expectations.
Even for general scalability, testing, and development, it can be much better to
focus on well compartmentalized pieces of such a system that have direct
mappings to well understood subjects.

While such solutions are possible, they're not trivial to implement *correctly*.
It shouldn't be surprising that there are very deep bodies of research on such
systems--usually under the subjects *term rewriting* and *symbolic computation*.
A good part of classical AI focused on the challenges induced by these automation
objectives and their implementations.

This doesn't mean--however--that they're prohibitively difficult to use or develop.
As demonstrated in [[citet:WillardRandomVariablesTheano2018]], Theano provides
some pattern-based graph manipulation using a form of unification.  This functionality
shares some of the same fundamental abstraction(s) as the more sophisticated systems
alluded to earlier, but it starts to fall short right where our objectives get started.

In the following, we'll demonstrate the critical short-comings, and introduce some
steps further into the direction of modern unification and term rewriting.
# [[citet:ByrdRelationalProgrammingminiKanren2009]] [[citet:RocklinlogpyLogicProgramming2018]]
# [[citet:WillardRoleSymbolicComputation2017]]
* A Language for Graph Manipulation
In this case, graph manipulation mostly consists of term rewriting in the
context of a Theano graphs, and--as we've stated earlier--the term rewriting is
mostly driven by algebraic considerations.

The mechanical aspects of this work is largely generalizable in terms of
orchestrated unification.  In the lead-up articles
[[citet:WillardRandomVariablesTheano2018]] and
[[citet:WillardSymbolicMathPyMC32018]], we used src_python{PatternSub}, which
uses unification (and reification) to implement pattern matching and
substitution (i.e. rewrite rules).

:REMARK:
In the context of Python, this sort of work has some fundamental limitations and
unnecessarily confusing aspects.  Python doesn't lend itself to symbolic manipulation,
making things like expression manipulation and traversal particularly onerous.
:END:

Beyond some small technical issues, src_python{PatternSub} only provides a
limited form of unification, and doesn't provide a programmable context for
controlling exactly how and when the unification is performed.

:EXAMPLE:
Let's attempt to implement the replacement in Equation [[eqref:eq:norm_var_sink]]
using src_python{PatternSub}.

#+ATTR_LATEX: :float nil
#+CAPTION: A naively specified Horseshoe model.
#+NAME: hs-model
#+BEGIN_SRC python :exports none :results silent
size_Y_rv = theano.shared(1)
tau_rv = CauchyRV(0, 1, name='\\tau')
beta_stddev = tt.abs_(tau_rv)
beta_rv = NormalRV(0, beta_stddev, name='\\beta')
Y_rv = NormalRV(beta_rv, 1, size=size_Y_rv, name='Y')
#+END_SRC

#+NAME: hs-var-expansion-opt-setup
#+BEGIN_SRC python :exports none :results silent :noweb strip-export
<<mcmc-requirements>>
#+END_SRC

#+NAME: hs_var_expansion_opt
#+BEGIN_SRC python :results output :noweb yes :wrap "SRC python :eval never"
<<hs-model>>

norm_sink_pats = [
    # N(0, a^2) -> a N(0, 1)
    tt.gof.opt.PatternSub(
        (NormalRV, 'b_x', 'a_x', 'size_x', 'rng_x'),
        (tt.mul, 'a_x',
         (NormalRV, 'b_x', tt.constant(1.0), 'size_x', 'rng_x'))),
]

norm_sink_opts = tt.gof.opt.EquilibriumOptimizer(
    norm_sink_pats, max_use_ratio=10)
hs_Y_graph = FunctionGraph(tt_inputs([Y_rv]), [Y_rv])
hs_Y_graph_opt = hs_Y_graph.clone()

_ = norm_sink_opts.optimize(hs_Y_graph_opt)
#+END_SRC

We see in Listing [[hs_var_expansion_opt]] that moving from a node that produces two outputs
(i.e. a src_python{RandomVariable} outputs the symbolic RNG *and* a tensor from
the sample space) to one that produces only a single output (i.e. a product)
result in an error.  A work-around for this doesn't seem possible, and a fix for
src_python{PatternSub.transform} is most likely necessary.

#+RESULTS: hs_var_expansion_opt
#+begin_SRC python :eval never
/tmp/user/1000/babel-t1fT37/python-tOxLaN in <module>()
     18 hs_Y_graph_opt = hs_Y_graph.clone()
     19
---> 20 _ = norm_sink_opts.optimize(hs_Y_graph_opt)

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in optimize(self, fgraph, *args, **kwargs)
     95             orig = theano.tensor.basic.constant.enable
     96             theano.tensor.basic.constant.enable = False
---> 97             ret = self.apply(fgraph, *args, **kwargs)
     98         finally:
     99             theano.tensor.basic.constant.enable = orig

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in apply(self, fgraph, start_from)
   2511                         nb = change_tracker.nb_imported
   2512                         t_opt = time.time()
-> 2513                         lopt_change = self.process_node(fgraph, node, lopt)
   2514                         time_opts[lopt] += time.time() - t_opt
   2515                         if not lopt_change:

~/apps/anaconda3/envs/github-website/lib/python3.6/site-packages/theano/gof/opt.py in process_node(self, fgraph, node, lopt)
   2056         if len(old_vars) != len(replacements):
   2057             raise ValueError('Optimizer %s gave wrong number of replacements'
-> 2058                              % lopt)
   2059         # None in the replacement mean that this variable isn't used
   2060         # and we want to remove it

ValueError: Optimizer normal_rv(b_x, a_x, size_x, rng_x) -> Elemwise{mul,no_inplace}(a_x, normal_rv(b_x, TensorConstant{1.0}, size_x, rng_x)) gave wrong number of replacements


#+end_SRC
:END:

The miniKanren [[citep:ByrdminiKanrenorg2019]] domain-specific language (DSL)
provides an abstraction within which a majority of the relevant term rewriting
ideas are neatly organized and implementable.  While most miniKanren
implementations are expressed seamlessly in a Lisp-like language, the abstractions are not
limited to Lisp, and there are implementations in nearly every major language.
Here, we will use a pure Python implementation provided by the
package src_python{kanren} [[citep:RocklinlogpyLogicProgramming2018]] .

To get started, we'll create a new src_python{theano.gof.opt.LocalOptimizer} that is
driven by miniKanren results.

* A miniKanren Theano Optimizer
:PROPERTIES:
# :header-args: :noweb-ref theano-minikanren-opt
:END:

As in the example Hy compiler from
[[citet:WillardReadableStringsRelational2018a]], we need to specify how
unification occurs in the context of Theano objects.

Throughout, we'll make extensive use of multiple-dispatch (implemented by the
Python library src_python{multipledispatch}
[[citep:RocklinMultipledispatchContribute2019]]).

#+NAME: minikanren-opt-imports
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
from collections import Callable
from warnings import warn

import numpy as np

import theano
import theano.tensor as tt

from theano.printing import debugprint as tt_dprint

from kanren import var, run, eq, conde, lall, fact, Relation, isvar
from kanren.core import success, fail

from kanren.term import term, operator, arguments
from kanren.assoccomm import eq_assoccomm, eq_assoc, eq_comm
from kanren.assoccomm import commutative, associative

from unification import variables
from unification.core import unify, reify, _unify, _reify
from unification.more import unify_object

from theano.tensor import Elemwise
from theano.scalar.basic import mul, add

from multipledispatch import dispatch
#+END_SRC

** Theano Graph Unification and Reification

In the following, we implement unification using src_python{unification}--itself
using src_python{multipledispatch}.

We start by defining meta objects that wrap the existing Theano graph objects.
Using these meta objects, we can create graphs containing partially constructed
objects--or logic variables--as well as define our own graph orderings and
normal/canonical forms.

#+NAME: theano-meta-objects
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import abc

# TODO: Replace `from_obj` with a dispatched function?
# from multipledispatch import dispatch


def meta_reify_all(rands):
    # We want as many of the rands reified as possible,
    any_unreified = False
    reified_rands = []
    for s in rands:
        if isinstance(s, MetaSymbol):
            rrand = s.reify()
            reified_rands += [rrand]
            any_unreified |= isinstance(rrand, MetaSymbol)
            any_unreified |= isvar(rrand)
        elif MetaSymbol.is_meta(s):
            reified_rands += [s]
            any_unreified |= True
        else:
            reified_rands += [s]

    return reified_rands, any_unreified


class MetaSymbol(abc.ABC):
    """Meta objects for unification and such.
    """
    # TODO: Consider automatically registering base types.
    # Might need to make this a `type`.
    #
    # def __new__(cls, base, **kwargs):
    #     cls.register(base)
    #     res = object.__new__(cls)
    @property
    @abc.abstractmethod
    def base(self):
        """The base type/rator for this meta object."""
        pass

    @classmethod
    def base_classes(cls, mro_order=True):
        res = tuple(c.base for c in cls.__subclasses__())
        if cls is not MetaSymbol:
            res = (cls.base,) + res
        sorted(res, key=lambda cls: len(cls.mro()), reverse=mro_order)
        return res

    @classmethod
    def is_meta(cls, obj):
        return isinstance(obj, MetaSymbol) or isvar(obj)

    @classmethod
    def from_obj(cls, obj):
        """Create a meta object for a given base object.

        XXX: Be careful when overriding this: `isvar` checks are necessary!
        """
        if cls.is_meta(obj) or obj is None:
            return obj

        if isinstance(obj, (list, tuple)):
            # Convert elements of the iterable
            return type(obj)([cls.from_obj(o) for o in obj])

        if not isinstance(obj, cls.base_classes()):
            # We might've been given something convertible to a type with a
            # meta type, so let's try that
            try:
                obj = tt.as_tensor_variable(obj)
            except (ValueError, tt.AsTensorError):
                pass

            # Check for a meta type again
            if not isinstance(obj, cls.base_classes()):
                raise ValueError(
                    'Could not find a MetaSymbol class for {}'.format(obj))

        try:
            obj_cls = next(filter(lambda t: isinstance(obj, t.base),
                                  cls.__subclasses__()))
        except StopIteration:
            res = cls(*[getattr(obj, s)
                        for s in getattr(cls, '__slots__', [])],
                      obj=obj)
        else:
            # Descend into this class to find a more suitable one, if any.
            res = obj_cls.from_obj(obj)

        return res

    def __init__(self, obj=None):
        self.obj = obj

    def rands(self):
        """Create a tuple of the meta object's operator parameters (i.e. "rands").
        """
        return tuple(getattr(self, s)
                     for s in getattr(self, '__slots__', []))

    def reify(self):
        """Create a concrete base object from this meta object (and its
        rands).
        """
        if self.obj is not None:
            return self.obj
        else:
            reified_rands, any_unreified = meta_reify_all(self.rands())

            # If not all the rands reified, then create another meta
            # object--albeit one with potentially more non-`None` `obj` fields.
            rator = self.base if not any_unreified else type(self)
            res = rator(*reified_rands)

            if not any_unreified:
                self.obj = res

            return res

    def __eq__(self, other):
        """Syntactic equality between meta objects and their bases.
        """
        res = False
        if ((type(self) == type(other) and
             self.base == other.base) or
                # Compare against base objects, as well
                self.base == type(other)):
            if hasattr(self, '__slots__') and self.__slots__:
                # Are all the object rands equal?
                res = all(getattr(self, attr) == getattr(other, attr)
                          for attr in self.__slots__)
            # TODO: Do we want these?  They're a bit limiting, since
            # reified objects can construct their `obj`s and those
            # won't be equal to other--potentially equivalent--base
            # objects.
            # elif self.base == type(other) and hasattr(self, 'obj'):
            #     # Is our associated concrete object equal to the base object?
            #     res = self.obj == other
            elif hasattr(self, 'obj') and hasattr(other, 'obj'):
                res = self.obj == other.obj
            else:
                # Are the objects identical?
                res = self is other
        return res

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        def _make_hashable(x):
            if isinstance(x, list):
                return tuple(x)
            elif isinstance(x, np.ndarray):
                return x.data.tobytes()
            else:
                return x
        return hash(tuple(_make_hashable(p) for p in self.rands()))

    def __str__(self):
        if self.obj is None:
            params = self.rands()
            args = ', '.join([str(p) for p in params])
            res = '{}({})'.format(self.__class__.__name__, args)
        else:
            res = str(self.obj)
        return res

    def __repr__(self):
        args = ', '.join([repr(p) for p in self.rands()] +
                         ['obj={}'.format(repr(self.obj))])
        return '{}({})'.format(
            self.__class__.__name__, args)


class MetaType(MetaSymbol):
    base = theano.Type


class MetaRandomStateType(MetaType):
    base = tt.raw_random.RandomStateType


class MetaTensorType(MetaType):
    base = tt.TensorType
    __slots__ = ['dtype', 'broadcastable', 'name']

    def __init__(self, dtype, broadcastable, name, obj=None):
        super().__init__(obj=obj)
        self.dtype = dtype
        self.broadcastable = broadcastable
        self.name = name


class MetaOp(MetaSymbol):
    base = tt.Op

    def __call__(self, *args, ttype=None, index=None, name=None):
        """Emulate `make_node` for this `Op` and return .

        This will fill-in missing/unreifiable parts of the output variable with
        logic variables.
        """
        res_apply = MetaApply(self, args)
        tt_apply = res_apply.reify()
        if not self.is_meta(tt_apply):
            return MetaVariable.from_obj(tt_apply.default_output())
        # TODO: Will this correctly associate the present meta `Op`
        # and its components with the resulting meta variable?
        # How about when `tt_apply` is fully reified?

        # TODO: Elemewise has an `output_types` method that can be
        # used to infer the output type of this variable.
        ttype = ttype or var()
        index = index if index is not None else var()
        name = name
        res_var = MetaVariable(ttype, tt_apply, index, name)
        return res_var


class MetaElemwise(MetaOp):
    base = tt.Elemwise

    def __call__(self, *args, ttype=None, index=None, name=None):
        obj_nout = getattr(self.obj, 'nfunc_spec', [None])[-1]
        if obj_nout == 1 and index is None:
            index = 0
        return super().__call__(*args, ttype=ttype, index=index, name=name)


class MetaApply(MetaSymbol):
    base = tt.Apply
    __slots__ = ['op', 'inputs']

    def __init__(self, op, inputs, outputs=None, obj=None):
        super().__init__(obj=obj)
        self.op = MetaOp.from_obj(op)
        self.inputs = tuple(MetaSymbol.from_obj(i) for i in inputs)
        self.outputs = outputs

    def reify(self):
        if getattr(self, 'obj', None):
            return self.obj
        else:
            tt_op = self.op.reify()
            if not self.is_meta(tt_op):
                reified_rands, any_unreified = meta_reify_all(self.inputs)
                if not any_unreified:
                    tt_var = tt_op(*reified_rands)
                    self.obj = tt_var.owner
                    return tt_var.owner
            return self


class MetaVariable(MetaSymbol):
    base = theano.Variable
    __slots__ = ['type', 'owner', 'index', 'name']

    def __init__(self, type, owner, index, name, obj=None):
        super().__init__(obj=obj)
        self.type = MetaType.from_obj(type)
        self.owner = MetaApply.from_obj(owner)
        self.index = index
        self.name = name

    def reify(self):
        if getattr(self, 'obj', None):
            return self.obj

        if not self.owner:
            return super().reify()

        # Having an `owner` causes issues (e.g. being consistent about
        # other, unrelated outputs of an `Apply` node), and, in this case,
        # the `Apply` node that owns this variable needs to construct it.
        reified_rands, any_unreified = meta_reify_all(self.rands())
        tt_apply = self.owner.obj

        if tt_apply:
            # If the owning `Apply` reified, then one of its `outputs`
            # corresponds to this variable.  Our `self.index` value should
            # tell us which, but, when that's not available, we can
            # sometimes infer it.
            if tt_apply.nout == 1:
                tt_index = 0
                # Make sure we didn't have a mismatched non-meta index value.
                assert (isvar(self.index) or
                        self.index is None or
                        self.index == 0)
                # Set/replace `None` or meta value
                self.index = 0
                tt_var = tt_apply.outputs[tt_index]
            elif not self.is_meta(self.index):
                tt_var = tt_apply.outputs[self.index]
            elif self.index is None:
                tt_var = tt_apply.default_output()
                self.index = tt_apply.outputs.index(tt_var)
            else:
                return self
            # If our name value is not set/concrete, then use the reified
            # value's.  Otherwise, use ours.
            if isvar(self.name) or self.name is None:
                self.name = tt_var.name
            else:
                tt_var.name = self.name
            self.obj = tt_var
            return tt_var
        return super().reify()


class MetaTensorVariable(MetaVariable):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.TensorVariable


class MetaConstant(MetaVariable):
    base = theano.Constant
    __slots__ = ['type', 'data']

    def __init__(self, type, data, name=None, obj=None):
        super().__init__(type, None, None, name, obj=obj)
        self.data = data


class MetaTensorConstant(MetaConstant):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.TensorConstant
    __slots__ = ['type', 'data', 'name']

    def __init__(self, type, data, name=None, obj=None):
        super().__init__(type, data, name, obj=obj)


class MetaSharedVariable(MetaVariable):
    base = tt.sharedvar.SharedVariable
    __slots__ = ['name', 'type', 'data', 'strict']

    @classmethod
    def from_obj(cls, obj):
        if isvar(obj):
            return obj
        res = cls(obj.name, obj.type, obj.container.data, obj.container.strict,
                  obj=obj)
        return res

    def __init__(self, name, type, data, strict, obj=None):
        super().__init__(type, None, None, name, obj=obj)
        self.data = data
        self.strict = strict


class MetaTensorSharedVariable(MetaSharedVariable):
    # TODO: Could extend `theano.tensor.var._tensor_py_operators`, too.
    base = tt.sharedvar.TensorSharedVariable


class MetaScalarSharedVariable(MetaSharedVariable):
    base = tt.sharedvar.ScalarSharedVariable
#+END_SRC

Just to make life a little bit easier, we create a mock analog of the module
alias src_python{tt} in Listing [[theano-meta-accessor]].
#+NAME: theano-meta-accessor
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import types


class TheanoMetaAccessor(object):
    def __getattr__(self, obj):
        print('getattr')
        theano_obj = getattr(tt, obj)
        if isinstance(theano_obj, types.FunctionType):
            @staticmethod
            def meta_obj(*args, **kwargs):
                res = theano_obj(*args, **kwargs)
                return MetaSymbol.from_obj(res)
        else:
            meta_obj = MetaSymbol.from_obj(theano_obj)

        setattr(TheanoMetaAccessor, obj, meta_obj)

        return getattr(TheanoMetaAccessor, obj)

mt = TheanoMetaAccessor()
#+END_SRC

In Listing [[theano-object-unify]] we create dispatch functions so that unification
and reification works with our Theano meta object classes and ordinary Theano
objects themselves.

#+NAME: theano-object-unify
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
tt_class_abstractions = tuple(c.base for c in MetaSymbol.__subclasses__())


def unify_MetaSymbol(u, v, s):
    # We need this, because `unify_object` only checks the object
    # types and unifies the `__slots__` (or `__dict__`) attributes.
    # Those steps miss the case when objects are equal (and unifiable)
    # based on identity or other object-level criteria (e.g. other non-
    # `__slots__` attributes).
    if u == v:
        return s
    # We want to unify subclasses against their base classes,
    # since we sometimes can't say exactly which type an meta
    # object should be (e.g. a `tt.Variable` or `tt.TensorVariable`).
    u_sub_v = isinstance(u, type(v))
    v_sub_u = isinstance(v, type(u))
    if not (u_sub_v or v_sub_u):
        return False

    # If they both don't have the same slots, then they won't
    # unify.  That's restrictive in some ways, but more reasonable in others.
    u_slots = getattr(u, '__slots__', [])
    v_slots = getattr(v, '__slots__', [])
    if u_slots or v_slots:
        return unify([getattr(u, slot) for slot in u_slots],
                    [getattr(v, slot) for slot in v_slots],
                    s)
    return u.obj == v.obj and s


_unify.add((MetaSymbol, MetaSymbol, dict), unify_MetaSymbol)
_unify.add((MetaSymbol, tt_class_abstractions, dict),
           lambda u, v, s: unify_MetaSymbol(u, MetaSymbol.from_obj(v), s))
_unify.add((tt_class_abstractions, MetaSymbol, dict),
           lambda u, v, s: unify_MetaSymbol(MetaSymbol.from_obj(u), v, s))
_unify.add((tt_class_abstractions, tt_class_abstractions, dict),
           lambda u, v, s: unify_MetaSymbol(MetaSymbol.from_obj(u),
                                            MetaSymbol.from_obj(v), s))


def _reify_MetaSymbol(o, s):
    # `o.obj` could be a Theano object, but it could also be a logic variable,
    # in which case the `rands` should not be the same.
    # TODO: Seems like we could short-circuit some of the reification when
    # `o.obj` is present.
    rands = o.rands()
    new_rands = reify(rands, s)
    if rands == new_rands:
        return o
    else:
        newobj = type(o)(*new_rands)
        return newobj


_reify.add((MetaSymbol, dict), _reify_MetaSymbol)


def _reify_TheanoClasses(o, s):
    meta_obj = MetaSymbol.from_obj(o)
    return reify(meta_obj, s)


_reify.add((tt_class_abstractions, dict), _reify_TheanoClasses)


_isvar = isvar.resolve((object,))

isvar.add((MetaSymbol,), lambda x: _isvar(x) or isvar(x.obj))
#+END_SRC

The additions in Listing [[theano-object-terms]] create dispatch functions
for src_python{kanren.term.operator} and src_python{kanren.term.arguments},
which allow us to use some algebraically aware forms of
unification--like src_python{kanren.assoccomm.eq_assoccomm} (i.e. associative
and commutative equality/unification).
#+NAME: theano-object-terms
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
def operator_MetaVariable(x):
    # Get an apply node, if any
    x_owner = getattr(x, 'owner', None)
    if x_owner and hasattr(x_owner, 'op'):
        return x_owner.op
    return None


operator.add((MetaVariable,), operator_MetaVariable)
operator.add((tt.Variable,), lambda x: operator(MetaVariable.from_obj(x)))


def arguments_MetaVariable(x):
    # Get an apply node, if any
    x_owner = getattr(x, 'owner', None)
    if x_owner and hasattr(x_owner, 'op'):
        return x_owner.inputs
    return None


arguments.add((MetaVariable,), arguments_MetaVariable)
arguments.add((tt.Variable,), lambda x: arguments(MetaVariable.from_obj(x)))

# Enable [re]construction of terms
term.add((tt.Op, (list, tuple)), lambda op, args: term(MetaOp.from_obj(op), args))
term.add((MetaOp, (list, tuple)), lambda op, args: op(*args))

meta_add = MetaOp.from_obj(tt.add)
meta_mul = MetaOp.from_obj(tt.mul)
meta_inv = MetaOp.from_obj(tt.inv)
meta_sub = MetaOp.from_obj(tt.sub)

fact(commutative, meta_add)
fact(commutative, meta_mul)
fact(associative, meta_add)
fact(associative, meta_mul)
#+END_SRC

** Testing                                                        :noexport:

Listing [[theano-object-tools]] provides a high-level form of graph object comparison
(i.e. one that isn't point-equality-like).  This is especially useful during testing,
and whenever we aren't concerned with objects being strictly identical.

#+NAME: theano-object-tools
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
from collections import OrderedDict


to_meta = MetaSymbol.from_obj

def expand_meta(x, tt_print=tt.pprint):
    if isinstance(x, MetaSymbol):
        return OrderedDict([('rator', x.base),
                            ('rands', tuple(expand_meta(p)
                                            for p in x.rands())),
                            ('obj', expand_meta(getattr(x, 'obj', None)))])
    elif tt_print and isinstance(x, theano.gof.op.Op):
        return x.name
    elif tt_print and isinstance(x, theano.gof.graph.Variable):
        return tt_print(x)
    else:
        return x


def graph_equal(x, y):
    """Compare elements in a Theano graph using their object properties and not
    just identity.
    """
    try:
        if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):
            return (len(x) == len(y) and
                    all(MetaSymbol.from_obj(xx) == MetaSymbol.from_obj(yy)
                        for xx, yy in zip(x, y)))
        return MetaSymbol.from_obj(x) == MetaSymbol.from_obj(y)
    except ValueError:
        return False

#+END_SRC

#+NAME: theano-meta-classes-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_meta_classes():
    vec_tt = tt.vector('vec')
    vec_m = MetaSymbol.from_obj(vec_tt)
    assert vec_m.obj == vec_tt
    assert type(vec_m) == MetaTensorVariable

    vec_type_m = vec_m.type
    assert type(vec_type_m) == MetaTensorType
    assert vec_type_m.dtype == vec_tt.dtype
    assert vec_type_m.broadcastable == vec_tt.type.broadcastable
    assert vec_type_m.name == vec_tt.type.name

    meta_add = MetaElemwise(tt.add)
    assert graph_equal(tt.add(1, 2), meta_add(1, 2).reify())

    meta_var = meta_add(1, var()).reify()
    # TODO: Would be better if was `MetaTensorVariable`.
    assert isinstance(meta_var, MetaVariable)
    assert isinstance(meta_var.owner.op.obj, theano.Op)
    assert isinstance(meta_var.owner.inputs[0].obj, tt.TensorConstant)

    test_vals = [1, 2.4]
    meta_vars = MetaSymbol.from_obj(test_vals)
    assert meta_vars == [tt.as_tensor_variable(x) for x in test_vals]


test_meta_classes()
#+END_SRC

#+NAME: theano-unification-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_unification():
    x, y, a, b = tt.dvectors('xyab')
    x_s = tt.scalar('x_s')
    y_s = tt.scalar('y_s')
    c = tt.constant(1, 'c')
    d = tt.constant(2, 'd')
    x_l = tt.vector('x_l')
    y_l = tt.vector('y_l')
    z_l = tt.vector('z_l')

    with variables(x_l):
        assert a == reify(x_l, {x_l: a})
        test_expr = 1 + 2 * x_l
        test_reify_res = reify(test_expr, {x_l: a})
        assert graph_equal(test_reify_res, 1 + 2*a)

    with variables(x_l):
        z = tt.add(b, a)
        assert {x_l: z} == unify(x_l, z)
        assert {x_l: b} == unify(tt.add(x_l, a), tt.add(b, a))

    with variables(x_l, y_l):
        assert {x_l: b, y_l: a} == unify(1/tt.add(x_l, a), 1/tt.add(b, y_l))

    with variables(x):
        assert unify(x, b)[x] == b
        assert unify([x], [b])[x] == b
        assert unify((x,), (b,))[x] == b
        assert unify(x + 1, b + 1)[x] == b
        assert unify(x + a, b + a)[x] == b

    with variables(x):
        assert unify(a + b, a + x)[x] == b

    with variables(x):
        assert b == next(eq(a + b, a + x)({}))[x]

    # Generalize unification for an `Op` over `TensorTypes`
    x_lvar = var('x_lvar')
    y_lvar = var('y_lvar')

    mt_expr_add = mt.add(x_lvar, y_lvar)

    # The parameters are vectors
    tt_expr_add_1 = tt.add(x, y)
    assert graph_equal(tt_expr_add_1,
                       reify(mt_expr_add,
                             unify(mt_expr_add, tt_expr_add_1)).reify())

    # The parameters are scalars
    tt_expr_add_2 = tt.add(x_s, y_s)
    assert graph_equal(tt_expr_add_2,
                       reify(mt_expr_add,
                             unify(mt_expr_add, tt_expr_add_2)).reify())

    # The parameters are constants
    tt_expr_add_3 = tt.add(c, d)
    assert graph_equal(tt_expr_add_3,
                       reify(mt_expr_add, unify(mt_expr_add, tt_expr_add_3)).reify())


test_unification()
#+END_SRC

#+NAME: theano-term-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_terms():
    x, a, b = tt.dvectors('xab')
    test_expr = x + a * b

    assert test_expr.owner.op == operator(test_expr)
    assert test_expr.owner.inputs == arguments(test_expr)
    assert graph_equal(test_expr, term(operator(test_expr), arguments(test_expr)))
#+END_SRC

#+NAME: theano-kanren-tests
#+BEGIN_SRC python :exports none :results silent :noweb-ref theano-minikaren-opt
def test_kanren():
    x, a, b = tt.dvectors('xab')

    with variables(x):
        assert b == run(1, x, eq(a + b, a + x))[0]
        assert b == run(1, x, eq(a * b, a * x))[0]


test_kanren()
#+END_SRC

#+HEADER: :noweb-ref theano-minikaren-opt
#+NAME: theano-assoccomm-tests
#+BEGIN_SRC python :exports none :results silent
def test_assoccomm():
    from kanren.assoccomm import buildo

    x, a, b, c = tt.dvectors('xabc')
    test_expr = x + 1
    q = var('q')

    assert q == run(1, q, buildo(tt.add, test_expr.owner.inputs, test_expr))[0]
    assert tt.add == run(1, q, buildo(q, test_expr.owner.inputs, test_expr))[0].reify()
    assert graph_equal(tuple(test_expr.owner.inputs), run(1, q, buildo(tt.add, q, test_expr))[0])

    with variables(x):
        assert (to_meta(a),) == run(0, x, (eq_comm, to_meta(a * b), to_meta(b * x)))
        assert (to_meta(a),) == run(0, x, (eq_comm, to_meta(a + b), to_meta(b + x)))

    # XXX: This only works when the nested `Op`s have been collapsed
    # (i.e. after canonization--and a `+ 0`/`* 1` for the currently broken
    # Theano) See https://github.com/Theano/Theano/pull/6686
    with variables(x):
        res = run(0, x, (eq_assoc, to_meta(tt.add(a, b, c)), to_meta(tt.add(a, x))))
        assert graph_equal(res[0], b + c)
        res = run(0, x, (eq_assoc, to_meta(tt.mul(a, b, c)), to_meta(tt.mul(a, x))))
        assert graph_equal(res[0], b * c)


test_assoccomm()
#+END_SRC

** miniKanren Relations

Now that we're able to unify objects, src_python{kanren} relations should work
on Theano graphs.  We'll start with an example of some simple algebraic
simplifications and a miniKanren goal that applies them to a Theano graph object.

Listing [[kanren-reduces-relation]] creates a set of relations in miniKanren that
succinctly generalize a few algebraic and arithmetic properties.
In this instance, the relations--expressed as miniKanren goals--are indirectly
applied through the use of a src_python{Relation} object, which serves as a
more efficient means of defining and applying simple replacement rules.

#+NAME: kanren-reduces-relation
#+BEGIN_SRC python :exports code :results silent
reduces = Relation('reduces')

x_lvar = var('x_lvar')
y_lvar = var('y_lvar')
z_lvar = var('z_lvar')


def mt_type_params(x):
    return {'ttype': x.type, 'index': x.index, 'name': x.name}


# x + x -> 2 * x
x_add_mt = mt.add(x_lvar, x_lvar)
fact(reduces,
     x_add_mt,
     mt.mul(tt.constant(2), x_lvar, **mt_type_params(x_add_mt)))
# x * x -> x**2
pow_sum_mt = mt.mul(x_lvar, x_lvar)
fact(reduces,
     pow_sum_mt,
     mt.pow(x_lvar, tt.constant(2), **mt_type_params(pow_sum_mt)))
# -(-x) -> x
fact(reduces,
     mt.neg(mt.neg(x_lvar)),
     x_lvar)
# exp(log(x)) -> x
fact(reduces,
     mt.exp(mt.log(x_lvar)),
     x_lvar)
# log(exp(x)) -> x
fact(reduces,
     mt.log(mt.exp(x_lvar)),
     x_lvar)
# x**y * x**z -> x**(y + z)
pow_mul_mt = mt.mul(mt.pow(x_lvar, y_lvar),
                    mt.pow(x_lvar, z_lvar))
fact(reduces,
     pow_mul_mt,
     mt.pow(x_lvar,
            mt.add(y_lvar, z_lvar,
                   ,**mt_type_params(pow_mul_mt.owner.inputs[0]))))
#+END_SRC

:REMARK:
When we create a meta Theano variable using something
like src_python{mt.add(x_lvar, x_lvar)}, the result has logic variables in place
of the unknown tensor type, output index, and name values.
If we want the replacement terms to be fully determined by their matching
antecedent, we have to reference those variables in the replacement pattern.
This is the reason for src_python{mt_type_params}; it simply extracts those
variables and uses them in the replacement.
:END:

#+NAME: kanren-project-goal
#+BEGIN_SRC python :exports none :results none :noweb-ref theano-minikaren-opt
def project(vars, body_func):
    "A goal constructor for projecting logic variables."
    def goal(s):
        proj_vars = reify(vars, s)
        body_func(proj_vars)
        yield s
    return goal
#+END_SRC

# TODO: Add checks for extrema.

A goal for the reduction process is given in Listing [[kanren-reduce-goal]].  It is
a recursive goal that evaluates a single
#+NAME: kanren-reduce-goal
#+BEGIN_SRC python :exports code :results silent
def kanren_reduce(input_expr, n=0):
    def _reduce(in_expr, out_expr):
        expr_rdcd = var()
        return (conde,
                # Attempt to apply a single reduction
                [(reduces, in_expr, expr_rdcd),
                 # If it succeeds, consider another
                 (_reduce, expr_rdcd, out_expr)],
                # Return the input unchanged
                [eq(out_expr, in_expr)])

    reduced_expression = var()
    res = run(n, reduced_expression,
              (_reduce, input_expr, reduced_expression))

    return res
#+END_SRC

#+NAME: kanren-relation-tests
#+BEGIN_SRC python :exports none :results silent
def test_kanren_relation():
    a = tt.vector('a')

    def reify_all(x):
        if isinstance(x, (tuple, list)):
            return type(x)([r.reify() for r in x])
        return x.reify()

    # XXX: Expressions like `2*a` don't actually have inputs `2` and `a`;
    # They have inputs like `InplaceDimShuffle`d `2` and `a`, which won't be
    # properly represented by an equivalent meta object with inputs `2` and
    # `a`.
    # If we reify such meta objects, then the resulting object's inputs should
    # match.
    assert graph_equal((2*a, a + a), reify_all(kanren_reduce(a + a)))
    assert graph_equal((a**2, a * a), reify_all(kanren_reduce(a * a)))
    assert graph_equal((a, tt.log(tt.exp(a))), reify_all(kanren_reduce(tt.log(tt.exp(a)))))
    assert graph_equal((a, tt.exp(tt.log(a))), reify_all(kanren_reduce(tt.exp(tt.log(a)))))


test_kanren_relation()
#+END_SRC

:EXAMPLE:
One advantage to using miniKanren as a means of specifying rewrite rules, is
that it provides a stream of all possible replacements.

For example, Equation [[eqref:eq:kanren-reduce-example]] shows all the replacement
results for $x^{2} x^{2}$, which includes the original expression, combined
powers, and a squaring.

#+NAME: kanren-reduce-example
#+BEGIN_SRC python :eval never-export :exports both :results output scalar raw replace
import textwrap


tt_tex_options = {'latex': True, 'latex_aligned': True}

x = tt.vector('x')
exa_expr = x**2 * x**2

results = '\n\\\\\n'.join([
    '&=' + tt_tex_pprint(s.reify(), tt_tex_options)
    for s in kanren_reduce(exa_expr)
    if not graph_equal(exa_expr, s)
])

print("""
\\begin{{equation*}}
\\begin{{aligned}}
    {} &=
    \\\\
{}
\\end{{aligned}}
\\label{{eq:kanren-reduce-example}}
\\end{{equation*}}
""".format(tt_tex_pprint(exa_expr, tt_tex_options).strip('()'),
           textwrap.indent(results, '\t\t')))
#+END_SRC

#+RESULTS: kanren-reduce-example
\begin{equation*}
\begin{aligned}
    {x}^{2} \circ {x}^{2} &=
    \\
		&={{x}^{2}}^{2}
		\\
		&={x}^{(2 + 2)}
\end{aligned}
\label{eq:kanren-reduce-example}
\end{equation*}

:END:
** A miniKanren src_python{LocalOptimizer}

#+NAME: kanren-theano-opt-imports
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
import theano
from theano.gof import FunctionGraph, Feature, NodeFinder
from theano.gof.graph import inputs as tt_inputs, clone_get_equiv
from theano.gof.opt import LocalOptimizer, EquilibriumOptimizer
#+END_SRC

Listing [[kanren-theano-opt-class]] provides a src_python{LocalOptimizer} wrapper around
the src_python{kanren} functionality.

#+NAME: kanren-theano-opt-class
#+BEGIN_SRC python :exports code :results silent :noweb-ref theano-minikaren-opt
class KanrenRelationSub(LocalOptimizer):
    reentrant = True

    def __init__(self, kanren_relation, relation_lvars=None):
        """
        Parameters
        ==========
        kanren_relation: kanren.Relation or goal
            The miniKanren relation store or goal (taking input and output
            terms) to use.
        relation_lvars: Iterable
            A collection of term to be considered logic variables by miniKanren
            (e.g. Theano terms used in `kanren_relation`).
        """
        self.kanren_relation = kanren_relation
        self.relation_lvars = relation_lvars or []
        super().__init__()

    def transform(self, node):
        """
        TODO: Only uses *one* `run` result.
        """
        # TODO: Could do this with `self.tracks`?
        if not isinstance(node, tt.Apply):
            return False

        input_expr = node.default_output()

        with variables(*self.relation_lvars):
            q = var()
            res = run(1, q, (self.kanren_relation, input_expr, q))

        if len(res) > 0:
            new_node = res[0].reify()

            if MetaSymbol.is_meta(new_node):
                raise ValueError(
                    "Kanren results not fully reifiable: {}".format(new_node))

            # Handle (some) nodes with multiple outputs
            res = list(node.outputs)
            res[getattr(node.op, 'default_output', 0) or 0] = new_node
            return res
        else:
            return False

#+END_SRC

:EXAMPLE:
In Listing [[theano-optimize-helper]] we create a helper function that returns an
optimized version of its Theano tensor argument.

#+NAME: theano-optimize-helper
#+BEGIN_SRC python :exports code :results silent
def optimize_graph(x, optimization):
    if not isinstance(x, FunctionGraph):
        inputs = tt_inputs([x])
        outputs = [x]
        model_memo = clone_get_equiv(inputs, outputs,
                                     copy_orphans=False)
        cloned_inputs = [model_memo[i] for i in inputs]
        cloned_outputs = [model_memo[i] for i in outputs]

        x_graph = FunctionGraph(cloned_inputs, cloned_outputs, clone=False)
        x_graph.memo = model_memo
    else:
        x_graph = x

    x_graph_opt = x_graph.clone()
    optimization.optimize(x_graph_opt)
    return x_graph_opt.outputs[0]
#+END_SRC

Applying the reductions from Listing [[kanren-reduces-relation]], we see the rules applied in
succession--as expected.
#+NAME: theano-optimize-example
#+BEGIN_SRC python :exports code :results silent
reduces_opt = EquilibriumOptimizer([KanrenRelationSub(reduces)],
                                   max_use_ratio=10)

test_opt = optimize_graph(tt.log(tt.exp(a)), reduces_opt)
assert graph_equal(a, test_opt)

test_opt = optimize_graph(-tt.log(tt.exp(-a)), reduces_opt)
assert graph_equal(a, test_opt)
#+END_SRC
:END:
* MCMC Optimizations
With the full capabilities of miniKanren, we're better prepared to implement
general term rewriting rules for MCMC models.

In the following sections, we'll use some simple examples to demonstrate a few
term rewrites that are simple yet unautomated within most MCMC sampler
implementations.  These examples will also serve to further demonstrate some
relevant features of miniKanren.

** Simple Parameter Expansion
Let's re-attempt the replacement in Equation [[eqref:eq:norm_var_sink]].

#+NAME: kanren-normal-rescale-setup
#+BEGIN_SRC python :eval never-export :exports none :results silent :noweb strip-export
<<plotting-setup>>
<<mcmc-requirements>>
<<theano-minikaren-opt>>
<<theano-optimize-helper>>
#+END_SRC

#+NAME: kanren-normal-rescale-rule
#+BEGIN_SRC python :exports code :results silent :noweb yes
from unification.utils import transitive_get as walk


<<hs-model>>

mcmc_transforms = Relation('mcmc_transforms')

C_lvar = var('C_lvar')
name_lvar = var('name_lvar')
size_lvar = var('size_lvar')
rng_lvar = var('rng_lvar')
zero_const_lvar = MetaTensorConstant(var('zero_type'), 0, var('zero_name'))
one_const_lvar = MetaTensorConstant(var('zero_type'), 1)


mt.NormalRV = MetaOp.from_obj(NormalRV)
norm_scale_mt = mt.NormalRV(zero_const_lvar, C_lvar,
                            size_lvar, rng_lvar,
                            name=name_lvar)
type_lvars = mt_type_params(norm_scale_mt)

fact(mcmc_transforms,
     # N(0, a^2) -> a N(0, 1)
     norm_scale_mt,
     mt.mul(C_lvar,
            mt.NormalRV(zero_const_lvar, one_const_lvar,
                        size_lvar, rng_lvar,
                        **type_lvars)))
#+END_SRC

We have to make sure that the optimizer doesn't get caught in an endless
$1 \to 1 \cdot 1$ loop.  To do this, in Listing [[kanren-normal-rescale-guard]], we
set up a naive constraint that projects the current miniKanren state
(i.e. a src_python{dict} of logic variable replacements resulting from
unification) and checks that the unified value of src_python{C_lvar} is never
the identity element (i.e. ~1~).

#+NAME: kanren-normal-rescale-guard
#+BEGIN_SRC python :exports code :results silent :noweb yes
def not_eq(lvar, val):
    def _goal(s):
        lvar_val = walk(lvar, s)
        if isinstance(lvar_val, (tt.Constant, MetaConstant)):
            if lvar_val.data != val:
                yield s
        else:
            yield s
    return _goal


mcmc_goals = lambda x, y: (conde, ((mcmc_transforms, x, y),
                                   (not_eq, C_lvar, 1)))
#+END_SRC

The code in Listing [[kanren-normal-rescale-guard]] is also a great example of the
flexibility provided by the miniKanren framework.  In contrast to src_python{PatternSub},
we are now able to control a stream of unification results.

More specifically, in Listing [[kanren-normal-rescale-guard]], the
variable src_python{s} is the current state--produced by all goals that preceded
the present one--and, by not yielding the current state when our condition
fails, we have effectively said that unification fails.

While src_python{PatternSub} does provide nearly the same condition-checking
capability, it doesn't make the entire state of unification available, so--for
instance--one cannot easily write constraints that depend on the current value
of two logic variables.  Similarly, one cannot manipulate the current set of
unified values in the context of src_python{PatternSub}.

:REMARK:
It's also fairly straight-forward to apply this transform only
when src_python{C_lvar} is descended from a Cauchy src_python{RandomVariable}.
Taking this further, we could determine an expected value
for src_python{C_lvar}--through similar means--and, when it's zero, apply scale
parameter lifting/sinking transformations.
:END:

#+NAME: kanren-normal-rescale-example
#+BEGIN_SRC python :exports code :results none
mcmc_opt = EquilibriumOptimizer([KanrenRelationSub(mcmc_goals)],
                                max_use_ratio=10)

Y_rv_opt = optimize_graph(Y_rv, mcmc_opt)
#+END_SRC

#+NAME: kanren-normal-rescale-example-print
#+BEGIN_SRC python :exports results :results output scalar drawer replace
import textwrap


print("""
\\begin{{equation}}
{}
\\label{{eq:kanren-rescale-example}}
\\end{{equation}}
""".format(textwrap.indent(
    tt_tex_pprint(Y_rv_opt, {'latex': True, 'latex_aligned': True}), '\t\t')))
#+END_SRC

#+RESULTS: kanren-normal-rescale-example-print
:results:
\begin{equation}
		\begin{aligned}
		\tau &\sim \text{C}\left(0, 1\right), \quad \mathbb{R}
		\\
		\beta &\sim \text{N}\left(0, 1\right), \quad \mathbb{R}
		\\
		Y &\sim \text{N}\left((|\tau| \circ \beta), 1\right), \quad \mathbb{R}^{A_u[0]}
		\end{aligned}
		\\
		Y
\label{eq:kanren-rescale-example}
\end{equation}

:end:

The resulting graph is shown in Equation [[eqref:eq:kanren-rescale-example]].  As
expected, the $\tau$ term has been moved from the variance in $\beta$ to a
product with $\beta$ that is now the mean of $Y$.

While seemingly small, this sort of rewrite can significantly improve a
difficult sampling problem.  In the original formulation, when sampled values of
$\tau$ are near zero, $\beta$ will have a near zero variance--this is generally
the intended effect of shrinkage models.  Unfortunately, sampling distributions in
extreme parameter regions like this is often a problem.  A well-implemented
sampler should be able to handle *some* extremes, but it's not reasonable
to assume every case can/should be covered.

#+NAME: kanren-normal-rescale-example-hist-calc
#+BEGIN_SRC python :eval never-export :exports both :results silent
from timeit import default_timer as timer


Y_sampler = theano.function([], Y_rv)
Y_rescaled_sampler = theano.function([], Y_rv_opt)

size_Y_rv.set_value(1000)

_s = timer()
Y_samples = Y_sampler()
_e = timer()
time_Y = _e - _s

_s = timer()
Y_rescaled_samples = Y_rescaled_sampler()
_e = timer()
time_Y_rescaled = _e - _s
#+END_SRC

# TODO: Add table of time comparisons for smaller variances.

Furthermore, what makes a sampler robust, so that it can handle scenarios like
these?  Often, the answer involves transforms exactly like the one we've
implemented!
:REMARK:
Another good example is the Gamma distribution with all its parameter
edge-cases.
:END:
The more context available to a sampler, the more opportunity it has to make a
smarter choice.  Those choices are fundamentally limited when the context only
consists of a distribution and its parameters.  We have more context, because
we're working with the entire model, and--as a result--more opportunity.

#+NAME: kanren-normal-rescale-example-hist-calc
#+BEGIN_SRC python :eval never-export :exports none :results silent
# TODO: Time these operations to show how incredibly slow the first one is.
Y_all = pd.DataFrame({'Y_rv': Y_samples, 'Y_rv_opt': Y_rescaled_samples})

plt.close()
fig, ax = plt.subplots()
fig.set_size_inches(12.5, 9.5)

_ = Y_all.hist(bins=50, ax=ax)

# plt.show()
#+END_SRC

#+NAME: kanren-normal-rescale-example-hist
#+HEADER: :var output_dir=(btw--org-publish-property :figure-dir)
#+HEADER: :post org_fig_wrap(data=*this*, options="[keepaspectratio]", placement="[p!]", caption="")
#+BEGIN_SRC python :eval never-export :exports results :results value raw
fig_filenames = [os.path.join(output_dir, 'kanren-normal-rescale-example-hist')
                 + os.path.extsep + out_ext
                 for out_ext in ['pdf', 'png']]

for fname in fig_filenames:
   plt.savefig(fname)

_ = os.path.relpath(fig_filenames[-1])
#+END_SRC

#+RESULTS: kanren-normal-rescale-example-hist
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 1.0\textwidth :height 1.0\textwidth :float t :options [keepaspectratio] :placement [p!]
#+CAPTION:
#+NAME: fig:kanren-normal-rescale-example-hist
[[file:../../figures/kanren-normal-rescale-example-hist.png]]

# ** AR Model
# In [[citet:GelmanTransformingparameterssimple2019]], a simple AR model is proposed:
# \begin{equation}
# \begin{gathered}
#   \eta_1 \sim \operatorname{N}\left(0, \frac{\sigma}{\sqrt{1 - \rho^{2}}} \right)
#   \\
#   \eta_t \sim \operatorname{N}\left(\rho \eta_{t-1}, \sigma\right), \quad t \in \left\{2, 3, \dots, T\right\}
#   \\
#   \rho \in \left[0, 1\right]
# \end{gathered}
# \label{eq:gelman-time-series-pre}
# \end{equation}
#
# According to the article, Equation [[eqref:eq:gelman-time-series-pre]] had slow
# mixing and needed to be re-parameterized.
# \begin{equation}
# \end{equation}

** Normal-Gamma Gibbs Sampling
[[citet:ZhangTraceclassMarkov2019]] provides a prescription for more efficient
Gibbs block sampling based on Normal-Gamma family parameters.
This is exactly the kind of high-level theoretical work that can be implemented
in a sufficiently sophisticated, algebraically aware term rewriting context.

:REMARK:
One of the "sophistications" missing here is *constraint relations* in our miniKanren
implementation.
:END:
* Discussion

#+BIBLIOGRAPHY: ../tex/symbolic-pymc3.bib
#+BIBLIOGRAPHYSTYLE: plainnat
