
@article{bauschke_symbolic_2006,
  title = {Symbolic Computation of {{Fenchel}} Conjugates},
  author = {Bauschke, Heinz H. and Mohrenschildt, Martin V.},
  year = {2006},
  volume = {40},
  pages = {18},
  issn = {19322240},
  doi = {10.1145/1151446.1151453},
  journal = {ACM Communications in Computer Algebra},
  number = {1}
}

@article{ByrdaKanrenFreshName2007,
  title = {{{$\alpha$Kanren A Fresh Name}} in {{Nominal Logic Programming}}},
  author = {Byrd, William E. and Friedman, Daniel P.},
  year = {2007}
}

@misc{ByrdminiKanrenorg2019,
  title = {{{miniKanren}}.Org},
  author = {Byrd, William},
  year = {2019},
  url = {http://minikanren.org/},
  urldate = {2018-01-08}
}

@phdthesis{ByrdRelationalProgrammingminiKanren2009,
  title = {Relational {{Programming}} in {{miniKanren}}: {{Techniques}}, {{Applications}}, and {{Implementations}}},
  shorttitle = {Relational {{Programming}} in {{miniKanren}}},
  author = {Byrd, William E.},
  year = {2009},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.363.5478},
  school = {faculty of the University Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the Department of Computer Science, Indiana University}
}

@techreport{CaretteSimplifyingProbabilisticPrograms2016,
  title = {Simplifying {{Probabilistic Programs Using Computer Algebra}}},
  author = {Carette, Jacques and Shan, Chung-chieh},
  year = {2016},
  institution = {{Indiana University}},
  url = {https://www.cs.indiana.edu/cgi-bin/techreports/TRNNN.cgi?trnum=TR719},
  urldate = {2018-01-16},
  keywords = {Holonomic functions,integral transforms,measure theory,Probability Theory,Symbolic and Algebraic Manipulation},
  number = {TR719}
}

@article{carvalho_horseshoe_2010,
  title = {The Horseshoe Estimator for Sparse Signals},
  author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
  year = {2010},
  volume = {97},
  pages = {465-480},
  issn = {00063444},
  doi = {10.1093/biomet/asq017},
  journal = {Biometrika},
  keywords = {Normal scale mixture,Ridge regression,Robustness,shrinkage,sparsity,Thresholding},
  number = {2},
  pmid = {20882103}
}

@article{combettes_proximal_2011,
  title = {Proximal Splitting Methods in Signal Processing},
  author = {Combettes, Patrick L and Pesquet, Jean-Christophe},
  year = {2011},
  pages = {185--212},
  journal = {Fixed-point algorithms for inverse problems in science and engineering},
  keywords = {alternating-direction method of multipliers,backward-backward algorithm,convex optimization,denoising,douglas-rachford algorithm,forward-backward algorithm,Iterative thresholding,landweber method,parallel computing,proximal algorithm,restoration and reconstruction,sparsity,splitting}
}

@article{donoho2006compressed,
  title = {Compressed Sensing},
  author = {Donoho, David L},
  year = {2006},
  volume = {52},
  pages = {1289-1306},
  journal = {IEEE Transactions on information theory},
  number = {4}
}

@misc{GelmanTransformingparameterssimple2019,
  title = {Transforming Parameters in a Simple Time-Series Model; Debugging the {{Jacobian}} \guillemotleft{} {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  author = {Gelman, Andrew},
  year = {2019},
  month = jan,
  url = {https://statmodeling.stat.columbia.edu/2019/01/25/transforming-parameters-in-a-simple-time-series-model-debugging-the-jacobian/},
  urldate = {2019-01-28}
}

@phdthesis{HamiltonSymbolicconvexanalysis2005,
  title = {Symbolic Convex Analysis},
  author = {Hamilton, Chris H.},
  year = {2005},
  url = {http://docserver.carma.newcastle.edu.au/id/eprint/296},
  urldate = {2017-02-25},
  school = {Simon Fraser University}
}

@inproceedings{KucukelbirAutomaticvariationalinference2015,
  title = {Automatic Variational Inference in {{Stan}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
  year = {2015},
  pages = {568--576},
  url = {http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan},
  urldate = {2016-09-03}
}

@article{LucetWhatshapeyour2010,
  title = {What Shape Is Your Conjugate? {{A}} Survey of Computational Convex Analysis and Its Applications},
  shorttitle = {What Shape Is Your Conjugate?},
  author = {Lucet, Yves},
  year = {2010},
  volume = {52},
  pages = {505--542},
  url = {http://epubs.siam.org/doi/abs/10.1137/100788458},
  urldate = {2017-02-25},
  journal = {SIAM review},
  number = {3}
}

@article{LukasiewiczProbabilisticlogicprogramming2001,
  title = {Probabilistic Logic Programming with Conditional Constraints},
  author = {Lukasiewicz, Thomas},
  year = {2001},
  volume = {2},
  pages = {289--339},
  url = {http://dl.acm.org/citation.cfm?id=377983},
  urldate = {2017-01-17},
  journal = {ACM Transactions on Computational Logic (TOCL)},
  number = {3}
}

@article{MohaselAfsharProbabilisticInferencePiecewise2016,
  title = {Probabilistic {{Inference}} in {{Piecewise Graphical Models}}},
  author = {Mohasel Afshar, Hadi},
  year = {2016},
  url = {https://digitalcollections.anu.edu.au/handle/1885/107386},
  urldate = {2016-09-03},
  abstract = {In many applications of probabilistic inference the models
    contain piecewise densities that are differentiable except at
    partition boundaries. For instance, (1) some models may
    intrinsically have finite support, being constrained to some
    regions; (2) arbitrary density functions may be approximated by
    mixtures of piecewise functions such as piecewise polynomials or
    piecewise exponentials; (3) distributions derived from other
    distributions (via random variable transformations) may be highly
    piecewise; (4) in applications of Bayesian inference such as
    Bayesian discrete classification and preference learning, the
    likelihood functions may be piecewise; (5) context-specific
    conditional probability density functions (tree-CPDs) are
    intrinsically piecewise; (6) influence diagrams (generalizations
    of Bayesian networks in which along with probabilistic inference,
    decision making problems are modeled) are in many applications
    piecewise; (7) in probabilistic programming, conditional
    statements lead to piecewise models. As we will show, exact
    inference on piecewise models is not often scalable (if
    applicable) and the performance of the existing approximate
    inference techniques on such models is usually quite poor.
    This thesis fills this gap by presenting scalable and accurate
    algorithms for inference in piecewise probabilistic graphical
    models. Our first contribution is to present a variation of Gibbs
    sampling algorithm that achieves an exponential sampling speedup
    on a large class of models (including Bayesian models with
    piecewise likelihood functions). As a second contribution, we
    show that for a large range of models, the time-consuming Gibbs
    sampling computations that are traditionally carried out per
    sample, can be computed symbolically, once and prior to the
    sampling process. Among many potential applications, the
    resulting symbolic Gibbs sampler can be used for fully automated
    reasoning in the presence of deterministic constraints among
    random variables. As a third contribution, we are motivated by
    the behavior of Hamiltonian dynamics in optics \textemdash{}in particular,
    the reflection and refraction of light on the refractive
    surfaces\textemdash{} to present a new Hamiltonian Monte Carlo method that
    demonstrates a significantly improved performance on piecewise
    models.
    Hopefully, the present work represents a step towards scalable
    and accurate inference in an important class of probabilistic
    models that has largely been overlooked in the literature.},
  copyright = {http://legaloffice.weblogs.anu.edu.au/content/copyright/},
  keywords = {binary trees,gibbs sampling},
  language = {en}
}

@inproceedings{PaigeCompilationTargetProbabilistic2014,
  title = {A {{Compilation Target}} for {{Probabilistic Programming Languages}}.},
  booktitle = {{{ICML}}},
  author = {Paige, Brooks and Wood, Frank},
  year = {2014},
  pages = {1935--1943},
  url = {http://www.jmlr.org/proceedings/papers/v32/paige14.pdf},
  urldate = {2016-09-03}
}

@article{parikh_proximal_2014,
  title = {Proximal {{Algorithms}}},
  author = {Parikh, Neal and Boyd, Stephen},
  year = {2014},
  volume = {1},
  pages = {123--231},
  issn = {2167-3888},
  doi = {10.1561/2400000003},
  journal = {Foundations and Trends in Optimization},
  keywords = {shrinkage,sparsity},
  number = {3}
}

@article{Parkbayesianlasso2008,
  title = {The bayesian lasso},
  author = {Park, Trevor and Casella, George},
  year = {2008},
  volume = {103},
  pages = {681–686},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337},
  urldate = {2017-01-17},
  journal = {Journal of the American Statistical Association},
  number = {482}
}

@article{peasgood_method_2009,
  title = {A {{Method}} to {{Symbolically Compute Convolution Integrals}}},
  author = {Peasgood, Richard},
  year = {2009},
  url = {https://uwspace.uwaterloo.ca/handle/10012/4884},
  urldate = {2016-01-20}
}

@article{PedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research}
}

@article{polson_proximal_2015,
  title = {Proximal Algorithms in Statistics and Machine Learning},
  author = {Polson, Nicholas G. and Scott, James G. and Willard, Brandon T.},
  year = {2015},
  month = nov,
  volume = {30},
  pages = {559-581},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/15-STS530},
  url = {http://projecteuclid.org/euclid.ss/1449670858},
  urldate = {2015-12-14},
  abstract = {Proximal algorithms are useful for obtaining solutions to difficult optimization problems, especially those involving nonsmooth or composite objective functions. A proximal algorithm is one whose basic iterations involve the proximal operator of some function, whose evaluation requires solving a specific optimization problem that is typically easier than the original problem. Many familiar algorithms can be cast in this form, and this “proximal view” turns out to provide a set of broad organizing principles for many algorithms useful in statistics and machine learning. In this paper, we show how a number of recent advances in this area can inform modern statistical practice. We focus on several main themes: (1) variable splitting strategies and the augmented Lagrangian; (2) the broad utility of envelope (or variational) representations of objective functions; (3) proximal algorithms for composite objective functions; and (4) the surprisingly large number of functions for which there are closed-form solutions of proximal operators. We illustrate our methodology with regularized Logistic and Poisson regression incorporating a nonconvex bridge penalty and a fused lasso penalty. We also discuss several related issues, including the convergence of nondescent algorithms, acceleration and optimization for nonconvex functions. Finally, we provide directions for future research in this exciting area at the intersection of statistics and optimization.},
  journal = {Statistical Science},
  keywords = {\#duplicates,ADMM,bayes MAP,Bayes MAP,divide and concur,Divide and Concur,envelopes,fused lasso,KL,Kurdyka–Łojasiewicz,non-convex optimisation,nonconvex,optimization,Optimization,proximal,proximal operators,regularization,Regularization,shrinkage,sparsity,splitting},
  language = {EN},
  number = {4}
}

@article{PolsonStatisticalTheoryDeep2015,
  title = {A {{Statistical Theory}} of {{Deep Learning}} via {{Proximal Splitting}}},
  author = {Polson, Nicholas G. and Willard, Brandon T. and Heidari, Massoud},
  year = {2015},
  url = {http://arxiv.org/abs/1509.06061},
  urldate = {2016-09-06},
  journal = {arXiv preprint arXiv:1509.06061}
}

@misc{RocklinlogpyLogicProgramming2018,
  title = {Logpy: {{Logic Programming}} in {{Python}}},
  shorttitle = {Logpy},
  author = {Rocklin, Matthew},
  year = {2018},
  month = jan,
  url = {https://github.com/logpy/logpy},
  urldate = {2018-01-07},
  copyright = {BSD-3-Clause}
}

@phdthesis{RocklinMathematicallyinformedlinear2013,
  title = {Mathematically Informed Linear Algebra Codes through Term Rewriting},
  author = {Rocklin, Matthew},
  year = {2013},
  url = {http://people.cs.uchicago.edu/~mrocklin/storage/dissertation.pdf},
  urldate = {2016-09-20},
  school = {PhD Thesis, August}
}

@misc{RocklinMultipledispatchContribute2019,
  title = {Multiple Dispatch. {{Contribute}} to Mrocklin/Multipledispatch Development by Creating an Account on {{GitHub}}},
  author = {Rocklin, Matthew},
  year = {2019},
  month = jan,
  url = {https://github.com/mrocklin/multipledispatch},
  urldate = {2019-01-22},
  copyright = {View license}
}

@phdthesis{RoyComputabilityinferencemodeling2011,
  title = {Computability, Inference and Modeling in Probabilistic Programming},
  author = {Roy, Daniel M.},
  year = {2011},
  url = {https://pdfs.semanticscholar.org/2ace/2f8090cb5dcb5a629937771dfb7a312adb33.pdf},
  urldate = {2017-01-17},
  school = {Massachusetts Institute of Technology}
}

@article{SalvatierProbabilisticprogrammingPython2016,
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  year = {2016},
  month = apr,
  volume = {2},
  pages = {e55},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.55},
  url = {https://peerj.com/articles/cs-55},
  urldate = {2016-04-28},
  journal = {PeerJ Computer Science},
  language = {en}
}

@inproceedings{ScibiorPracticalprobabilisticprogramming2015,
  title = {Practical Probabilistic Programming with Monads},
  booktitle = {{{ACM SIGPLAN Notices}}},
  author = {{\'S}cibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.},
  year = {2015},
  volume = {50},
  pages = {165--176},
  publisher = {{ACM}},
  url = {http://dl.acm.org/citation.cfm?id=2804317},
  urldate = {2016-09-03}
}

@article{scott_parameter_2010,
  title = {Parameter expansion in local-shrinkage models},
  author = {Scott, James G.},
  year = {2010},
  url = {http://arxiv.org/abs/1010.5265},
  urldate = {2015-06-13},
  journal = {arXiv preprint arXiv:1010.5265},
  keywords = {MCMC,Normal scale mixtures,parameter expansion,sparsity}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A}} Simple Way to Prevent Neural Networks from Overfitting},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  volume = {15},
  pages = {1929--1958},
  url = {http://dl.acm.org/citation.cfm?id=2670313},
  urldate = {2015-08-06},
  journal = {The Journal of Machine Learning Research},
  number = {1}
}

@book{sympydevelopmentteam_sympy_2014,
  title = {{{SymPy}}: {{Python}} Library for Symbolic Mathematics},
  author = {{SymPy Development Team}},
  year = {2014},
  url = {http://www.sympy.org}
}

@article{TranDeepProbabilisticProgramming2016,
  title = {Deep {{Probabilistic Programming}}},
  author = {Tran, Dustin and Hoffman, Matt and Murphy, Kevin and Brevdo, Eugene and Saurous, Rif A. and Blei, David M.},
  year = {2016},
  url = {http://bayesiandeeplearning.org/papers/BDL_43.pdf},
  urldate = {2017-01-17}
}

@article{Wangmechanicalmathematics1960,
  title = {Toward Mechanical Mathematics},
  author = {Wang, Hao},
  year = {1960},
  volume = {4},
  pages = {2--22},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5392526},
  urldate = {2016-10-18},
  journal = {IBM Journal of research and development},
  number = {1}
}

@misc{Willardbrandonwillardtheanosympy,
  title = {Brandonwillard/Theano\_sympy},
  author = {Willard, Brandon},
  url = {https://github.com/brandonwillard/theano_sympy},
  urldate = {2017-01-20},
  abstract = {theano\_sympy - Function to transform theano graph {$<$}-{$>$} sympy graph.},
  journal = {GitHub}
}

@misc{WillardRandomVariablesTheano2018,
  title = {Random {{Variables}} in {{Theano}}},
  author = {Willard, Brandon T.},
  year = {2018},
  month = dec,
  url = {https://brandonwillard.github.io/random-variables-in-theano.html},
  urldate = {2019-01-16},
  language = {en}
}

@misc{WillardReadableStringsRelational2018a,
  title = {Readable {{Strings}} and {{Relational Programming}} in {{Hy}}},
  author = {Willard, Brandon T. and Willard, Brandon T.},
  year = {2018},
  month = dec,
  url = {https://brandonwillard.github.io/readable-strings-and-relational-programming-in-hy.html},
  urldate = {2019-01-22},
  journal = {Brandon T. Willard},
  language = {en}
}

@misc{WillardRoleSymbolicComputation2017,
  title = {A {{Role}} for {{Symbolic Computation}} in the {{General Estimation}} of {{Statistical Models}}},
  author = {Willard, Brandon T.},
  year = {2017},
  month = jan,
  url = {https://brandonwillard.github.io/a-role-for-symbolic-computation-in-the-general-estimation-of-statistical-models.html},
  urldate = {2017-02-11},
  abstract = {A Role for Symbolic Computation in the General Estimation of Statistical Models code\{white-space: pre;\} div.sourceCode \{ overflow-x: auto; \} table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode \{ margin: 0; padding: 0; vertical-align: baseline; border: none; \} table.sourceCode \{ width: 100\%; line-height: 100\%; \} td.lineNumbers \{ text-align: right; padding-right: 4px; padding-left: 4px; background-color ...\vphantom\}},
  journal = {Brandon T. Willard}
}

@misc{WillardSymbolicMathPyMC32018,
  title = {Symbolic {{Math}} in {{PyMC3}}},
  author = {Willard, Brandon T.},
  year = {2018},
  month = dec,
  url = {https://brandonwillard.github.io/symbolic-math-in-pymc3.html},
  urldate = {2018-12-27}
}

@misc{Willardsymbolicpymc2019,
  title = {Symbolic-Pymc},
  author = {Willard, Brandon T.},
  year = {2019},
  month = feb,
  url = {https://github.com/pymc-devs/symbolic-pymc},
  urldate = {2020-03-18},
  abstract = {Tools for the symbolic manipulation of PyMC models, Theano, and TensorFlow graphs.},
  howpublished = {PyMC},
  keywords = {bayesian,minikanren,probabilistic-programming,pymc,relational-programming,symbolic-computation,symbolic-math,tensorflow,theano}
}

@inproceedings{WoodNewApproachProbabilistic2014,
  title = {A {{New Approach}} to {{Probabilistic Programming Inference}}.},
  booktitle = {{{AISTATS}}},
  author = {Wood, Frank and {van de Meent}, Jan-Willem and Mansinghka, Vikash},
  year = {2014},
  pages = {1024--1032},
  url = {http://www.jmlr.org/proceedings/papers/v33/wood14.pdf},
  urldate = {2016-09-03}
}

@article{ZhangTraceclassMarkov2019,
  title = {Trace Class {{Markov}} Chains for the {{Normal}}-{{Gamma Bayesian}} Shrinkage Model},
  author = {Zhang, Liyuan and Khare, Kshitij and Xing, Zeren},
  year = {2019},
  volume = {13},
  pages = {166--207},
  issn = {1935-7524},
  doi = {10.1214/18-EJS1491},
  url = {https://projecteuclid.org/euclid.ejs/1547607848},
  urldate = {2019-01-22},
  abstract = {High-dimensional data, where the number of variables exceeds or is comparable to the sample size, is now pervasive in many scientific applications. In recent years, Bayesian shrinkage models have been developed as effective and computationally feasible tools to analyze such data, especially in the context of linear regression. In this paper, we focus on the Normal-Gamma shrinkage model developed by Griffin and Brown [7]. This model subsumes the popular Bayesian lasso model, and a three-block Gibbs sampling algorithm to sample from the resulting intractable posterior distribution has been developed in [7]. We consider an alternative two-block Gibbs sampling algorithm, and rigorously demonstrate its advantage over the three-block sampler by comparing specific spectral properties. In particular, we show that the Markov operator corresponding to the two-block sampler is trace class (and hence Hilbert-Schmidt), whereas the operator corresponding to the three-block sampler is not even Hilbert-Schmidt. The trace class property for the two-block sampler implies geometric convergence for the associated Markov chain, which justifies the use of Markov chain CLT's to obtain practical error bounds for MCMC based estimates. Additionally, it facilitates theoretical comparisons of the two-block sampler with sandwich algorithms which aim to improve performance by inserting inexpensive extra steps in between the two conditional draws of the two-block sampler.},
  journal = {Electronic Journal of Statistics},
  keywords = {Bayesian shrinakge,Data Augmentation,Markov chain Monte Carlo,normal-Gamma model,trace class operators},
  language = {EN},
  number = {1}
}


