\documentclass[12pt]{article}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{suffix}
\usepackage{color}

% Sadly, can't use this because it breaks greek letters.
%\usepackage[slantedGreek]{mathpazo}
% \usepackage{breqn}

\usepackage{todonotes}
\usepackage{draftwatermark}
\SetWatermarkScale{1}
\SetWatermarkLightness{0.90}

% used by Pweave
\usepackage{graphicx}

\definecolor{bg}{rgb}{0.95,0.95,0.95}
\usepackage[backgroundcolor=bg, topline=false, bottomline=false,
leftline=false, rightline=false]{mdframed}

<<pweave_code, echo=False, evaluate=False>>=
from pynoweb_tools.editor_utils import nvim_weave

input_file_base = nvim_weave(rel_figdir='../figures',
                             rel_outdir="./",
                             format_opts={'width': r'\textwidth',
                                          'figfmt': '.png',
                                          'savedformats': ['.png', '.pdf']})

# Compile document to markdown
assert os.system('make {}.pdf'.format(input_file_base)) == 0
assert os.system('make {}.md'.format(input_file_base)) == 0
@

\usepackage{minted}
\setminted{fontsize=\footnotesize, breaklines=true, breakanywhere=true,
breakautoindent=true}

% this order is important
%\PassOptionsToPackage{hyphens}{url}
\RequirePackage[hyphens]{url}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[authoryear]{natbib}

% Apparently cleveref must always be last.
\usepackage{cleveref}

\allowdisplaybreaks

\include{math-commands}

\graphicspath{{../../figures/}{../figures/}{./figures/}{./}}

\title{More Proximal Estimation}

\author{Brandon T. Willard}

\date{2017-03-06}

\begin{document}

\maketitle

\section{Introduction}

The focal point of this short exposition will be an
elaboration of \citet{willard_role_2017} via the numeric
implementation of the basic $\ell_1$ penalization problem,
\begin{equation}
  \argmin_{\beta} \left\{
  \frac{1}{2} \|y - X \beta\|^2_2
    + \lambda \|\beta\|_1
  \right\}
  \;,
  \label{eq:lasso}
\end{equation}
alongside some additional, related topics concerning automation and
symbolic computation.
Again, our framing of the problem is in terms of proximal methods
\citep{parikh_proximal_2014, combettes_proximal_2011}, how they relate
to some common estimation practices--specifically, coordinate descent
and line-searching--and their implementation in Theano
\citep{bergstra_theano_2010}.


We start by focusing on the connections with coordinate-wise path estimation of
\Cref{eq:lasso}--and related problems--in the statistics literature.  The goal
is one among many \citep{polson_proximal_2015} steps intended to provide
details of the already well established relationship between these problems and
proximal methods.  Minimally, these connections can contextualize the past
developments and direction of work in this area, and--ideally--open up avenues
for readily drawing from established work in proximal algorithms across
numerous fields.

Although \Cref{eq:lasso} is strictly $\ell_1$--or Lasso, we simply allude to
the relatively minor alterations that produce the ElasticNet, grouped Lasso
and related models.  Under the proximal framework, the distinctions between
these models do not generally lead to major theoretical re-workings, or the
need to consider principles not already found within the proximal methods.

For example, the transition from standard Lasso to grouped Lasso may not be
trivial, but--within the proximal framework--the change amounts to an
intermediate operator composition step \citep{argyriou_efficient_2011,
hu_proximal}.
This change can be introduced into existing proximal algorithms without
significant alteration to the established steps.  In some cases the exact
change can be entirely transparent and only result in nominally different
arguments to existing functions.

\section{Proximal and Computational Components}

First, the workhorse of the proximal framework: the \emph{proximal
operator}.

\begin{Def}[Proximal Operator]
  \begin{equation}
    \prox_{\phi}(x) =
    \argmin_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
  \end{equation}
\end{Def}

<<python_setup, evaluate=True, echo=False, results='hidden'>>=
import numpy as np
import scipy as sc
import pandas as pd

import pymc3 as pm

import theano
import theano.tensor as tt
from theano.printing import debugprint as tt_print
from theano.printing import pprint as tt_pprint

import matplotlib.pyplot as plt

# %matplotlib inline
# %config InlineBackend.figure_format = 'svg'

import seaborn as sns

# plt.rcParams["figure.figsize"] = (12, 8)
# plt.rcParams['text.usetex'] = True
plt.rc('text', usetex=True)

sns.set(style="whitegrid")
sns.set_style("whitegrid", {"axes.linewidth": ".7"})

theano.config.mode = 'FAST_COMPILE'
@

Inspired by \Cref{eq:lasso}, we produce a toy dataset as follows:
<<simulate_data, evaluate=True, echo=True>>=
from theano import shared as tt_shared

M = 50
M_nonzero = M * 2 // 10

beta_true = np.zeros(M)
beta_true[:M_nonzero] = np.exp(-np.arange(M_nonzero)) * 100

N = int(np.alen(beta_true) * 0.4)
X = np.random.randn(N, M)
mu_true = X.dot(beta_true)
y = mu_true + sc.stats.norm.rvs(np.zeros(N), scale=10)

X_tt = tt_shared(X, name='X', borrow=True)
y_tt = tt_shared(y, name='y', borrow=True)

# Estimation starting parameters...
beta_0 = np.zeros(X.shape[1]).astype('float64')

# Gradient [starting] step size
alpha_0 = 1. / np.linalg.norm(X, 2)**2
# np.linalg.matrix_rank(X)

# Regularization value heuristic
# beta_ols = np.linalg.lstsq(X, y)[0]
# lambda_max = 0.1 * np.linalg.norm(beta_ols, np.inf)
lambda_max = np.linalg.norm(X.T.dot(y), np.inf)
@

As in \citet{willard_role_2017}, we can start with a model defined within a
system like PyMC3 \citep{salvatier_probabilistic_2016}.
<<create_pymc3_model, evaluate=True, echo=True>>=
with pm.Model() as lasso_model:
    beta_rv = pm.Laplace('beta', mu=0, b=1,
                         shape=X.shape[1])
    y_rv = pm.Normal('y', mu=X_tt.dot(beta_rv), sd=1,
                     shape=y.shape[0], observed=y_tt)
@
In this setting one might then arrive at the necessary steps toward estimation
automatically (i.e.  identify the underlying $\ell_1$ estimation problem).  We
discuss this more in \citet{willard_role_2017}.

For simplicity we'll just assume that all components of the estimation problem
are know--i.e. loss and penalty functions.  The proximal operator
that arises in this standard example is the \emph{soft thresholding} operator.
In Theano, it can be implemented with the following:
<<theano_soft_thresholding, echo=True, evaluate=True>>=
def tt_soft_threshold(beta_, lambda_):
    return tt.sgn(beta_) * tt.maximum(tt.abs_(beta_) - lambda_, 0)
@

\begin{remark}
  This operator can take other forms, and the one used here is likely not
  the best.  The \texttt{maximum} can be replaced by other conditional-like
  statements--such as
  \begin{equation*}
    \operatorname{S}(z, \lambda) =
    \begin{cases}
     \sgn(\beta) (\beta - \lambda) & \beta > \lambda
     \\
     0 & \text{otherwise}
    \end{cases}
    \;.
  \end{equation*}
  If we were to--say--multiply the output of this operator with
  another, more difficult to compute result, then we might also wish to
  extend this multiplication into the definition of the operator and
  avoid its computation in the $\beta \leq \lambda$ case.

  Barring any reuses of this quantity, or a need to preserve undefined
  results produced by an expensive product with zero, we would ideally
  like a "compiler" to make such an optimization itself.  It isn't
  clear how a standard compiler--or interpreter/hybrid--could
  safely make this optimization, whereas it does seem more reasonable
  as a symbolic/Theano optimization.

  Optimizations like this are--I think--a necessary step to enable
  expressive, generalized methods, truly rapid prototyping at the math
  level.
\end{remark}

<<theano_simplify_funcs, evaluate=True, echo=False, results='hidden'>>=
#
# Check out the Theano options
#
# print(theano.opt.config)
# FYI: Use a theano.gof.NavigatorOptimizer
#
from theano.gof import Query
from theano.compile import optdb
from theano.gof.graph import inputs as tt_inputs

# theano.opt.config.optimizer_verbose = True
# theano.opt.config.metaopt_verbose = True
theano.opt.config.exception_verbosity = 'high'

# Some details:
# theano.compile.mode.optdb._names
# theano.compile.mode.optdb.print_summary()
# theano.compile.get_default_mode()

# fast_run = optdb.query(Query(include=['fast_run']))
our_opts = optdb.query(Query(include=['specialize',
                                      'canonicalize',
                                      'stabilize']))

def to_fgraph(outs, ins=None):

    if not isinstance(outs, (list, tuple)):
        outs = [outs]

    if ins is None:
        ins = tt_inputs(outs)
    elif not isinstance(ins, (list, tuple)):
        ins = [ins]

    # This first way loses all connection between the original
    # nodes and the cloned ones if `copy_inputs=True`:
    # ins, outs = theano.gof.graph.clone(ins, outs,
    #                                    copy_inputs=False)
    # This is how you can keep that mapping:
    # ins_map = theano.gof.graph.clone_get_equiv(ins, outs,
    #                                            # Set this to False if you want
    #                                            # to keep using the original
    #                                            # top-level inputs, but new
    #                                            # inbetween nodes.
    #                                            copy_inputs_and_orphans=True)
    # ins, outs = [ins_map[in_] for in_ in ins], [ins_map[out_] for in_ in outs]

    fgraph = theano.gof.FunctionGraph(ins, outs)

    return fgraph

def tt_simplify(fgraph):

    if not isinstance(fgraph, theano.gof.fg.FunctionGraph):
        fgraph = to_fgraph(fgraph)

    #fgraph = fgraph.clone()
    res = our_opts.optimize(fgraph)

    return fgraph
@

<<echo=False, evaluate=False>>=
#
# A small demo:
#
# neg_sum_test = create_fgraph(-tt.sum(-y_rv))
# neg_logl_test = create_fgraph(-tt.sum(-(y_rv - X_tt.dot(beta_rv))**2))
#
# # tt_pprint(neg_sum_test.outputs[0])
# tt_pprint(neg_logl_test.outputs[0])
#
# our_opt.optimize(neg_logl_test)
#
# tt_pprint(neg_logl_test.outputs[0])

# TODO: What about fusion?
# theano.config.tensor.local_elemwise_fusion = True
# optdb.query(Query(include=['elemwise_fusion'])).optimize(nlogl_graph)

from theano import clone as tt_clone

# Clone the negative log-likelihood of our observation model.
nlogl_rv = -lasso_model.observed_RVs[0].logpt
# nlogl = tt_clone(nlogl_rv, {beta_rv: beta_tt})
nlogl = tt_clone(nlogl_rv)
nlogl.name = "-logl"

# beta_tt = tt_shared(beta_0, name='beta_prox')

# Get a Theano function graph.
# nlogl_graph = to_fgraph(nlogl, ins=[y_tt, X_tt, beta_tt])
nlogl_graph = to_fgraph(nlogl, ins=[y_tt, X_tt, beta_rv])

# Perform some Theano simplifications manually.
# Could also do something like:
#    tt_function([beta_rv], [nlogl], mode="FAST_RUN").maker.fgraph
# TODO: The opts still aren't getting rid of the negative sign; eh.
nlogl_graph = tt_simplify(nlogl_graph)
# beta_clone = nlogl_graph.inputs[-1]
beta_tt = nlogl_graph.inputs[-1]


# Clone the result and use it from here on.
# nlogl_out = tt_clone(nlogl_graph.outputs[0],
#                      {beta_clone: beta_tt})
nlogl_out = nlogl_graph.outputs[0]
@

Now, assuming that we've obtained the relevant loss and penalty functions--for
example, in PyMC3--then we can proceed to setting up the exact context of our
proximal problem.
<<example_theano_graph_step, echo=True, evaluate=True, results='hidden'>>=
from theano import clone as tt_clone

# Clone the negative log-likelihood of our observation model.
nlogl_rv = -lasso_model.observed_RVs[0].logpt
nlogl = tt_clone(nlogl_rv)
nlogl.name = "-logl"
beta_tt = tt_inputs([nlogl])[4]
@

\section{Proximal Gradient}

In what follows it will be convenient to generalize a bit and work in terms of
arbitrary loss and penalty functions $l$ and $\phi$, respectively, which in our
case corresponds to
\begin{gather*}
  l(\beta) = \frac12 \|y - X \beta\|^2_2, \quad
  \text{and}\;
  \phi(\beta) = \|\beta\|_1
  \;.
\end{gather*}

The proximal gradient \citep{combettes_proximal_2011} algorithm is a staple of
the proximal framework that provides solutions to problems of the form
\begin{equation}
  \argmin_\beta \left\{
    l(\beta) + \lambda \phi(\beta)
  \right\}
  \;.
\end{equation}
when both $l$ and $\phi$ are lower semi-continuous convex
functions, and $l$ is differentiable with Lipschitz gradient.

The solution is given as the following fixed-point:
\begin{equation}
  \beta = \prox_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}
The constant step size $\alpha$ is related to the Lipschitz constant
of $\nabla l$, but can more generally be a sequence obeying certain constraints.
Since our $l$ under consideration is $\ell_2$, we have the incredibly
standard $\nabla l(\beta) = X^\top (X \beta - y)$.

\subsection{Implementation}

As in \citet{willard_role_2017}, we provide an implementation of a
proximal gradient step.
<<prox_gradient_step, echo=True, evaluate=True>>=
from theano import function as tt_function
from theano.compile.nanguardmode import NanGuardMode

tt_func_mode = NanGuardMode(nan_is_error=True,
                            inf_is_error=False,
                            big_is_error=False)


def prox_gradient_step(loss, beta_tt, prox_func,
                       alpha_tt=None, lambda_tt=None,
                       return_loss_grad=False,
                       tt_func_kwargs={'mode': tt_func_mode}
                       ):
    r""" Creates a function that produces a proximal gradient step.

    Arguments
    =========
    loss: TensorVariable
        Continuously differentiable "loss" function in the objective
        function.
    beta_tt: TensorVariable
        Variable argument of the loss function.
    prox_fn: function
        Function that computes the proximal operator for the "penalty"
        function.  Must take two parameters: the first a TensorVariable
        of the gradient step, the second a float or Scalar value.
    alpha_tt: float, Scalar (optional)
        Gradient step size.
    lambda_tt: float, Scalar (optional)
        Additional scalar value passed to `prox_fn`.
        TODO: Not sure if this should be here; is redundant.
    """
    loss_grad = tt.grad(loss, wrt=beta_tt)
    loss_grad.name = "loss_grad"

    if alpha_tt is None:
        alpha_tt = tt.scalar(name='alpha')
        alpha_tt.tag.test_value = 1
    if lambda_tt is None:
        lambda_tt = tt.scalar(name='lambda')
        lambda_tt.tag.test_value = 1

    beta_grad_step = beta_tt - alpha_tt * loss_grad
    beta_grad_step.name = "beta_grad_step"

    prox_grad_step = prox_func(beta_grad_step, lambda_tt * alpha_tt)
    prox_grad_step.name = "prox_grad_step"

    inputs = []
    updates = None
    if isinstance(beta_tt, tt.sharedvar.SharedVariable):
        updates = [(beta_tt, prox_grad_step)]
    else:
        inputs += [beta_tt]
    if not isinstance(alpha_tt, tt.sharedvar.SharedVariable):
        inputs += [alpha_tt]
    if not isinstance(lambda_tt, tt.sharedvar.SharedVariable):
        inputs += [lambda_tt]

    prox_grad_step_fn = tt_function(inputs,
                                    prox_grad_step,
                                    updates=updates,
                                    **tt_func_kwargs)

    res = (prox_grad_step_fn,)
    if return_loss_grad:
        res += (loss_grad,)

    return res
@
This function could also be implemented in a less ``stateful'' fashion,
in which the terms are given as arguments to each function call--instead
of shared variable.

\subsection{Step Sizes}

A critical aspect of the proximal gradient approach involves the use of
appropriate step sizes, $\alpha$.  They needn't always be fixed values, and,
because of this, we can search for an appropriate value during estimation.
These values have an obvious connection to the performance of a method--beyond
basic guarantees of convergence--and sometimes
Most often, there are ranges of such values that are easily specified by
the Lipschitz properties of the operators involved.
The same function-analytic considerations underlie the classical line-search
methods in iterative optimization, give meaning to ``tuning parameters'',
and often highlight the need for more ``mathematical'' considerations in
practice.

In the spirit of more mathematical considerations and their automation,
one particularly relevant direction can be found in Theano's
experimental matrix ``Hints''.  The ideas behind
\texttt{theano.sandbox.linalg.ops.\{psd, spectral\_radius\_bound\}}
encompass some of the machinery needed to automatically determine
applicable and efficient $\alpha$ constants and sequences.

In our example, we use the standard backtracking line-search.
<<backtracking_search, echo=True, evaluate=True>>=
def backtracking_search(beta_, alpha_,
                        prox_fn, loss_fn, loss_grad_fn,
                        lambda_=1, bt_rate=0.5, obj_tol=1e-5):
    # alpha_start = alpha_
    z = beta_
    beta_start_ = beta_
    loss_start_ = loss_fn(beta_)
    loss_grad_start_ = loss_grad_fn(beta_)
    while True:

        beta_ = beta_start_ - alpha_ * loss_grad_start_
        z = prox_fn(beta_, alpha_ * lambda_)

        loss_z = loss_fn(z)
        step_diff = z - beta_start_
        loss_diff = loss_z - loss_start_
        line_diff = alpha_ * (loss_diff - loss_grad_start_.T.dot(step_diff))
        line_diff -= step_diff.T.dot(step_diff) / 2.

        if line_diff <= obj_tol:
            return z, alpha_, loss_z

        alpha_ *= bt_rate
        assert alpha_ >= 0, 'invalid step size: {}'.format(alpha_)
@

\begin{remark}
  Routines like this that make use of the gradient and other quantities might
  also be good candidates for execution in Theano, if only because of the graph
  optimizations that are able to remedy obviously redundant computations.

  In this vein, we could consider performing the line-search, and/or the entire
  optimization loop, within a Theano \texttt{scan} operation.  We could also
  create \texttt{Op}s that represents gradient and line-search step.  These
  might make graph construction much simpler, and be more suited for the
  current optimization framework.

  Although \texttt{scan} and tighter Theano integration may not on average
  produce better results than our current use of its compiled functions, we
  still wish to emphasize the possibilities.
\end{remark}


<<echo=False, evaluate=False>>=
gamma_ls_upper_tt = tt.nlinalg.norm(X_tt.T.dot(y_tt), 'inf')

print(gamma_ls_upper_tt.eval())

step_rate = tt_shared(0.6, 'step_rate')

def backtrack_one_step(beta_prev, beta_cur,
                       logl_prev, logl_cur,
                       logl_grad_prev, logl_grad_cur,
                       gamma_prox_prev, gamma_prox_cur,
                       step_rate_):

    # beta_val, logl_val, logl_grad_val, mse_val = prox_step()
    # step_vec = beta_val - beta_val_prev
    # logl_quad_expans = logl_val_prev + logl_grad_val_prev.T.dot(step_vec)
    # logl_quad_expans += 0.5 / lambda_1_tt * step_vec.T.dot(step_vec)
    # if logl_val <= logl_quad_expans:
    #     break
    # if lambda_1_tt.get_value() <= 0:
    #     raise ValueError('invalid step size: {}'.format(
    #         lambda_1_tt.get_value()))
    # # Could just make this an argument to prox_step...
    # lambda_1_tt.set_value(lambda_1_tt.get_value() * bt_rate)

    # step_vec = beta_cur - beta_prev
    # logl_quad_expans = logl_prev + logl_grad_prev.T.dot(step_vec)
    # logl_quad_expans += 0.5 / gamma_prox_cur * step_vec.T.dot(step_vec)

    # armijo_cond = logl_cur <= logl_quad_expans
    # res = (beta_cur, logl_cur, logl_grad_cur, gamma_prox_cur * step_rate_)

    return res + (theano.scan_module.until(armijo_cond),)


bt_ls_vals, bt_ls_ups = theano.scan(backtrack_one_step,
                                    outputs_info = [dict(initial=beta_tt, taps=[-2, -1]),
                                                    dict(initial=logl, taps=[-2, -1]),
                                                    dict(initial=logl_grad, taps=[-2, -1]),
                                                    dict(initial=lambda_tt, taps=[-2, -1])
                                                    ],
                                    non_sequences = [step_rate],
                                    n_steps = 1000)

backtrack_search = tt_function([step_rate], bt_ls_vals, updates=bt_ls_ups)
@
<<echo=False, evaluate=False>>=
# Scan tests

prox_fn = tt_function([beta_tt, lambda_tt],
                      tt_soft_threshold(beta_tt, lambda_tt))

prox_step_tt = tt_clone(prox_fn.maker.fgraph.outputs[0],
                        {prox_fn.maker.fgraph.inputs[0]: beta_tt,
                         prox_fn.maker.fgraph.inputs[1]: lambda_tt})

prox_grad_step_fn, loss_grad = prox_gradient_step(
    nlogl, beta_tt, tt_soft_threshold,
    return_loss_grad=True)

prox_grad_step_tt = prox_grad_step_fn.maker.fgraph.outputs[0]

loss_fn = tt_function([beta_tt], nlogl)

loss_tt = loss_fn.maker.fgraph.outputs[0]

loss_grad_fn = tt_function([beta_tt], loss_grad)


# loss_grad_tt = loss_grad_fn.maker.fgraph.outputs[0]
loss_grad_tt = tt_clone(loss_grad_fn.maker.fgraph.outputs[0],
                        {loss_grad_fn.maker.fgraph.inputs[0]: beta_tt})
loss_grad_tt.tag.test_value = beta_tt.tag.test_value

# beta_val_tt, logl_val_tt, logl_grad_val_tt, mse_val_tt, beta_prox_tt = prox_step.maker.fgraph.outputs

def backtrack_one_step(beta_start_,
                       alpha_,
                       loss_grad_start_,
                       prox_step_):
    beta_ = beta_start_.squeeze() - alpha_ * loss_grad_start_

    # Custom step for this new beta.  I.e.
    # # z = prox_fn(beta_, alpha_ * lambda_)
    beta_old_ = tt_inputs([prox_step_])[0]
    lambda_old_ = tt_inputs([prox_step_])[1]
    z = tt_clone(prox_step_,
                 {beta_old_: beta_,
                  lambda_old_: alpha_ * lambda_old_})

    # # loss_z = loss_fn(z)
    # beta_old_, = filter(lambda x: x.name == "beta",
    #                     tt_inputs([loss_]))
    # loss_z = tt_clone(loss_,
    #                   {beta_old_: z})

    # # step_diff = z - beta_start_.squeeze()
    # # loss_diff = loss_z - loss_start_
    # # line_diff = alpha_ * (loss_diff - loss_grad_start_.T.dot(step_diff))
    # # line_diff -= step_diff.T.dot(step_diff) / 2.
    # # if line_diff <= obj_tol:
    # #     return z, alpha_, loss_z
    armijo_cond = theano.scan_module.until(
        # loss_z[0] <= 1e-6
        z.sum() <= 1e-6
    )

    # # alpha_ *= bt_rate
    # # assert alpha_ >= 0, 'invalid step size: {}'.format(alpha_)

    # DEBUG: theano.printing.Print(a_new)

    # Turn `z` into a row vector.
    z_ret = z.reshape((-1, 1)).T
    return z_ret, armijo_cond

alpha_tt = tt.scalar('alpha')
alpha_tt.tag.test_value = 1
# lim = tt.as_tensor_variable(10)
# a_seq_tt = tt.as_tensor_variable(np.r_[0, 0])
b_seq_tt = tt.alloc([beta_tt], 1, beta_tt.shape[0])
bt_ls_vals, bt_ls_ups = theano.scan(backtrack_one_step,
                                    outputs_info=[
                                        # dict(initial=a_seq_tt, taps=[-2, -1]),
                                        dict(initial=b_seq_tt, taps=[-1]),
                                    ],
                                    non_sequences=[
                                        alpha_tt,
                                        loss_grad_tt,
                                        prox_step_tt,
                                    ],
                                    n_steps=1000
                                    )

bt_ls_scan = tt_function([beta_tt, alpha_tt, lambda_tt], bt_ls_vals, updates=bt_ls_ups)

test_scan_res = bt_ls_scan(beta_0, 1, 1)
test_scan_res
test_scan_res.shape
@

\section{Examples}

First, we need to set up the basic functions, which--in this case--are
constructed from the Theano graphs.
<<prox_gradient_setup, echo=True, evaluate=True>>=
lambda_tt = tt.scalar('lambda')
lambda_tt.tag.test_value = 1

prox_fn = tt_function([beta_tt, lambda_tt],
                      tt_soft_threshold(beta_tt, lambda_tt))

prox_grad_step_fn, loss_grad = prox_gradient_step(
    nlogl, beta_tt, tt_soft_threshold,
    return_loss_grad=True)

loss_fn = tt_function([beta_tt], nlogl)
loss_grad_fn = tt_function([beta_tt], loss_grad)

cols_fns = [
    (lambda i, b: i, r'$i$'),
    (lambda i, b: np.asscalar(loss_fn(b)),
        r'$l(\beta^{(i)})$'),
    (lambda i, b: np.linalg.norm(b - beta_true, 2),
        r'$\|\beta^{(i)} - \beta^*\|^2_2$')
]
@

For a baseline comparison--and sanity check--we'll use the \texttt{cvxpy}
library \citep{diamond_cvxpy:_2016}.
<<cvxopt_run, echo=True, evaluate=True, results='hidden'>>=
import cvxpy as cvx

beta_var_cvx = cvx.Variable(M, name='beta')
lambda_cvx = 1e-2 * lambda_max * N

cvx_obj = cvx.Minimize(0.5 * cvx.sum_squares(y - X * beta_var_cvx)
                       + lambda_cvx * cvx.norm(beta_var_cvx, 1) )
cvx_prob = cvx.Problem(cvx_obj)

_ = cvx_prob.solve(solver=cvx.CVXOPT, verbose=True)

beta_cvx = np.asarray(beta_var_cvx.value).squeeze()
loss_cvx = loss_fn(beta_cvx)
beta_cvx_err = np.linalg.norm(beta_cvx - beta_true, 2)
@

We now have the necessary pieces to perform an example estimation.
We'll start with an exceedingly large step size and let backtracking
line-search find a good value.
<<iterative_run_func, echo=False, evaluate=True>>=
def iterative_run(step_fn, loss_fn_, cols_fns,
                  max_iters=1000, stop_tol=1e-5,
                  stop_loss=True):

    beta_val = step_fn.beta_0
    loss_val = -np.inf

    iter_values = [[col_fn(0, beta_val) for col_fn, _ in cols_fns]]
    for i in xrange(1, max_iters):
        beta_prev = np.copy(beta_val)
        loss_prev = loss_val

        beta_val = step_fn.step(beta_val)
        loss_val = loss_fn_(beta_val)

        # TODO: Could stop on either or both criteria.
        if stop_loss:
            # stop_diff = np.abs(loss_val - loss_prev)
            stop_diff = np.sum(np.square(loss_val - loss_prev))
            stop_diff /= loss_prev**2
        else:
            stop_diff = np.linalg.norm(beta_val - beta_prev, 2)
            # stop_diff /= np.linalg.norm(beta_prev, 2)

        iter_values.append([col_fn(i, beta_val)
                            for col_fn, _ in cols_fns])

        if i > 0 and stop_diff < stop_tol:
            break

    columns = zip(*cols_fns)[1]

    iters_df = pd.DataFrame(iter_values, columns=columns)

    return iters_df, beta_val
@
<<prox_gradient_class, echo=True, evaluate=True>>=
class ProxGradient(object):

    def __init__(self, y, X, beta_0,
                 prox_fn_, loss_fn_, loss_grad_fn_,
                 alpha_0):

        self.y = y
        self.X = X
        self.alpha_val = alpha_0
        self.beta_0 = beta_0
        self.N, self.M = X.shape
        self.prox_fn_ = prox_fn_
        self.loss_fn_ = loss_fn_
        self.loss_grad_fn_ = loss_grad_fn_

    def step(self, beta):
        beta_val = np.copy(beta)

        beta_val, self.alpha_val, _ = backtracking_search(
            beta_val, self.alpha_val,
            self.prox_fn_, self.loss_fn_, self.loss_grad_fn_)

        return beta_val
@
<<prox_gradient_run, echo=True, evaluate=True>>=
beta_0 = np.zeros(M).astype('float64')
lambda_val = 1e-2 * lambda_max
pg_step = ProxGradient(y, X, beta_0,
                       lambda x, a: prox_fn(x, N * lambda_val * a),
                       loss_fn, loss_grad_fn, 10)

pg_cols_fns = cols_fns + [(lambda *args, **kwargs: pg_step.alpha_val, r'$\alpha$')]
pg_est_data, _ = iterative_run(pg_step, loss_fn, pg_cols_fns)
pg_ls_data = pd.DataFrame(pg_est_data)
# pg_ls_data = pg_ls_data.append(pg_est_data, ignore_index=True)
@
<<pg_ls_plot, echo=False, fig=True, caption=r'Minimization by proximal gradient with backtracking line-search.'>>=
pg_ls_plot_data = pg_ls_data.set_index(r'$i$')
pg_ls_plot_data = pg_ls_plot_data.stack()
pg_ls_plot_data = pg_ls_plot_data.reset_index(level=1,
                                              name='value')

pg_ls_plot_data.rename_axis({'level_1': 'measure'},
                            axis=1, inplace=True)

g = sns.FacetGrid(pg_ls_plot_data,
                  row='measure',
                  sharey=False, sharex=True,
                  legend_out=False)

_ = g.map(plt.plot, r'value')

g.axes[0, 0].hlines(loss_cvx, 0,
                    pg_ls_plot_data.index.get_values().max(),
                    linestyles='dashed',
                    label='cvx')

g.axes[1, 0].hlines(beta_cvx_err, 0,
                    pg_ls_plot_data.index.get_values().max(),
                    linestyles='dashed',
                    label='cvx')

g.axes[0, 0].legend()
g.axes[1, 0].legend()
g.axes[0, 0].set_yscale('symlog')

plt.tight_layout()

g.axes[0, 0].set_xbound(-2,
                        pg_ls_plot_data.index.get_values().max() + 1)

@

\Cref{fig:pg_ls_plot} shows the results for proximal gradient steps alongside
the step size changes due to backtracking line-search.  Regarding the latter,
for our example a sufficient step size is found in the first few iterations, so
the overall result isn't too interesting.  Fortunately, this sort of behaviour
isn't uncommon, and it makes line-search quite effective in practice.

The other most noticeable result in \Cref{fig:pg_ls_plot} is the quick
decrease in loss for the first 20 or so iterations, then the very
gradual convergence iterates $\beta^{(i)}$.  This is nice property to
contrast with the coordinate-wise estimation in the next section, so
we'll revisit that; however, it's worth mentioning now that the choice of
stopping criteria should probably be informed by the optimization method
used and its convergence properties--among other things.

\section{Coordinate-wise Estimation}

Given that our loss is a composition of $\ell_2$ and a linear operator
of finite dimension, we can conveniently exploit some conditional
separability to obtain simple estimation steps in each coordinate.
This effectively characterizes coordinate--or cyclic--descent in our case,
and it shows up as a common technique in the estimation of $\ell_1$ models
\citep{friedman_pathwise_2007, mazumder_regularization_2009,
scikit-learn_sklearn.linear_model.elasticnet_2017}.

% See `sk.linear_model.ElasticNet.path`, `cd_fast.sparse_enet_coordinate_descent`
% in sklearn/linear_model/coordinate_descent.py for a proper comparison

From a more statistical perspective, the basics of coordinate-wise methods
begin with the ``partial residuals'', $r_{-m} \in \Re^{N}$, discussed in
\citet{friedman_pathwise_2007} and implicitly defined for our problem by
\begin{equation}
  \begin{aligned}
    \beta^*
    &= \argmin_{\beta} \left\{
      \frac12
      \|
	y - X(\beta - e_m \beta_m)
	%y - \sum_{m^\prime \neq m} X_{\cdot,m^\prime} \beta_{m^\prime}
	- X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
      \right\}
    \\
    &= \argmin_{\beta} \left\{
      \frac12
      \|r_{-m} - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
    \right\}
  \;.
  \end{aligned}
  \label{eq:partial_resid}
\end{equation}
The last expression hints at the most basic idea behind the coordinate-wise
approach: conditional minimization in each $m$.  Its exact solution in
each coordinate is given by the aforementioned soft thresholding function,
which--as we've already stated--is a proximal operator.  In symbols,
$\prox_{\lambda \left|\cdot\right|}(x) = \operatorname{S}_\lambda(x)$, where
the latter is the soft thresholding operator.


Now, if we wanted to relate this type of coordinate-descent to the much more
general statement of the proximal gradient solution in
\Cref{eq:forward-backward}, we would use a result like the
following:

\begin{proposition}
  For $X$ such that $\1^\top X e_m = 0$ and
  $e^\top_m X^\top X e_m = 1$, $m \in \{1, \dots, M\}$,
  the coordinate-wise step of the Lasso in
  \citet[Equation (9)]{friedman_pathwise_2007},
  \begin{equation}
    \beta_m = \operatorname{S}_{\lambda}\left[
      \sum_{n}^N X_{n,m} \left(
      y_n - \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime}
      \right)
    \right]
    \;,
  \end{equation}
  has a proximal gradient fixed-point solution under a Euclidean basis
  decomposition with the form
  \begin{equation}
    \beta =
    \sum^M_m \prox_{\alpha \lambda \phi}\left[
      e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
    \right] e_m
    \;.
    \label{eq:prox_grad_descent}
  \end{equation}

  \begin{proof}
  We start with an expansion of the terms in
  $\prox_{\lambda \phi} \equiv \operatorname{S}_\lambda$.
  After simplifying the notation with
  \begin{gather*}
    \sum^N_{n} X_{n,m} z_n = e^\top_m X^\top z, \quad \text{and} \quad
    \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime} =
    X \left(\beta - \beta_m e_m \right)
    \;,
  \end{gather*}
  the expanded argument of $\operatorname{S}$ reduces to
  \begin{equation*}
    \begin{aligned}
      e^\top_m X^\top \left(y - X\left( \beta - e_m \beta_m\right)\right)
      &= e^\top_m X^\top X e_m \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &= \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &= e^\top_m \left(\beta + X^\top \left(y - X \beta\right)\right)
    \end{aligned}
  \end{equation*}
  where the last step follows from $X$ standardization.
  This establishes the relationship with \Cref{eq:forward-backward}
  only component-wise.  The connection to full vector $\beta$ iterations
  in \Cref{eq:forward-backward} comes from the following orthogonal basis
  property of proximal operators:
  \begin{equation}
    \prox_{\lambda \phi \circ e^\top_m}(z) =
    \sum^M_m \prox_{\lambda \phi}\left(e^\top_m z\right) e_m
    \;.
    \label{eq:prox_ortho_basis}
  \end{equation}
  This together with $z = \beta - \alpha \nabla l(\beta)$ yields the
  proximal gradient fixed-point statement, i.e.
  \begin{equation*}
    \begin{aligned}
      \beta
      &=
      \sum^M_m \prox_{\alpha \lambda \phi}\left[
	e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
      \right] e_m
      \\
      &=
      \sum^M_m \prox_{\alpha \lambda \phi}\left(
      \beta_m + \alpha e_m^\top X^\top \left(y - X \beta \right)
      \right) e_m
      \;.
    \end{aligned}
  \end{equation*}
  \end{proof}
\end{proposition}

Association with the proximal gradient algorithm can provide a fairly general
means of expand and characterizing existing coordinate descent approaches.
Perhaps the most readily available improvements involve sequence acceleration
\citep{beck_fast_2014}.

\begin{remark}
  \label{rem:bases}
  The property in \Cref{eq:prox_ortho_basis} can be extended to other
  orthonormal bases.  This might be a means of relating $X$-based
  orthogonalizations of a regression problem, enforce different forms
  of sparsity on $\beta$ estimates, or to simply investigate
  convergence properties.
\end{remark}

\subsection{Implementation}

The following performs a standard form of coordinate descent:
<<coord_desc_class, echo=True, evaluate=True>>=
class CoordDescent(object):

    def __init__(self, y, X, beta_0, prox_fn_, col_seq=None):

        self.y = y
        self.X = X
        self.beta_0 = beta_0
        self.N, self.M = X.shape
        self.Xb = np.dot(self.X, self.beta_0)
        self.prox_fn_ = prox_fn_

        # (Inverse) 2-norm of each column/feature, i.e.
        #   np.reciprocal(np.diag(np.dot(X.T, X)))
        #
        # Notice how much larger each coordinate step is compared
        # to `alpha_0`.
        self.alpha_vals = np.reciprocal((self.X**2).sum(axis=0))

        if col_seq is None:
            self.col_seq = np.arange(self.M)

    def reset(self):
        self.Xb = np.dot(self.X, self.beta_0)

    def step(self, beta):
        beta_val = np.copy(beta)

        for j in self.col_seq:
            X_j = self.X[:, j]
            alpha_val = self.alpha_vals[j]

            # A little cheaper to just subtract the column's contribution...
            self.Xb -= X_j * beta_val[j]

            Xt_r = np.dot(X_j.T, self.y - self.Xb) * alpha_val
            beta_val[j] = self.prox_fn_(np.atleast_1d(Xt_r), alpha_val)

            # ...and add the updated column back.
            self.Xb += X_j * beta_val[j]

        self.beta_last = beta_val

        return beta_val
@

<<coord_desc_example, echo=True, evaluate=True>>=
beta_0 = np.zeros(M).astype('float64')
lambda_val = 1e-2 * lambda_max
cd_step = CoordDescent(y, X, beta_0,
                       lambda x, a: prox_fn(x, N * lambda_val * a))

cd_cols_fns = cols_fns + [(lambda *args, **kwargs: j, "replication")]

pg_coord_data = pd.DataFrame()
for j in range(15):
    est_data, _ = iterative_run(cd_step, loss_fn, cd_cols_fns)
    pg_coord_data = pg_coord_data.append(est_data,
                                         ignore_index=True)
    # Reset internal state of our step method, since we're
    # running multiple replications.
    cd_step.reset()
    np.random.shuffle(cd_step.col_seq)
@

<<pg_coord_plot, echo=False, fig=True, caption=r'Minimization by coordinate descent.'>>=
pg_coord_plot_data = pg_coord_data.set_index([r'$i$', "replication"])
pg_coord_plot_data = pg_coord_plot_data.stack(level=0)
pg_coord_plot_data = pg_coord_plot_data.reset_index(
    level=[-2, -1], name='value')

_ = pg_coord_plot_data.rename_axis({'level_2': 'measure'},
                                   axis=1, inplace=True)
sn_cp = sns.color_palette()[:1]
g = sns.FacetGrid(pg_coord_plot_data,
                  row='measure',
                  hue='replication',
                  palette=sn_cp,
                  sharey=False, sharex=True,
                  legend_out=False)

_ = g.map(plt.plot, r'value', alpha=0.3)

cvx_loss_lines = g.axes[0, 0].hlines(loss_cvx,
                                     *g.axes[0, 0].get_xbound(),
                                     linestyles='dashed',
                                     label='cvx')

cvx_err_lines = g.axes[1, 0].hlines(beta_cvx_err,
                                    *g.axes[1, 0].get_xbound(),
                                    linestyles='dashed',
                                    label='cvx')

cvx_loss_lgd = g.axes[0, 0].legend([cvx_err_lines], ['cvx'])
cvx_err_lgd = g.axes[1, 0].legend([cvx_loss_lines], ['cvx'])

g.axes[0, 0].set_yscale('symlog', linthreshy=1)
# g.axes[1, 0].set_yscale('linear')

plt.tight_layout()

g.axes[0, 0].set_xbound(-2,
                        pg_coord_plot_data.index.get_values().max() + 1)
@

\Cref{fig:pg_coord_plot} demonstrates how well coordinate descent can work on
some problems.  The number of iterations between coordinate descent and
``batch'' proximal gradient is fairly noticeable in this example, yet both
approach effectively the same limits as \texttt{cvx}.  It seems like the same
ideas behind most batched vs. non-batched gradient and sampling methods (e.g.
batched Gibbs sampling) are in effect here, as well.   In those cases, spectral
properties of the $X$, or $X^\top X$, space often play a big part, and this
idea connects back to our comment in \Cref{rem:bases}.

\subsubsection{Regularization Paths}

Also, due to the relatively fast convergence of coordinate descent, the
method is a little more suitable for the computation of regularization paths--
i.e. varying $\lambda$ between iterations.  There is much more to this
topic, but for simplicity let's just say that each $\lambda$ step
has a ``warm-start'' from the previous descent iteration.

Next, we make a small extension to demonstrate the computation
of regularization paths--using \texttt{lasso\_path} for comparison.
<<reg_path_example, echo=True, evaluate=True>>=
from sklearn.linear_model import lasso_path, enet_path

beta_0 = np.zeros(M).astype('float64')

lambda_path, beta_path, _ = lasso_path(X, y)
path_len = np.alen(lambda_path)

beta_last = beta_0
pg_path_data = pd.DataFrame()
for i, lambda_ in enumerate(lambda_path):
    cd_path_step = CoordDescent(y, X, beta_last,
                        lambda x, a: prox_fn(x, N * lambda_ * a))

    cd_cols_fns = cols_fns[1:] + [
        (lambda *args, **kwargs: lambda_, r'$\lambda$')]
    est_data, beta_last = iterative_run(cd_path_step, loss_fn,
                                        cd_cols_fns,
                                        stop_tol=1e-4,
                                        stop_loss=True)

    pg_path_data = pg_path_data.append(est_data.iloc[-1, :],
                                       ignore_index=True)

@
<<sk_lasso_path, echo=True, evaluate=True>>=
cd_cols_fns = cols_fns[1:] + [
    (lambda *args, **kwargs: lambda_path[args[0]], r'$\lambda$')]

iter_values = []
for i, beta_ in enumerate(beta_path.T):
    iter_values.append([col_fn(i, beta_)
                        for col_fn, _ in cd_cols_fns])

sklearn_path_data = pd.DataFrame(iter_values,
                                 columns=zip(*cd_cols_fns)[1])
sklearn_path_data = sklearn_path_data.assign(
    replication=None, type='sklearn')

pg_path_data = pg_path_data.assign(type='pg')
pg_path_data = pg_path_data.append(sklearn_path_data,
                                   ignore_index=True)
@

<<pg_path_plot, echo=False, fig=True, caption=r'Regularization paths via coordinate descent.'>>=
pg_path_plot_data = pg_path_data.set_index(
    [r'$\lambda$', "type"])
# pg_path_plot_data = pg_path_plot_data.drop('replication', 1)

pg_path_plot_data = pg_path_plot_data.stack(level=0)
pg_path_plot_data = pg_path_plot_data.reset_index()

_ = pg_path_plot_data.rename_axis({'level_2': 'measure',
                                   0: 'value'},
                                   axis=1, inplace=True)

# sn_cp = sns.color_palette()[:1]
g = sns.FacetGrid(pg_path_plot_data,
                  row='measure',
                  hue='type',
                  # palette=sn_cp,
                  sharey=False, sharex=True,
                  legend_out=False)

def df_plot(x, y, color, label, **kwargs):
    data = kwargs.pop('data')
    data.plot(x=x, y=y, ax=plt.gca(),
              color=color, label=label,
              **kwargs)

_ = g.map_dataframe(df_plot,
                    r'$\lambda$', 'value',
                    alpha=0.7)
g.add_legend()
g.axes[0, 0].set_xscale('log')

plt.tight_layout()
@


% Now, let's devise an accelerated coordinate-wise algorithm
% using the forward-backward connection and the technique of
% \citet{teboulle}.

% \section{Non-convex Problems}
% One of our more recent projects involves a non-convex regularization term,
% which results from an approximation to the Horseshoe prior
% \citep{bhadra_horseshoe_2017}.
% Given as a complete data posterior--coordinate-wise--the problem is
% \begin{equation}
%   p(\beta_i, u_i \mid y_i, \tau) \propto
%   \exp\left(
%     -\frac{(y_i - \beta_i)^2}{2} - \frac{u_i \beta^2_i}{\tau}
%     + \log\left(1-e^{-u_i}\right) - \log(u_i)
%   \right)
% \end{equation}
% so that the MAP estimate can be stated as
% \begin{equation}
% \argmin_{\beta_i, u_i} \left\{
%     \frac{(y_i - \beta_i)^2}{2} + \frac{u_i \beta^2_i}{\tau}
%     - \log\left(1-e^{-u_i}\right) + \log(u_i)
%     \right\}
% \end{equation}
% <<echo=False, evaluate=False>>=
% # hslike_EM <- function(Y, X) {
% # 	a<-0.05
% # 	p<-ncol(X)
% # 	n<-length(Y)
% # 	tol = 1e-8
% # 	err=1
% # 	t=1
% # 	thetahat = rep(1,p)
% # 	thetahat=scale(t(X)%*%Y)
% # 	while (err>tol){
% # 		utilde = (1/(2*pi))*(a^1.5/(thetahat^2*(thetahat^2 +a)))
% # 		D=diag(as.vector(a/(2*utilde)))
% # 		DXT = D%*%t(X)
% # 		XTY = t(X)%*%Y
% # 		cov=D - DXT%*%(solve(X%*%DXT + diag(n)))%*%t(DXT)
% # 		thetahatnew = cov%*%XTY
% # 		anew = (a^1.5/(p*pi))*sum(1/(thetahatnew^2+a))
% # 		err<-sum((thetahatnew-thetahat)^2) + (a-anew)^2
% #         cat("EM Iteration ",t, "Error ", err, "a ", a, "\n")
% # 		thetahat=thetahatnew
% # 		a=anew
% # 		t<-t+1
% # 	}
% # 	return(list(thetahat=thetahat, ahat=a))
% # }
% @



\section{Discussion}

Maybe small steps in this direction could even motivate much larger changes
in our standard tools--like full support for sub-differentials and set-valued
results, instead of the overly restrictive point-wise characterizations
currently ubiquitous.  A similar idea is present in the symbolic
calculation of limits addressed using filters \citep{beeson_meaning_2005}, and
is somewhat analogous to the more modern jump from matrix and linear algebra
libraries to tensor libraries.
Our use of the proximal framework is, in part, motivated by its near seamless
use \emph{and} simultaneous bypassing of set-valued maps--in implementation, at
least.  In a sense, frameworks like these could be minor abstractions to
libraries with set-valued support.

The value provided by the symbolic tools we've discussed are not \emph{just}
their ability to act as compilers at a ``math level'', but more for their
ability to concretely encode mathematical characterizations of optimization
problems and methods.  Work in this direction is not new by any means; however,
the combination of open-source tools and industry interest in algorithms that
fall under the broad class of proximal methods (e.g. gradient descent, ADMM, EM,
etc.) provides a more immediate reason to pursue these abstractions in
code and automate their use.

\cite{wytock_new_2016} discusses a similar approach to optimization (i.e.
symbolic and with close coupling to useful mathematical abstractions), and
there are other good examples \citep{diamond_cvxpy:_2016} of effective
mathematical abstractions applied in code.
In most cases, these examples do not--however--attempt to place their implementations
upon a symbolic foundation and then realize their relevant
methodologies in that context.  Most often, the mathematical abstractions--or
just the methods--are directly implemented at the highest levels of
abstraction possible (e.g. individual functions for ADMM,
forward-backward/proximal gradient and Douglas-Rachford
\cite{svaiter_pyprox_2017}).  This is most common and it makes sense in terms
of simplicity, but offers none of the extensibility, generalization, or
possibly efficiency provided by shared efforts across related projects.
For example, in the context described here, implementations immediately
benefit from Theano's code conversion, parallelization and relevant
improvements to its basic graph optimizations.  The latter covers
both low-level implementation improvements--including automatic use
of relevant BLAS functions--as well as high-level tensor algebra
simplifications.
In a development community that builds from such tools, related efficiency and
performance gains could occur much more often and be much more specific
to certain areas of mathematics.

The utility of these encodings may manifest as--say--Theano optimizations that
make direct use of the orthonormal basis property in \Cref{eq:prox_ortho_basis}, or
the Moreau-Fenchel theorem, and automate consideration for
multiple estimation methods via splitting (e.g. ADMM, Douglas-Rachford,
etc.)--perhaps by making decisions based on operator/matrix properties one
could reasonably specific in this context.
In future installments we'll attempt to delve into the details of these
features.

\bibliographystyle{plainnat}
\bibliography{more-proximal-estimation}


\end{document}
