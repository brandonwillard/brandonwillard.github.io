\documentclass[12pt]{article}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{suffix}
\usepackage{color}

% Sadly, can't use this because it breaks greek letters.
%\usepackage[slantedGreek]{mathpazo}
% \usepackage{breqn}

\usepackage{todonotes}
\usepackage{draftwatermark}
\SetWatermarkScale{1}
\SetWatermarkLightness{0.90}

% used by Pweave
\usepackage{graphicx}

\definecolor{bg}{rgb}{0.95,0.95,0.95}
\usepackage[backgroundcolor=bg, topline=false, bottomline=false,
leftline=false, rightline=false]{mdframed}

<<pweave_code, echo=False, evaluate=False>>=
from pynoweb_tools.editor_utils import nvim_weave

input_file_base = nvim_weave(rel_figdir='../figures',
                             rel_outdir="./",
                             format_opts={'width': r'\textwidth',
                                          'figfmt': '.png',
                                          'savedformats': ['.png', '.pdf']})

# Compile document to markdown
assert os.system('make {}.pdf'.format(input_file_base)) == 0
assert os.system('make {}.md'.format(input_file_base)) == 0
@

\usepackage{minted}
\setminted{fontsize=\footnotesize, breaklines=true, breakanywhere=true,
breakautoindent=true}

% this order is important
%\PassOptionsToPackage{hyphens}{url}
\RequirePackage[hyphens]{url}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[authoryear]{natbib}

% Apparently cleveref must always be last.
\usepackage{cleveref}

\allowdisplaybreaks

\include{math-commands}

\graphicspath{{../../figures/}{../figures/}{./figures/}{./}}

\title{More Proximal Estimation}

\author{Brandon T. Willard}

\date{2017-03-06}

\begin{document}

\maketitle

\section{Introduction}

The focal point of this short exposition will be an elaboration of the basic
$\ell_1$ penalization problem discussed in \citet{willard_role_2017},
\begin{equation}
  \argmin_{\beta} \left\{
  \frac{1}{2} \|y - X \beta\|^2_2
    + \lambda \|\beta\|_1
  \right\}
  \;.
  \label{eq:lasso}
\end{equation}
We continue our discussion on topics concerning automation and
symbolic computation in Theano \citep{bergstra_theano_2010}, as
well as the mathematical methodology we believe is suitable for
such implementations.
Again, our framing of the problem is in terms of ``proximal methods''
\citep{parikh_proximal_2014, combettes_proximal_2011}.  Along the way
we propose one simple means of placing the well-known technique of coordinate
descent within the scope of proximal methods via a general property of proximal
operators.  These efforts are a continued outgrowth of our work in
\citet{polson_proximal_2015}.


% Although \Cref{eq:lasso} is strictly $\ell_1$--or Lasso, we simply allude to
% the relatively minor alterations that produce the ElasticNet, grouped Lasso
% and related models.  Under the proximal framework, the distinctions between
% these models do not generally lead to major theoretical re-workings, or the
% need to consider principles not already found within the proximal methods.

% For example, the transition from standard Lasso to grouped Lasso may not be
% trivial, but--within the proximal framework--the change amounts to an
% intermediate operator composition step \citep{argyriou_efficient_2011,
% hu_proximal}.
% This change can be introduced into existing proximal algorithms without
% significant alteration to the established steps.  In some cases the exact
% change can be entirely transparent and only result in nominally different
% arguments to existing functions.

\section{Proximal and Computational Components}

First, we [re]-introduce the workhorse of proximal methods: the
\emph{proximal operator}.

\begin{Def}[Proximal Operator]
  \begin{equation}
    \prox_{\phi}(x) =
    \argmin_{z} \left\{
    \frac{1}{2} \left(z - x\right)^2 + \phi(z)
    \right\}
    \;.
  \end{equation}
\end{Def}

<<python_setup, evaluate=True, echo=False, results='hidden'>>=
import numpy as np
import scipy as sc
import pandas as pd

import pymc3 as pm

import theano
import theano.tensor as tt
from theano.printing import debugprint as tt_print
from theano.printing import pprint as tt_pprint

import matplotlib.pyplot as plt

# %matplotlib inline
# %config InlineBackend.figure_format = 'svg'

import seaborn as sns

# plt.rcParams["figure.figsize"] = (12, 8)
# plt.rcParams['text.usetex'] = True
plt.rc('text', usetex=True)

sns.set(style="whitegrid")
sns.set_style("whitegrid", {"axes.linewidth": ".7"})

theano.config.mode = 'FAST_COMPILE'
@

Inspired by \Cref{eq:lasso}, we produce a toy dataset as follows:
<<simulate_data, evaluate=True, echo=True>>=
from theano import shared as tt_shared

M = 50
M_nonzero = M * 2 // 10

beta_true = np.zeros(M)
beta_true[:M_nonzero] = np.exp(-np.arange(M_nonzero)) * 100

N = int(np.alen(beta_true) * 0.4)
X = np.random.randn(N, M)
mu_true = X.dot(beta_true)
y = mu_true + sc.stats.norm.rvs(np.zeros(N), scale=10)

X_tt = tt_shared(X, name='X', borrow=True)
y_tt = tt_shared(y, name='y', borrow=True)

# Estimation starting parameters...
beta_0 = np.zeros(X.shape[1]).astype('float64')

# Gradient [starting] step size
alpha_0 = 1. / np.linalg.norm(X, 2)**2
# np.linalg.matrix_rank(X)

# Regularization value heuristic
# beta_ols = np.linalg.lstsq(X, y)[0]
# lambda_max = 0.1 * np.linalg.norm(beta_ols, np.inf)
lambda_max = np.linalg.norm(X.T.dot(y), np.inf)
@

As in \citet{willard_role_2017}, we can start with a model defined within a
system like PyMC3 \citep{salvatier_probabilistic_2016}.
<<create_pymc3_model, evaluate=True, echo=True>>=
with pm.Model() as lasso_model:
    beta_rv = pm.Laplace('beta', mu=0, b=1,
                         shape=X.shape[1])
    y_rv = pm.Normal('y', mu=X_tt.dot(beta_rv), sd=1,
                     shape=y.shape[0], observed=y_tt)
@
In this setting one might then arrive at the necessary steps toward estimation
automatically (i.e.  identify the underlying $\ell_1$ estimation problem).  We
discuss this more in \citet{willard_role_2017}.

For simplicity, we'll just assume that all components of the estimation problem
are know--i.e. loss and penalty functions.  The proximal operator
that arises in this standard example is the \emph{soft thresholding} operator.
In Theano, it can be implemented with the following:
<<theano_soft_thresholding, echo=True, evaluate=True>>=
def tt_soft_threshold(beta_, lambda_):
    return tt.sgn(beta_) * tt.maximum(tt.abs_(beta_) - lambda_, 0)
@

\begin{remark}
  This operator can take other forms, and the one used here is likely not
  the best.  The \texttt{maximum} can be replaced by other conditional-like
  statements--such as
  \begin{equation*}
    \operatorname{S}(z, \lambda) =
    \begin{cases}
     \sgn(\beta) (\beta - \lambda) & \beta > \lambda
     \\
     0 & \text{otherwise}
    \end{cases}
    \;.
  \end{equation*}
  If we were to--say--multiply the output of this operator with
  another, more difficult to compute result, then we might also wish to
  extend this multiplication into the definition of the operator and
  avoid its computation in the $\beta \leq \lambda$ case.

  Barring any reuses of this quantity, or a need to preserve undefined
  results produced by an expensive product with zero, we would ideally
  like a "compiler" to make such an optimization itself.  It isn't
  clear how a standard compiler--or interpreter/hybrid--could
  safely make this optimization, whereas it does seem more reasonable
  as a symbolic/Theano optimization.

  Optimizations like this are--I think--a necessary step to enable
  expressive, generalized methods, truly rapid prototyping at the math
  level.
\end{remark}

<<theano_simplify_funcs, evaluate=True, echo=False, results='hidden'>>=
#
# Check out the Theano options
#
# print(theano.opt.config)
# FYI: Use a theano.gof.NavigatorOptimizer
#
from theano.gof import Query
from theano.compile import optdb
from theano.gof.graph import inputs as tt_inputs

# theano.opt.config.optimizer_verbose = True
# theano.opt.config.metaopt_verbose = True
theano.opt.config.exception_verbosity = 'high'

# Some details:
# theano.compile.mode.optdb._names
# theano.compile.mode.optdb.print_summary()
# theano.compile.get_default_mode()

# fast_run = optdb.query(Query(include=['fast_run']))
our_opts = optdb.query(Query(include=['specialize',
                                      'canonicalize',
                                      'stabilize']))

def to_fgraph(outs, ins=None):

    if not isinstance(outs, (list, tuple)):
        outs = [outs]

    if ins is None:
        ins = tt_inputs(outs)
    elif not isinstance(ins, (list, tuple)):
        ins = [ins]

    # This first way loses all connection between the original
    # nodes and the cloned ones if `copy_inputs=True`:
    # ins, outs = theano.gof.graph.clone(ins, outs,
    #                                    copy_inputs=False)
    # This is how you can keep that mapping:
    # ins_map = theano.gof.graph.clone_get_equiv(ins, outs,
    #                                            # Set this to False if you want
    #                                            # to keep using the original
    #                                            # top-level inputs, but new
    #                                            # inbetween nodes.
    #                                            copy_inputs_and_orphans=True)
    # ins, outs = [ins_map[in_] for in_ in ins], [ins_map[out_] for in_ in outs]

    fgraph = theano.gof.FunctionGraph(ins, outs)

    return fgraph

def tt_simplify(fgraph):

    if not isinstance(fgraph, theano.gof.fg.FunctionGraph):
        fgraph = to_fgraph(fgraph)

    #fgraph = fgraph.clone()
    res = our_opts.optimize(fgraph)

    return fgraph
@

<<echo=False, evaluate=False>>=
#
# A small demo:
#
# neg_sum_test = create_fgraph(-tt.sum(-y_rv))
# neg_logl_test = create_fgraph(-tt.sum(-(y_rv - X_tt.dot(beta_rv))**2))
#
# # tt_pprint(neg_sum_test.outputs[0])
# tt_pprint(neg_logl_test.outputs[0])
#
# our_opt.optimize(neg_logl_test)
#
# tt_pprint(neg_logl_test.outputs[0])

# TODO: What about fusion?
# theano.config.tensor.local_elemwise_fusion = True
# optdb.query(Query(include=['elemwise_fusion'])).optimize(nlogl_graph)

from theano import clone as tt_clone

# Clone the negative log-likelihood of our observation model.
nlogl_rv = -lasso_model.observed_RVs[0].logpt
# nlogl = tt_clone(nlogl_rv, {beta_rv: beta_tt})
nlogl = tt_clone(nlogl_rv)
nlogl.name = "-logl"

# beta_tt = tt_shared(beta_0, name='beta_prox')

# Get a Theano function graph.
# nlogl_graph = to_fgraph(nlogl, ins=[y_tt, X_tt, beta_tt])
nlogl_graph = to_fgraph(nlogl, ins=[y_tt, X_tt, beta_rv])

# Perform some Theano simplifications manually.
# Could also do something like:
#    tt_function([beta_rv], [nlogl], mode="FAST_RUN").maker.fgraph
# TODO: The opts still aren't getting rid of the negative sign; eh.
nlogl_graph = tt_simplify(nlogl_graph)
# beta_clone = nlogl_graph.inputs[-1]
beta_tt = nlogl_graph.inputs[-1]


# Clone the result and use it from here on.
# nlogl_out = tt_clone(nlogl_graph.outputs[0],
#                      {beta_clone: beta_tt})
nlogl_out = nlogl_graph.outputs[0]
@

Now, assuming that we've obtained the relevant loss and penalty functions--for
example, in PyMC3--then we can proceed to setting up the exact context of our
proximal problem.
<<example_theano_graph_step, echo=True, evaluate=True, results='hidden'>>=
from theano import clone as tt_clone

# Clone the negative log-likelihood of our observation model.
nlogl_rv = -lasso_model.observed_RVs[0].logpt
nlogl = tt_clone(nlogl_rv)
nlogl.name = "-logl"
beta_tt = tt_inputs([nlogl])[4]
@

\section{Proximal Gradient}

In what follows it will be convenient to generalize a bit and work in terms of
arbitrary loss and penalty functions $l$ and $\phi$, respectively, which in our
case corresponds to
\begin{gather*}
  l(\beta) = \frac12 \|y - X \beta\|^2_2, \quad
  \text{and}\;
  \phi(\beta) = \|\beta\|_1
  \;.
\end{gather*}

The proximal gradient \citep{combettes_proximal_2011} algorithm is a staple of
the proximal framework that provides solutions to problems of the form
\begin{equation}
  \argmin_\beta \left\{
    l(\beta) + \lambda \phi(\beta)
  \right\}
  \;,
\end{equation}
when both $l$ and $\phi$ are lower semi-continuous convex
functions, and $l$ is differentiable with Lipschitz gradient.

The solution is given as the following fixed-point:
\begin{equation}
  \beta = \prox_{\alpha \lambda \phi}(\beta - \alpha \nabla l(\beta))
  \;.
  \label{eq:forward-backward}
\end{equation}
The constant step size $\alpha$ is related to the Lipschitz constant
of $\nabla l$, but can also be a sequence obeying certain constraints.
Since our $l$ under consideration is $\ell_2$, we have the incredibly
standard $\nabla l(\beta) = X^\top (X \beta - y)$.

\subsection{Implementation}

As in \citet{willard_role_2017}, we provide an implementation of a
proximal gradient step.
<<prox_gradient_step, echo=True, evaluate=True>>=
from theano import function as tt_function
from theano.compile.nanguardmode import NanGuardMode

tt_func_mode = NanGuardMode(nan_is_error=True,
                            inf_is_error=False,
                            big_is_error=False)


def prox_gradient_step(loss, beta_tt, prox_func,
                       alpha_tt=None, lambda_tt=None,
                       return_loss_grad=False,
                       tt_func_kwargs={'mode': tt_func_mode}
                       ):
    r""" Creates a function that produces a proximal gradient step.

    Arguments
    =========
    loss: TensorVariable
        Continuously differentiable "loss" function in the objective
        function.
    beta_tt: TensorVariable
        Variable argument of the loss function.
    prox_fn: function
        Function that computes the proximal operator for the "penalty"
        function.  Must take two parameters: the first a TensorVariable
        of the gradient step, the second a float or Scalar value.
    alpha_tt: float, Scalar (optional)
        Gradient step size.
    lambda_tt: float, Scalar (optional)
        Additional scalar value passed to `prox_fn`.
        TODO: Not sure if this should be here; is redundant.
    """
    loss_grad = tt.grad(loss, wrt=beta_tt)
    loss_grad.name = "loss_grad"

    if alpha_tt is None:
        alpha_tt = tt.scalar(name='alpha')
        alpha_tt.tag.test_value = 1
    if lambda_tt is None:
        lambda_tt = tt.scalar(name='lambda')
        lambda_tt.tag.test_value = 1

    beta_grad_step = beta_tt - alpha_tt * loss_grad
    beta_grad_step.name = "beta_grad_step"

    prox_grad_step = prox_func(beta_grad_step, lambda_tt * alpha_tt)
    prox_grad_step.name = "prox_grad_step"

    inputs = []
    updates = None
    if isinstance(beta_tt, tt.sharedvar.SharedVariable):
        updates = [(beta_tt, prox_grad_step)]
    else:
        inputs += [beta_tt]
    if not isinstance(alpha_tt, tt.sharedvar.SharedVariable):
        inputs += [alpha_tt]
    if not isinstance(lambda_tt, tt.sharedvar.SharedVariable):
        inputs += [lambda_tt]

    prox_grad_step_fn = tt_function(inputs,
                                    prox_grad_step,
                                    updates=updates,
                                    **tt_func_kwargs)

    res = (prox_grad_step_fn,)
    if return_loss_grad:
        res += (loss_grad,)

    return res
@

\subsection{Step Sizes}

A critical aspect of the proximal gradient approach--and most
optimization--involves the use of appropriate step sizes, $\alpha$.  They
needn't always be fixed values, and, because of this, we can search for a
suitable value during estimation.  Furthermore, in some cases, step sizes can
be sequences amenable to acceleration techniques \citep{beck_fast_2014}.

These values have obvious connections to the performance of an optimization
method--beyond basic guarantees of convergence, so the power of any
implementation will depend on how much support it has for various types of step
size sequences.

Often acceptable ranges of step size values are derived from Lipschitz
and related properties of the functions involved--and/or their gradients.
Similar considerations underlie the classical line-search methods in
optimization, and give meaning to what some call ``tuning parameters''.
These connections between function-analytic properties and
``tuning parameters'' themselves highlight the need for more mathematical
coverage within implementations--by which we imply their place in a fully
computational, symbolic setting.

In this spirit, one particularly relevant direction of work can be found in
Theano's experimental matrix ``Hints''.  The ideas behind
\texttt{theano.sandbox.linalg.ops.\{psd, spectral\_radius\_bound\}}
examples of the machinery needed to automatically determine
applicable and efficient $\alpha$ constants and sequences.

In our example, we use the standard backtracking line-search.
<<backtracking_search, echo=True, evaluate=True>>=
def backtracking_search(beta_, alpha_,
                        prox_fn, loss_fn, loss_grad_fn,
                        lambda_=1, bt_rate=0.5, obj_tol=1e-5):
    # alpha_start = alpha_
    z = beta_
    beta_start_ = beta_
    loss_start_ = loss_fn(beta_)
    loss_grad_start_ = loss_grad_fn(beta_)
    while True:

        beta_ = beta_start_ - alpha_ * loss_grad_start_
        z = prox_fn(beta_, alpha_ * lambda_)

        loss_z = loss_fn(z)
        step_diff = z - beta_start_
        loss_diff = loss_z - loss_start_
        line_diff = alpha_ * (loss_diff - loss_grad_start_.T.dot(step_diff))
        line_diff -= step_diff.T.dot(step_diff) / 2.

        if line_diff <= obj_tol:
            return z, alpha_, loss_z

        alpha_ *= bt_rate
        assert alpha_ >= 0, 'invalid step size: {}'.format(alpha_)
@

\begin{remark}
  Routines like this that make use of the gradient and other quantities might
  also be good candidates for execution in Theano, if only because of the graph
  optimizations that are able to remedy obviously redundant computations.

  In this vein, we could consider performing the line-search, and/or the entire
  optimization loop, within a Theano \texttt{scan} operation.  We could also
  create \texttt{Op}s that represents gradient and line-search step.  These
  might make graph construction much simpler, and be more suited for the
  current optimization framework.

  Although \texttt{scan} and tighter Theano integration may not on average
  produce better results than our current use of its compiled functions, we
  still wish to emphasize the possibilities.

  Likewise, an \texttt{Op} for the proximal operator might also be necessary for
  solving proximal operators automatically in closed-form (when possible)
  within a graph.
  This is based on the standard use of lookup tables combined with sets of
  algebraic relationships and identities used in symbolic algebra libraries for
  automatic differentiation and integration.  The same can be done to extend
  the coverage of known closed-form solutions to proximal operators in an
  automated setting.

\end{remark}

<<echo=False, evaluate=False>>=
gamma_ls_upper_tt = tt.nlinalg.norm(X_tt.T.dot(y_tt), 'inf')

print(gamma_ls_upper_tt.eval())

step_rate = tt_shared(0.6, 'step_rate')

def backtrack_one_step(beta_prev, beta_cur,
                       logl_prev, logl_cur,
                       logl_grad_prev, logl_grad_cur,
                       gamma_prox_prev, gamma_prox_cur,
                       step_rate_):

    # beta_val, logl_val, logl_grad_val, mse_val = prox_step()
    # step_vec = beta_val - beta_val_prev
    # logl_quad_expans = logl_val_prev + logl_grad_val_prev.T.dot(step_vec)
    # logl_quad_expans += 0.5 / lambda_1_tt * step_vec.T.dot(step_vec)
    # if logl_val <= logl_quad_expans:
    #     break
    # if lambda_1_tt.get_value() <= 0:
    #     raise ValueError('invalid step size: {}'.format(
    #         lambda_1_tt.get_value()))
    # # Could just make this an argument to prox_step...
    # lambda_1_tt.set_value(lambda_1_tt.get_value() * bt_rate)

    # step_vec = beta_cur - beta_prev
    # logl_quad_expans = logl_prev + logl_grad_prev.T.dot(step_vec)
    # logl_quad_expans += 0.5 / gamma_prox_cur * step_vec.T.dot(step_vec)

    # armijo_cond = logl_cur <= logl_quad_expans
    # res = (beta_cur, logl_cur, logl_grad_cur, gamma_prox_cur * step_rate_)

    return res + (theano.scan_module.until(armijo_cond),)


bt_ls_vals, bt_ls_ups = theano.scan(backtrack_one_step,
                                    outputs_info = [dict(initial=beta_tt, taps=[-2, -1]),
                                                    dict(initial=logl, taps=[-2, -1]),
                                                    dict(initial=logl_grad, taps=[-2, -1]),
                                                    dict(initial=lambda_tt, taps=[-2, -1])
                                                    ],
                                    non_sequences = [step_rate],
                                    n_steps = 1000)

backtrack_search = tt_function([step_rate], bt_ls_vals, updates=bt_ls_ups)
@
<<echo=False, evaluate=False>>=
# Scan tests

prox_fn = tt_function([beta_tt, lambda_tt],
                      tt_soft_threshold(beta_tt, lambda_tt))

prox_step_tt = tt_clone(prox_fn.maker.fgraph.outputs[0],
                        {prox_fn.maker.fgraph.inputs[0]: beta_tt,
                         prox_fn.maker.fgraph.inputs[1]: lambda_tt})

prox_grad_step_fn, loss_grad = prox_gradient_step(
    nlogl, beta_tt, tt_soft_threshold,
    return_loss_grad=True)

prox_grad_step_tt = prox_grad_step_fn.maker.fgraph.outputs[0]

loss_fn = tt_function([beta_tt], nlogl)

loss_tt = loss_fn.maker.fgraph.outputs[0]

loss_grad_fn = tt_function([beta_tt], loss_grad)


# loss_grad_tt = loss_grad_fn.maker.fgraph.outputs[0]
loss_grad_tt = tt_clone(loss_grad_fn.maker.fgraph.outputs[0],
                        {loss_grad_fn.maker.fgraph.inputs[0]: beta_tt})
loss_grad_tt.tag.test_value = beta_tt.tag.test_value

# beta_val_tt, logl_val_tt, logl_grad_val_tt, mse_val_tt, beta_prox_tt = prox_step.maker.fgraph.outputs

def backtrack_one_step(beta_start_,
                       alpha_,
                       loss_grad_start_,
                       prox_step_):
    beta_ = beta_start_.squeeze() - alpha_ * loss_grad_start_

    # Custom step for this new beta.  I.e.
    # # z = prox_fn(beta_, alpha_ * lambda_)
    beta_old_ = tt_inputs([prox_step_])[0]
    lambda_old_ = tt_inputs([prox_step_])[1]
    z = tt_clone(prox_step_,
                 {beta_old_: beta_,
                  lambda_old_: alpha_ * lambda_old_})

    # # loss_z = loss_fn(z)
    # beta_old_, = filter(lambda x: x.name == "beta",
    #                     tt_inputs([loss_]))
    # loss_z = tt_clone(loss_,
    #                   {beta_old_: z})

    # # step_diff = z - beta_start_.squeeze()
    # # loss_diff = loss_z - loss_start_
    # # line_diff = alpha_ * (loss_diff - loss_grad_start_.T.dot(step_diff))
    # # line_diff -= step_diff.T.dot(step_diff) / 2.
    # # if line_diff <= obj_tol:
    # #     return z, alpha_, loss_z
    armijo_cond = theano.scan_module.until(
        # loss_z[0] <= 1e-6
        z.sum() <= 1e-6
    )

    # # alpha_ *= bt_rate
    # # assert alpha_ >= 0, 'invalid step size: {}'.format(alpha_)

    # DEBUG: theano.printing.Print(a_new)

    # Turn `z` into a row vector.
    z_ret = z.reshape((-1, 1)).T
    return z_ret, armijo_cond

alpha_tt = tt.scalar('alpha')
alpha_tt.tag.test_value = 1
# lim = tt.as_tensor_variable(10)
# a_seq_tt = tt.as_tensor_variable(np.r_[0, 0])
b_seq_tt = tt.alloc([beta_tt], 1, beta_tt.shape[0])
bt_ls_vals, bt_ls_ups = theano.scan(backtrack_one_step,
                                    outputs_info=[
                                        # dict(initial=a_seq_tt, taps=[-2, -1]),
                                        dict(initial=b_seq_tt, taps=[-1]),
                                    ],
                                    non_sequences=[
                                        alpha_tt,
                                        loss_grad_tt,
                                        prox_step_tt,
                                    ],
                                    n_steps=1000
                                    )

bt_ls_scan = tt_function([beta_tt, alpha_tt, lambda_tt], bt_ls_vals, updates=bt_ls_ups)

test_scan_res = bt_ls_scan(beta_0, 1, 1)
test_scan_res
test_scan_res.shape
@

\section{Examples}

First, we need to set up the basic functions, which--in this case--are
constructed from the Theano graphs.
<<prox_gradient_setup, echo=True, evaluate=True>>=
lambda_tt = tt.scalar('lambda')
lambda_tt.tag.test_value = 1

prox_fn = tt_function([beta_tt, lambda_tt],
                      tt_soft_threshold(beta_tt, lambda_tt))

prox_grad_step_fn, loss_grad = prox_gradient_step(
    nlogl, beta_tt, tt_soft_threshold,
    return_loss_grad=True)

loss_fn = tt_function([beta_tt], nlogl)
loss_grad_fn = tt_function([beta_tt], loss_grad)

cols_fns = [
    (lambda i, b: i, r'$i$'),
    (lambda i, b: np.asscalar(loss_fn(b)),
        r'$l(\beta^{(i)})$'),
    (lambda i, b: np.linalg.norm(b - beta_true, 2),
        r'$\|\beta^{(i)} - \beta^*\|^2_2$')
]
@

For a baseline comparison--and sanity check--we'll use the \texttt{cvxpy}
library \citep{diamond_cvxpy:_2016}.
<<cvxopt_run, echo=True, evaluate=True, results='hidden'>>=
import cvxpy as cvx

beta_var_cvx = cvx.Variable(M, name='beta')
lambda_cvx = 1e-2 * lambda_max * N

cvx_obj = cvx.Minimize(0.5 * cvx.sum_squares(y - X * beta_var_cvx)
                       + lambda_cvx * cvx.norm(beta_var_cvx, 1) )
cvx_prob = cvx.Problem(cvx_obj)

_ = cvx_prob.solve(solver=cvx.CVXOPT, verbose=True)

beta_cvx = np.asarray(beta_var_cvx.value).squeeze()
loss_cvx = loss_fn(beta_cvx)
beta_cvx_err = np.linalg.norm(beta_cvx - beta_true, 2)
@

We now have the necessary pieces to perform an example estimation.
We'll start with an exceedingly large step size and let backtracking
line-search find a good value.
<<iterative_run_func, echo=False, evaluate=True>>=
def iterative_run(step_fn, loss_fn_, cols_fns,
                  max_iters=1000, stop_tol=1e-5,
                  stop_loss=True):

    beta_val = step_fn.beta_0
    loss_val = -np.inf

    iter_values = [[col_fn(0, beta_val) for col_fn, _ in cols_fns]]
    for i in xrange(1, max_iters):
        beta_prev = np.copy(beta_val)
        loss_prev = loss_val

        beta_val = step_fn.step(beta_val)
        loss_val = loss_fn_(beta_val)

        # TODO: Could stop on either or both criteria.
        if stop_loss:
            # stop_diff = np.abs(loss_val - loss_prev)
            stop_diff = np.sum(np.square(loss_val - loss_prev))
            stop_diff /= loss_prev**2
        else:
            stop_diff = np.linalg.norm(beta_val - beta_prev, 2)
            # stop_diff /= np.linalg.norm(beta_prev, 2)

        iter_values.append([col_fn(i, beta_val)
                            for col_fn, _ in cols_fns])

        if i > 0 and stop_diff < stop_tol:
            break

    columns = zip(*cols_fns)[1]

    iters_df = pd.DataFrame(iter_values, columns=columns)

    return iters_df, beta_val
@
<<prox_gradient_class, echo=True, evaluate=True>>=
class ProxGradient(object):

    def __init__(self, y, X, beta_0,
                 prox_fn_, loss_fn_, loss_grad_fn_,
                 alpha_0):

        self.y = y
        self.X = X
        self.alpha_val = alpha_0
        self.beta_0 = beta_0
        self.N, self.M = X.shape
        self.prox_fn_ = prox_fn_
        self.loss_fn_ = loss_fn_
        self.loss_grad_fn_ = loss_grad_fn_

    def step(self, beta):
        beta_val = np.copy(beta)

        beta_val, self.alpha_val, _ = backtracking_search(
            beta_val, self.alpha_val,
            self.prox_fn_, self.loss_fn_, self.loss_grad_fn_)

        return beta_val
@
<<prox_gradient_run, echo=True, evaluate=True>>=
beta_0 = np.zeros(M).astype('float64')
lambda_val = 1e-2 * lambda_max
pg_step = ProxGradient(y, X, beta_0,
                       lambda x, a: prox_fn(x, N * lambda_val * a),
                       loss_fn, loss_grad_fn, 10)

pg_cols_fns = cols_fns + [(lambda *args, **kwargs: pg_step.alpha_val, r'$\alpha$')]
pg_est_data, _ = iterative_run(pg_step, loss_fn, pg_cols_fns)
pg_ls_data = pd.DataFrame(pg_est_data)
# pg_ls_data = pg_ls_data.append(pg_est_data, ignore_index=True)
@
<<pg_ls_plot, echo=False, fig=True, caption=r'Minimization by proximal gradient with backtracking line-search.'>>=
pg_ls_plot_data = pg_ls_data.set_index(r'$i$')
pg_ls_plot_data = pg_ls_plot_data.stack()
pg_ls_plot_data = pg_ls_plot_data.reset_index(level=1,
                                              name='value')

pg_ls_plot_data.rename_axis({'level_1': 'measure'},
                            axis=1, inplace=True)

g = sns.FacetGrid(pg_ls_plot_data,
                  row='measure',
                  sharey=False, sharex=True,
                  legend_out=False)

_ = g.map(plt.plot, r'value')

g.axes[0, 0].hlines(loss_cvx, 0,
                    pg_ls_plot_data.index.get_values().max(),
                    linestyles='dashed',
                    label='cvx')

g.axes[1, 0].hlines(beta_cvx_err, 0,
                    pg_ls_plot_data.index.get_values().max(),
                    linestyles='dashed',
                    label='cvx')

g.axes[0, 0].legend()
g.axes[1, 0].legend()
g.axes[0, 0].set_yscale('symlog')

plt.tight_layout()

g.axes[0, 0].set_xbound(-0.5,
                        pg_ls_plot_data.index.get_values().max() + 0.5)

@

\Cref{fig:pg_ls_plot} shows a couple convergence measures for proximal gradient
steps alongside the step size changes due to backtracking line-search.
Regarding the latter, in our example a sufficient step size is found within
the first few iterations, so the overall result isn't too interesting.
Fortunately, this sort of behaviour isn't uncommon, which makes line-search
quite effective in practice.

\section{Coordinate-wise Estimation}

Given that our loss is a composition of $\ell_2$ and a linear operator
of finite dimension (i.e. $X$), we can conveniently exploit conditional
separability and obtain simple estimation steps in each coordinate.
This is, effectively, what characterizes coordinate--or cyclic--descent.
Since it is a common technique in the estimation of $\ell_1$ models
\citep{friedman_pathwise_2007, mazumder_regularization_2009,
scikit-learn_sklearn.linear_model.elasticnet_2017}, it's
worthwhile to consider how it can viewed in terms of proximal operators.

% See `sk.linear_model.ElasticNet.path`, `cd_fast.sparse_enet_coordinate_descent`
% in sklearn/linear_model/coordinate_descent.py for a proper comparison

From a statistical perspective, the basics of coordinate-wise methods
begin with the ``partial residuals'', $r_{-m} \in \Re^{N}$ discussed in
\citet{friedman_pathwise_2007}, and implicitly defined by
\begin{equation}
  \begin{aligned}
    \beta^*
    &= \argmin_{\beta} \left\{
      \frac12
      \|
	y - X(\beta - e_m \beta_m)
	%y - \sum_{m^\prime \neq m} X_{\cdot,m^\prime} \beta_{m^\prime}
	- X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \lambda \sum_{m^\prime \neq m} \left|\beta_{m^\prime}\right|
      \right\}
    \\
    &= \argmin_{\beta} \left\{
      \frac12
      \|r_{-m} - X e_m \cdot \beta_{m}\|^2_2
      + \lambda \left|\beta_m\right|
      + \dots
    \right\}
  \;.
  \end{aligned}
  \label{eq:partial_resid}
\end{equation}
The last expression hints at the most basic idea behind the coordinate-wise
approach: conditional minimization in each $m$.  Its exact solution in
each coordinate is given by the aforementioned soft thresholding function,
which--as we've already stated--is a proximal operator.  In symbols,
$\prox_{\lambda \left|\cdot\right|}(x) = \operatorname{S}_\lambda(x)$, where
the latter is the soft thresholding operator.


Now, if we wanted to relate \Cref{eq:partial_resid} a proximal method
via the statement of a proximal gradient fixed-point solution--i.e.
\Cref{eq:forward-backward}--we might use the following property
of proximal operators:
\begin{lemma}
  \label{lem:prox_ortho_basis}
  \begin{equation}
    \prox_{\lambda \phi \circ e^\top_m}(z) =
    \sum^M_m \prox_{\lambda \phi}\left(e^\top_m z\right) e_m
    \;.
  \end{equation}
  \begin{proof}
    See \citet{chaux_variational_2007}.
  \end{proof}
\end{lemma}

The next result yields our desired connection.
\begin{proposition}
  For $X$ such that $\1^\top X e_m = 0$ and
  $e^\top_m X^\top X e_m = 1$, $m \in \{1, \dots, M\}$,
  the coordinate-wise step of the Lasso in
  \citet[Equation (9)]{friedman_pathwise_2007},
  \begin{equation}
    \beta_m = \operatorname{S}_{\lambda}\left[
      \sum_{n}^N X_{n,m} \left(
      y_n - \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime}
      \right)
    \right]
    \;,
  \end{equation}
  has a proximal gradient fixed-point solution under a Euclidean basis
  decomposition with the form
  \begin{equation}
    \beta =
    \sum^M_m \prox_{\alpha \lambda \phi}\left[
      e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
    \right] e_m
    \;.
    \label{eq:prox_grad_descent}
  \end{equation}

  \begin{proof}
  We start with an expansion of the terms in
  $\prox_{\lambda \phi} \equiv \operatorname{S}_\lambda$.
  After simplifying the notation with
  \begin{gather*}
    \sum^N_{n} X_{n,m} z_n = e^\top_m X^\top z, \quad \text{and} \quad
    \sum^M_{m^\prime \neq m} X_{n,m^\prime} \beta_{m^\prime} =
    X \left(\beta - \beta_m e_m \right)
    \;,
  \end{gather*}
  the expanded argument of $\operatorname{S}$ reduces to
  \begin{equation*}
    \begin{aligned}
      e^\top_m X^\top \left(y - X\left( \beta - e_m \beta_m\right)\right)
      &= e^\top_m X^\top X e_m \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &= \beta_m + e^\top_m X^\top \left(y - X \beta\right)
      \\
      &= e^\top_m \left(\beta + X^\top \left(y - X \beta\right)\right)
    \end{aligned}
  \end{equation*}
  where the last step follows from $X$ standardization.
  This establishes the relationship with \Cref{eq:forward-backward}
  only component-wise.
  Using \Cref{lem:prox_ortho_basis} together with $z = \beta - \alpha \nabla
  l(\beta)$ yields the proximal gradient fixed-point statement, i.e.
  \begin{equation*}
    \begin{aligned}
      \beta
      &=
      \sum^M_m \prox_{\alpha \lambda \phi}\left[
	e^\top_m \left(\beta - \alpha \nabla l(\beta)\right)
      \right] e_m
      \\
      &=
      \sum^M_m \prox_{\alpha \lambda \phi}\left(
      \beta_m + \alpha e_m^\top X^\top \left(y - X \beta \right)
      \right) e_m
      \;.
    \end{aligned}
  \end{equation*}
  \end{proof}
\end{proposition}

\begin{remark}
  \label{rem:bases}
  The property in \Cref{lem:prox_ortho_basis} can used with other
  orthonormal bases--providing yet another connection between proximal methods
  and established dimensionality reduction and sparse estimation techniques
  \citep{chaux_variational_2007}.
  Also, this property provides a neat way to think about $X$-based
  orthogonalizations in estimations for regression and grouped-penalization
  problems.
\end{remark}

\subsection{Implementation}

The following performs a standard form of coordinate descent:
<<coord_desc_class, echo=True, evaluate=True>>=
class CoordDescent(object):

    def __init__(self, y, X, beta_0, prox_fn_, col_seq=None):

        self.y = y
        self.X = X
        self.beta_0 = beta_0
        self.N, self.M = X.shape
        self.Xb = np.dot(self.X, self.beta_0)
        self.prox_fn_ = prox_fn_

        # (Inverse) 2-norm of each column/feature, i.e.
        #   np.reciprocal(np.diag(np.dot(X.T, X)))
        self.alpha_vals = np.reciprocal((self.X**2).sum(axis=0))

        if col_seq is None:
            self.col_seq = np.arange(self.M)

    def reset(self):
        self.Xb = np.dot(self.X, self.beta_0)

    def step(self, beta):
        beta_val = np.copy(beta)

        for j in self.col_seq:
            X_j = self.X[:, j]
            alpha_val = self.alpha_vals[j]

            # A little cheaper to just subtract the column's contribution...
            self.Xb -= X_j * beta_val[j]

            Xt_r = np.dot(X_j.T, self.y - self.Xb) * alpha_val
            beta_val[j] = self.prox_fn_(np.atleast_1d(Xt_r), alpha_val)

            # ...and add the updated column back.
            self.Xb += X_j * beta_val[j]

        self.beta_last = beta_val

        return beta_val
@

Our example randomizes the order of coordinates to loosely demonstrate the
range of efficiency possible in coordinate descent.
<<coord_desc_example, echo=True, evaluate=True>>=
beta_0 = np.zeros(M).astype('float64')
lambda_val = 1e-2 * lambda_max
cd_step = CoordDescent(y, X, beta_0,
                       lambda x, a: prox_fn(x, N * lambda_val * a))

cd_cols_fns = cols_fns + [(lambda *args, **kwargs: j, "replication")]

pg_coord_data = pd.DataFrame()
for j in range(15):
    est_data, _ = iterative_run(cd_step, loss_fn, cd_cols_fns)
    pg_coord_data = pg_coord_data.append(est_data,
                                         ignore_index=True)
    # Reset internal state of our step method, since we're
    # running multiple replications.
    cd_step.reset()
    np.random.shuffle(cd_step.col_seq)
@

<<pg_coord_plot, echo=False, fig=True, caption=r'Minimization by coordinate descent.'>>=
pg_coord_plot_data = pg_coord_data.set_index([r'$i$', "replication"])
pg_coord_plot_data = pg_coord_plot_data.stack(level=0)
pg_coord_plot_data = pg_coord_plot_data.reset_index(
    level=[-2, -1], name='value')

_ = pg_coord_plot_data.rename_axis({'level_2': 'measure'},
                                   axis=1, inplace=True)
sn_cp = sns.color_palette()[:1]
g = sns.FacetGrid(pg_coord_plot_data,
                  row='measure',
                  hue='replication',
                  palette=sn_cp,
                  sharey=False, sharex=True,
                  legend_out=False)

_ = g.map(plt.plot, r'value', alpha=0.3)

cvx_loss_lines = g.axes[0, 0].hlines(loss_cvx,
                                     *g.axes[0, 0].get_xbound(),
                                     linestyles='dashed',
                                     label='cvx')

cvx_err_lines = g.axes[1, 0].hlines(beta_cvx_err,
                                    *g.axes[1, 0].get_xbound(),
                                    linestyles='dashed',
                                    label='cvx')

cvx_loss_lgd = g.axes[0, 0].legend([cvx_err_lines], ['cvx'])
cvx_err_lgd = g.axes[1, 0].legend([cvx_loss_lines], ['cvx'])

g.axes[0, 0].set_yscale('symlog', linthreshy=1)
# g.axes[1, 0].set_yscale('linear')

plt.tight_layout()

g.axes[0, 0].set_xbound(-0.5,
                        pg_coord_plot_data.index.get_values().max() + 0.5)
@

\Cref{fig:pg_coord_plot} shows convergence measures for each randomized
coordinate order.  The [average] difference in the number of iterations
required for coordinate descent and proximal gradient
is fairly noticeable.  Nonetheless, both reach effectively the same limits.

\begin{remark}
  Similar ideas behind batched vs. non-batched steps and block sampling--found
  within the Gibbs sampling literature \citep{roberts_updating_1997}--could
  explain the variation due to coordinate order and the relative efficiency of
  coordinate descent.
  There are also connections with our comments in \Cref{rem:bases} and, to some
  extent, stochastic gradient descent (SGD) \citep{bertsekas_incremental_2010}.

  In a woefully lacking over-generalization, let's say that it comes down to
  the [spectral] properties of the composite operator(s) $l \circ X$ and/or
  $\nabla l \circ X$.  These determine the bounds of efficiency for steps in
  certain directions and how blocking or partitioning the dimensions of $\beta$
  nears or distances from those bounds.
\end{remark}

\subsubsection{Regularization Paths}

Also, due to the relatively fast convergence of coordinate descent, the method
is a little more suitable for the computation of regularization paths-- i.e.
varying $\lambda$ between iterations.  There is much more to this topic, but
for simplicity let's just note that each $\lambda$ step has a ``warm-start''
from the previous descent iteration--which helps--and that we're otherwise fine
with the solution provided by this approach.

Next, we make a small extension to demonstrate the computation
of regularization paths--using \texttt{lasso\_path} for comparison.
<<reg_path_example, echo=True, evaluate=True>>=
from sklearn.linear_model import lasso_path, enet_path

beta_0 = np.zeros(M).astype('float64')

lambda_path, beta_path, _ = lasso_path(X, y)
path_len = np.alen(lambda_path)

beta_last = beta_0
pg_path_data = pd.DataFrame()
for i, lambda_ in enumerate(lambda_path):
    cd_path_step = CoordDescent(y, X, beta_last,
                        lambda x, a: prox_fn(x, N * lambda_ * a))

    cd_cols_fns = cols_fns[1:] + [
        (lambda *args, **kwargs: lambda_, r'$\lambda$')]
    est_data, beta_last = iterative_run(cd_path_step, loss_fn,
                                        cd_cols_fns,
                                        stop_tol=1e-4,
                                        stop_loss=True)

    pg_path_data = pg_path_data.append(est_data.iloc[-1, :],
                                       ignore_index=True)

@
<<sk_lasso_path, echo=True, evaluate=True>>=
cd_cols_fns = cols_fns[1:] + [
    (lambda *args, **kwargs: lambda_path[args[0]], r'$\lambda$')]

iter_values = []
for i, beta_ in enumerate(beta_path.T):
    iter_values.append([col_fn(i, beta_)
                        for col_fn, _ in cd_cols_fns])

sklearn_path_data = pd.DataFrame(iter_values,
                                 columns=zip(*cd_cols_fns)[1])
sklearn_path_data = sklearn_path_data.assign(
    replication=None, type='sklearn')

pg_path_data = pg_path_data.assign(type='pg')
pg_path_data = pg_path_data.append(sklearn_path_data,
                                   ignore_index=True)
@
<<pg_path_plot, echo=False, fig=True, caption=r'Regularization paths via coordinate descent.'>>=
pg_path_plot_data = pg_path_data.set_index(
    [r'$\lambda$', "type"])
# pg_path_plot_data = pg_path_plot_data.drop('replication', 1)

pg_path_plot_data = pg_path_plot_data.stack(level=0)
pg_path_plot_data = pg_path_plot_data.reset_index()

_ = pg_path_plot_data.rename_axis({'level_2': 'measure',
                                   0: 'value'},
                                   axis=1, inplace=True)

# sn_cp = sns.color_palette()[:1]
g = sns.FacetGrid(pg_path_plot_data,
                  row='measure',
                  hue='type',
                  # palette=sn_cp,
                  sharey=False, sharex=True,
                  legend_out=False)

def df_plot(x, y, color, label, **kwargs):
    data = kwargs.pop('data')
    data.plot(x=x, y=y, ax=plt.gca(),
              color=color, label=label,
              **kwargs)

_ = g.map_dataframe(df_plot,
                    r'$\lambda$', 'value',
                    alpha=0.7)
g.add_legend()
g.axes[0, 0].set_xscale('log')

plt.tight_layout()
@

\section{Discussion}

Among the changes discussed earlier regarding Theano \texttt{Op}s for the
proximal objects used here, we would also like to motivate much larger changes
to the applied mathematician/statistician's standard tools by demonstrating
the relevance of less common--yet increasingly useful--abstractions.
For instance, the proximal methods are neatly framed within
operator theory and set-valued analysis, where concepts like the
resolvent, sub-differential/gradient and others are common.  Abstractions
like these provide a compact means of extending familiar ideas into
new contexts--such as non-differentiable functions.

Unfortunately, our numerical libraries do not provide much in the way
utilizing these abstractions.  Most are strictly founded in the
representation of point-valued mappings, which can require significant
work-arounds to handle even the most common non-differentiable functions (e.g.
the absolute value within our example problem).  Our use of the proximal
framework is, in part, motivated by its near seamless use \emph{and}
simultaneous bypassing of set-valued maps--in implementation, at least.

There is no fundamental restriction blocking support for set-valued maps,
however--aside from the necessary labor and community interest.  Even minimal
support could provide a context that makes frameworks like ours merely minor
abstractions.
A similar idea can be found in the symbolic calculation of limits via
filters \citep{beeson_meaning_2005}.  Perhaps we can liken these changes to the
modern evolution of linear algebra libraries to tensor libraries.

We would also like to stress that the value provided by the symbolic tools
discussed here (Theano, really) are not \emph{just} in their ability to act as
compilers at a ``math level'', but more for their ability to concretely encode
mathematical characterizations of optimization problems and methods.  Work in
this direction is not new by any means; however, the combination of open-source
tools and industry interest in algorithms that fall under the broad class of
proximal methods (e.g. gradient descent, ADMM, EM, etc.) provides a more
immediate reason to pursue these abstractions in code and automate their use.

Regarding the proximal methods, we can consider Theano optimizations that
make direct use of the orthonormal basis property in \Cref{lem:prox_ortho_basis}, or
the Moreau-Fenchel theorem, and automate consideration for
various estimation methods via splitting (e.g. ADMM, Douglas-Rachford,
etc.)--perhaps by making decisions based on inferred or specified
tensor, function, and operator properties.
In future installments we'll delve into the details of these ideas.

\cite{wytock_new_2016} also discuss similar ideas in an optimization setting,
such as the use of symbolic graphs and a close coupling with useful mathematical
abstractions--including proximal operators.
Additionally, there are many other good examples \citep{diamond_cvxpy:_2016} of
constructive mathematical abstractions applied in code.

In most cases, libraries providing optimization tools and supporting model
estimation do not attempt to root their implementations within an independently
developed symbolic framework and then realize their relevant methodologies in
that context.  Too often the mathematical abstractions--or the resulting
methods alone--are directly implemented at the highest levels of abstraction
possible.  This is what we see as the result of popular libraries like
\texttt{scikit-learn} and the body of \texttt{R} packages.
One can also find the same efforts for proximal methods themselves--e.g.
in \cite{svaiter_pyprox_2017}, where individual functions for ADMM,
forward-backward/proximal gradient and Douglas-Rachford are the end result.
This is the most common approach and it makes sense in terms of simplicity, but
offers very little of the extensibility, generalization, or efficiencies
provided by shared efforts across related projects and fields.

In the context of Theano, implementations immediately benefit from its code
conversion, parallelization and relevant improvements to its basic graph
optimizations.  The latter covers both low-level computational efficiency--such
as relevant application of BLAS functions--and high-level tensor algebra
simplifications.

In a development community that builds on these tools, related efficiency and
performance gains can occur much more often, without necessarily sacrificing
the specificity inherent to certain areas of application.  For example, we can
safely use the Rao-Blackwell theorem as the basis of a graph optimization
in PyMC3, so it could be included among that project's default offerings;
however, it would be far too cumbersome to use productively in a less specific
context.

\bibliographystyle{plainnat}
\bibliography{more-proximal-estimation}


\end{document}
